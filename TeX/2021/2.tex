\subsection{February}

\subsubsection{February 1st}
Today I learned the definition of a natural transformation. These, are more or less, maps---or homotopies---between functors. Actually, because I know some homotopy type theory, I'll present these as homotopies. Fix categories $\mathcal C$ and $\mathcal D$ with two functors $F,G:\mathcal C\to\mathcal D.$ Formally, a natural transformation $\eta$ is, type-theoretically, the data
\[\eta_\bullet:\prod_{X:\mathcal C}F(X)\to G(X)\]
which commutes with $F$ and $G$ nicely. In other words, $\eta$ is made up morphisms $\eta_X:F(x)\to G(X),$ and we require the following diagram to commute.
\begin{center}
    \begin{tikzcd}
        X \arrow[d, "f"'] & F(X) \arrow[d, "F(f)"'] \arrow[r, "\eta_X"] & G(X) \arrow[d, "G(f)"] \\
        Y                 & F(Y) \arrow[r, "\eta_Y"']                   & G(Y)                  
    \end{tikzcd}
\end{center}
As an example, in the category of vector spaces over $k,$ we have a natural transformation from $\op{id}$ to the endofunctor $V\mapsto V^{\vee\vee}$ the double-dual of $V.$ Proving this is not terribly enlightening: the required $\eta_V$ takes $v\in V$ to $(T:V\to k)\mapsto Tv\in V^{\vee\vee},$ and this works. In fact, if we restrict our view to finite-dimensional vector spaces, this natural transformation is made up of isomorphisms, and we call $\eta$ a natural isomorphism.

Let's make the analogy to homotopy type theory explicit. Viewing a morphism between objects in a type $A:\UU$ as a ``path'' between them, functions between types become functors for free. To be explicit, this amounts to the statement
\[\prod_{(x,y:A)}(x=_Ay)\to(f(x)=_Bf(y)),\]
where $f:A\to B.$ Now our natural transformation $\eta$ goes between two functor/functions $f,g:A\to B,$ in our analogy that $\to$ is $=$ for objects, we see
\[\eta_\bullet:\prod_{x:A}f(x)=_Bg(x)\]
in terms of homotopy type theory. That is, $\eta$ is just a witness for the $f\sim g,$ so natural transformations are really just homotopies in this analogy.

\subsubsection{February 2nd}
Today I learned about the principal congruence modular subgroup of level $N.$ Succinctly, this is the kernel of the homomorphism $\varphi_N:\op{SL}_2(\ZZ)\to\op{SL}_2(\ZZ/N\ZZ)$ by reducing$\pmod N.$ (Taking remainders commutes with matrix multiplication because matrix multiplication merely requires ring operations.) In terms of symbols, this is
\[\Gamma(N):=\left\{M\in\op{SL}_2(\ZZ) : M\equiv\begin{bmatrix}1 & 0\\0 & 1\end{bmatrix}\pmod N\right\}.\]
We remark that, immediately, $\op{SL}_2(\ZZ)/\Gamma(N)\subseteq\op{SL}_2(\ZZ/N\ZZ)$ is a finite subgroup, so $\Gamma(N)$ has finite index in $\op{SL}_2(\ZZ).$

In fact, we claim that $|\op{SL}_2(\ZZ)/\Gamma(N)|=|\op{SL}_2(\ZZ/N\ZZ)|$ exactly, which we show by claiming that
\[\frac{\op{SL}_2(\ZZ)}{\Gamma(N)}\cong\op{SL}_2(\ZZ/N\ZZ).\]
Because we've defined $\Gamma(N)$ as the kernel of $\varphi_N,$ it suffices to show that $\varphi_N$ has full image. That is, for each $M_N\in\op{SL}_2(\ZZ/N\ZZ),$ there exists an $M\in\op{SL}_2(\ZZ)$ such that $M\equiv M_N\pmod N.$ Well, fix
\[M_N\equiv\begin{bmatrix} a & b \\ c & d \end{bmatrix} \pmod N\]
for some integers $a,b,c,d.$ All we know for now is that $ad-bc\equiv1\pmod N,$ and we need to lift this to $a',b',c',d'$ with $a'd'-b'c'=1.$ Quickly, we remark that if any of the $a,b,c,d$ are currently $0,$ we may add $N$ to them. So without loss of generality, none of these are $0.$

To start, we claim that we can lift $a,b,c,d$ so that $\gcd(a,b)=1.$ Notice that
\[\gcd(a,b)\mid ad-bc\equiv1\pmod N,\]
so $\gcd(a,b)$ divides an integer coprime to $N.$ That is, $\gcd(a,b,N)=1,$ and this is enough to guarantee a $k$ such that $\gcd(a+kN,b)=1,$ which is what we need to lift. Indeed, we construct $k$ by the Chinese remainder theorem. Iterate over primes $p\mid b$ (because $b\ne0,$ there are finitely many such primes).
\begin{itemize}
    \item If $p\nmid a,$ then fix $k\equiv0\pmod p.$ This implies $a+kN\equiv a\pmod p,$ so $p\nmid a+kN.$
    \item If $p\mid a,$ then we must have $p\nmid N$ to maintain $\gcd(a,b,N)=1,$ so we may set
    \[k\equiv N^{-1}(1-a)\pmod p.\]
    This implies $a+kN\equiv1\pmod p,$ so $p\nmid a+kN.$
\end{itemize}
After constructing the $k$ by iterating over the above, we see that $p\mid b$ implies $p\nmid a+kN.$ Thus, $\gcd(a,b)=1,$ as required.

Now that we have $\gcd(a,b)=1,$ it is possible to construct $x$ and $y$ so that $ax-by=1.$ We need to show that there exist $c'$ and $d'$ with $ad'-bc'=1$ such that $c'\equiv c$ and $d'\equiv d\pmod N.$ Well, currently, we have that
\[ad-bc=1+kN\equiv1\pmod N\]
for some integer $k.$ But then
\[a\underbrace{(d-kNx)}_{d'}-b\underbrace{(c-kNy)}_{c'}=(ad-bc)-kN=1\]
constructs out $c'$ and $d'$ quite explicitly. This completes the proof.

\subsubsection{February 3rd}
Today I learned that $\lnot\lnot$ has the required operators to be a monad. To be explicit, $\lnot\lnot$ is an endofunctor in the category of types (using the propostion-as-types correspondence), taking $A:\UU$ to $\lnot\lnot A\equiv((A\to0)\to0):\UU.$

We begin by exhibiting $\eta_A$ ($\texttt{return}$ in Haskell), which is a map $A\to((A\to0)\to0).$ This will become something of a theme in these constructions, so we state it explicitly: the key observation is that we can view this as a the curried function
\[A\to(A\to0)\to0.\]
Now the construction of $\eta$ is easier: we take an element $a:A$ and a function $n_A:A\to0,$ and then we output $n_A(a):0.$ Written out, this is
\[\eta_A\equiv\lambda(a:A).\lambda(n_A:A\to0).f(a).\]
Technically $\eta$ also takes $A$ as an input, but we ignore this.

We continue by showing $\mu_A$ ($\texttt{join}$ in Haskell), which is a map from $(((A\to0)\to0)\to0)\to0$ to $(A\to0)\to0.$ There are a lot of arrows here, and we remark that we can actually exhibit
\[(((A\to0)\to0)\to0)\to(A\to0)\]
and then plug in $A\to0$ for $A$ to get $\mu_A.$ As expected, the correct way to think about this is as a curried function
\[(((A\to0)\to0)\to0)\to A\to0.\]
So now we have a function $f:((A\to0)\to0)\to0$ and an element $a:A,$ and we need to exhibit $0.$ Well, from our $a:A$ we know that we can exhibit $(A\to0)\to0$ because we have $\eta_A(a)$! From here $f$ sends $\eta_A(a)$ to $0.$ Written out, we have
\[\mu_A\equiv\lambda(f:\lnot\lnot\lnot\lnot A).\lambda(a^*:\lnot A).f(\eta_{\lnot A}(a^*)).\]
I have replaced the clearer $\to0$ notation for $\lnot$ for brevity.

This gives the needed natural transformations to be a monad, but we haven't shown the coherence laws; maybe I'll do them later, but they don't look like fun. We do remark that we do have Haskell's $\texttt{fmap}$ function, taking $A\to B$ and $\lnot\lnot A$ to output $\lnot\lnot B.$ Fully written out, we are exhibiting
\[(A\to B)\to((A\to0)\to0)\to((B\to0)\to0).\]
As usual, we interpet this as a curried function like
\[(A\to B)\to((A\to0)\to0)\to(B\to0)\to0.\]
That is, we get inputs $f:A\to B$ and $n_{A\to0}:(A\to0)\to0$ along with a $n_B:B\to0$ so that we now need to exhibit $0.$ The finishing trick, now, is to use the same idea as the functor of points: $n_B\circ f$ takes $A\to B\to 0,$ so $n_{A\to0}$ will send $n_B\circ f$ to $0.$ Fully written out,
\[\texttt{fmap}_{A\to B}\equiv\lambda(f:A\to B).\lambda(n_{A\to0}:\lnot\lnot A).\lambda(n_B:\lnot B).n_{A\to0}(n_B\circ f).\]
We remark that having a $\texttt{join}$ and an $\texttt{fmap}$ allows us to construct $\texttt{bind}:\lnot\lnot A\to(A\to\lnot\lnot B)\to\lnot\lnot B$ by
\[\lambda(a^*:\lnot\lnot A).\lambda(f:A\to\lnot\lnot B).\texttt{join}_B\left(\texttt{fmap}_{A\to\lnot\lnot B}(f)(a^*)\right).\]
Quickly, $\texttt{fmap}_{A\to\lnot\lnot B}(f)$ has type $\lnot\lnot A\to\lnot\lnot\lnot\lnot B,$ so we may input $a^*$ to get out an element of $\lnot\lnot\lnot\lnot B,$ from which $\texttt{join}_B$ finishes. This construction of $\texttt{bind}$ works in the general case.

What I like about the $A\mapsto\lnot\lnot A$ example is that it is kind of giving me the feeling that the type $Ma$ is somehow ``above'' the type $a,$ for general monads. Namely, if we can talk about $a,$ then we can certainly talk about $Ma,$ but the reverse does not seem to hold, and in the case of $\lnot\lnot,$ it is actually impossible in constructivist logic to be able to fully recover properties of $A$ from properties of $\lnot\lnot A.$ Somehow $Ma$ is a bit more ethereal than $a.$

\subsubsection{February 4th}
Today I learned a little more about decidability of symbolic algebra series convergence. Specifically, I know that if we permit sums over multiple variables, then series convergence is undecidable. To be clear, for the time being we are allowing $\QQ[x_1,\ldots],\pi,i,|\bullet|,\exp,$ as well as addition, multiplication, and composition of these functions.

This is the easier than it appears, and in fact we claim that we can do this with only $\QQ$ and $|\bullet|.$ The key observation is that we can build a $0$-indicator over $\ZZ.$ As a lemma, we note that
\[\frac12(x+y+|x-y|)=\begin{cases}x & x>y, \\ y & y>x,\end{cases}=\max\{x,y\}.\]
It follows that we can construct $\max$ using only $|\bullet|.$ We remark that, at a high level, $|\bullet|$ is necessary because $\max$ is a non-smooth function, and $|\bullet|$ should be sufficient because $\max$ is piecewise differentiable. Now fix some $\varepsilon>0,$ and we claim that
\[\iota_0(q):=\max\left(0,-\frac1\varepsilon|q|+1\right)=\begin{cases}
    1 & q=0, \\
    0 & |q|\ge\varepsilon.
\end{cases}\]
Note that we have not claimed behavior for $q\in(-\varepsilon,\varepsilon)\setminus\{0\},$ but if we fix $\varepsilon<1,$ then this interval will contain no integers. So $\iota_0$ can still work as a $0$-indicator. Anyways, at $q=0,$ the above reads $\iota_0(0)=\max(0,1)=1.$ And if $|q|\ge\varepsilon,$ then
\[-\frac1\varepsilon|q|+1<0,\]
so $\iota_0(q)=0.$

Now we show that it is undecidable to determine if a series in multiple variables converges. In fact, we claim that an oracle which can determine if multivariate series converge can determine if a Diophantine equation in $\NN$ has roots, which we know to be undecidable from Hilbert's 10th. Fix some Diophantine equation $p(x_1,\ldots,x_n)=0$ for $p\in\ZZ[x_1,\ldots,x_n].$

We begin by lifting $p$ to a Diophantine with either $0$ or infinitely many solutions. Indeed, we quickly consider the polynomial
\[\hat p(x_1,\ldots,x_{n+1}):=\left(x_{n+1}^2+1\right)p(x_1,\ldots,x_n).\]
In particular, if $p$ has a natural root named $(y_1,\ldots,y_n),$ then $\hat p$ has infinitely many solutions by $(y_1,\ldots,y_n,y_{n+1})$ for any $y_{n+1}\in\NN.$ However, if $p$ has no solutions, then $\hat p$ can't have any solutions, for $\hat p=0$ requires either $x_{n+1}^2+1=0,$ which is impossible, or $p(x_1,\ldots,x_n)=0.$

From here, the killing blow is to look at
\[\sum_{a_1,\ldots,a_{n+1}=0}^\infty\iota_0\left(\hat p(a_1,\ldots,a_{n+1})\right).\]
We claim that this sum diverges if and only if $p$ has a natural root. We divide this into cases.
\begin{itemize}
    \item If $p$ has no natural roots, then $\hat p$ has no natural roots, so we must always have $\hat p(a_1,\ldots,a_{n+1})\in\ZZ\setminus\{0\}.$ Thus $\iota_0$ always returns $0,$ and the series converges.
    \item If $p$ has a natural root, then $\hat p$ has infinitely many natural roots, so $\iota_0$ will return $1$ infinitely many times. Thus, the series diverges.
\end{itemize}
The claim follows, so being able to detect series convergence is enough to detect solutions to Diophantine equations, and we are done here.

I find it quite remarkable that this is doable with merely polynomials and $|\bullet|.$ I have been told that single-variable series convergence is undecidable with the usual suspects ($\QQ[x_1,\ldots],\pi,i,|\bullet|,\exp$), which must mean that decreasing the number of variables is a difficult task; I have not done this yet. If I put in $\floor\bullet$ and $\sqrt\bullet,$ then we recall that
\[f_2(n)=\left(\left|\floor{\sqrt n}-\left(n-\floor{\sqrt n}^2\right)\right|,n-\floor{\sqrt n}^2\right)\]
from a few days ago can surject $\NN\to\NN^2.$ It doesn't matter that $f_2$ isn't injective: if we have no roots, then we're only repeating $0$; and if we have roots, then the worst we can do is repeat $1,$ but we're already diverging anyways. My main qualm with $f_2$ is that $\sqrt\bullet$ isn't defined over all $\RR,$ so the set of functions we get are poorly defined. Also, $\floor\bullet$ introduces discontinuous functions, which is undesirable.

It might be possible to map $\NN\to\RR^\bullet$ in a way that is very close to $\NN^\bullet.$ In particular, we can build $\min(x,y)=-\max(-x,-y)$ and then, for $\varepsilon\in(0,1),$
\[\min\left(\frac1\varepsilon\max(-|q|+1,0),1\right)=\begin{cases}
    1 & |q|\le1-\varepsilon, \\
    0 & |q|\ge1.
\end{cases}\]
Thus, if we can get $\NN\to\RR^\bullet$ to be close enough to $\NN^\bullet$ so that $p(a_1,\ldots,a_n)=0$ implies we get some $p(x_1,\ldots,x_n)\le1-\varepsilon,$ then we can sum over the above indicator and still count the number of solutions to $p.$ This would finish, but of course the main difficulty is still that map $\NN\to\RR^\bullet.$

\subsubsection{February 5th}
Today I learned a concrete example of a monad, courtesy of Alexander Burton. As usual in functional programming, we are looking at functions as functors in the category of types. 

Fixing a monoid $(M,+),$ we define $T:A\mapsto A\times M$ to be our monad. To be explicit, this is endofunctor taking
\[\begin{cases}
    T(A)=A\times M & A\text{ object}, \\
    T(f)=f\times\op{id} & f:A\to B.
\end{cases}\]
We note quickly that we tend to think of $T(f)$ as $\texttt{fmap}$ in Haskell, for it takes $(A\to B)\to(T(A)\to T(B)).$ Fixing $0\in M$ as our identity element, we remark that this definition permits
\[\eta_A(a)=(a,0)\]
for $a\in A.$ Very quickly, we see $\eta_A:A\to T(A)$ ($\texttt{pure}$) is a natural transformation from $\op{id}$ to $T(\op{id})$ by chasing elements, as shown in the following commutative diagram. Here, $f:A\to B$ takes $f(a)=b.$
\begin{center}
    \begin{tikzcd}
        A \arrow[d, "f"'] & a \arrow[r, "\eta_A", maps to] \arrow[d, "\operatorname{id}(f)"', maps to] & {(a,0)} \arrow[d, "T(f)", maps to] \\
        B                 & b \arrow[r, "\eta_B"', maps to]                                            & {(b,0)}                           
    \end{tikzcd}
\end{center}
This works because $T(f)$ was defined cleanly. The final piece of data is a natural transformation $\mu_A:T^2(A)\to T(A)$ ($\texttt{join}$), which we define by
\[\mu_A((a,m),n)=(a,m+n).\]
This is actually a natural transformation, again by chasing elements in the following commutative diagram. Here, $f:A\to B$ takes $f(a)=b.$
\begin{center}
    \begin{tikzcd}
        A \arrow[d, "f"'] & {((a,m),n)} \arrow[r, "\mu_A", maps to] \arrow[d, "T^2(f)"', maps to] & {(a,m+n)} \arrow[d, "T(f)", maps to] \\
        B                 & {((b,m),n)} \arrow[r, "\eta_B"', maps to]                             & {(b,m+n)}                           
    \end{tikzcd}
\end{center}
Again, $T(f)$ being clean makes this natural.

It remains to show the coherence laws. The goal of this example is to explain why $\mu\circ T\mu=\mu\circ\mu T$ corresponds to associativity and $\mu\circ T\eta=\mu\circ\eta T=\op{id}$ corresponds to identity. It is somewhat magical to see morphisms able to communicate this.

We begin with associativity. We need to show that the following diagram commutes.
\begin{center}
    \begin{tikzcd}
        T^3(A) \arrow[d, "T(\mu_A)"'] \arrow[r, "\mu_{T(A)}"] & T^2(A) \arrow[d, "\mu_A"] \\
        T^2(A) \arrow[r, "\mu_A"']                            & T(A)                        
    \end{tikzcd}
\end{center}
As usual, this is done by chasing elements. We expand out $T$ and the rest of the morphisms they map to in the following diagram.
\begin{center}
    \begin{tikzcd}
        {(((a,m),n),k)} \arrow[d, "T(\mu_A)"', maps to] \arrow[r, "\mu_{T(A)}", maps to] & {((a,m),n+k)} \arrow[d, "\mu_A", maps to] \\
        {((a,m+n),k)} \arrow[r, "\mu_A"']                                                & {(a,m+n+k)}                              
    \end{tikzcd}
\end{center}
Here, the bottom arrow gives $(a,(m+n)+k),$ and the left arrow gives $(a,m+(n+k)),$ and these are equal exactly when $+$ associates. So this coherence law holds exactly because our monoid is associative.

Now we do identity. We need to show that the following diagram commutes.
\begin{center}
    \begin{tikzcd}
        T(A) \arrow[d, "T(\eta_A)"'] \arrow[r, "\eta_{T(A)}"] \arrow[rd, "\operatorname{id}"] & T^2(A) \arrow[d, "\mu_A"] \\
        T^2(A) \arrow[r, "\mu_A"']                                                            & T(A)                     
    \end{tikzcd}
\end{center}
Of course, we show this by chasing element, as seen in the following diagram.
\begin{center}
    \begin{tikzcd}
        {(a,m)} \arrow[d, "T(\eta_A)"', maps to] \arrow[r, "\eta_{T(A)}", maps to] \arrow[rd, "\operatorname{id}", maps to] & {((a,m),0)} \arrow[d, "\mu_A", maps to] \\
        {((a,0),m)} \arrow[r, "\mu_A"', maps to]                                                                            & {(a,m)}                                
    \end{tikzcd}
\end{center}
Here, the bottom arrow gives $(a,0+m),$ the right arrow gives $(a,m+0),$ and the diagonal arrow gives $(a,m).$ However, these are all equal exactly because our monoid as $0$ as identity. So we get out coherence law due to identity. This completes the proof that we have a monad.


\subsubsection{February 6th}
Today I learned how to recover the modulus of a linear congruential pseudorandom number generator, from \href{https://security.stackexchange.com/a/4306}{here}. Very quickly, a linear congruential generator is a sequence of pseudorandom modular classes $a_0,a_1,a_2,\ldots$ defined by a seed $a_0$ and the recurrence relation
\[a_{n+1}\equiv Aa_n+B\pmod N\]
for some constants $A,B,N.$ The question is how we should back-construct $A,B,N$ if given some (hopefully small) number of terms of $a_\bullet$; this would let us predict the rest of the pseudorandom sequence and is therefore bad. We note that if we know $N,$ we see
\[\begin{cases}
    a_1\equiv Aa_0+B\pmod N, \\
    a_2\equiv Aa_1+B\pmod N
\end{cases}\]
is a system of linear congruences. Assuming that the sequence is kind, we can use this to solve for $A$ and $B$ by just solving with row reduction in the normal way. Thus, we can recover $A$ and $B$ given $N$ in roughly $3$ terms. Explicitly, subtracting and then solving gives
\[A\equiv\frac{a_2-a_1}{a_1-a_0},\qquad B\equiv a_1-Aa_0\pmod N.\]
We remark that ``the sequence is kind'' means that $\gcd(u_1-u_0,N)=1$ so that we can actually do division here. However, this is a non-problem, for $\gcd(u_1-u_0,N)$ will get propagated through the entire sequence, so we can just divide it from all terms, and then do the modular division safely. Note if $u_1-u_0\equiv0,$ then the sequence is constant.

It remains to solve for the modulus $N$ from our sequence. The outline is that our linear recurrence lets us generate modular classes, so we might be able to turn this into a way to generate numbers $0\pmod N,$ in which case we should be able to take $\gcd$ to get $N.$ To begin, we get rid of the constant term by defining
\[b_n:=a_{n+1}-a_n.\]
With $n+1$ consecutive terms of $a_\bullet,$ we can solve for $n$ terms of $b_\bullet,$ so we will need more terms, but so it goes. Anyways, the nice thing is that
\[b_{n+1}\equiv(a_{n+2}-a_{n+1})\equiv A(a_{n+1}-a_n)\equiv Ab_n.\]
To turn this into a sequence $0\pmod N,$ we define
\[c_n:=b_{n+2}b_n-b_{n+1}^2\equiv A^2b_n-A^2b_n\equiv0\pmod N.\]
We need yet another term of $a_\bullet$ for the $c_\bullet$ sequence, but so it goes. Anyways, this generates a lot of numbers which are $0\pmod N.$ At a high level, we expect
\[\gcd(c_0,c_1,\ldots)=N\gcd\left(\frac{c_0}N,\frac{c_1}N,\ldots\right)=N\]
because there's no reason that $c_\bullet/N$ should have any common factors. For example, even if we were just to look at $\gcd(c_n/N,c_{n+1}/N),$ we expect this to be $1$ with probability $6/\pi^2$ for each $n.$ So after $10$ terms, the probability is less than $1\%.$ Thus, about $12$ terms of $a_\bullet$ is probably sufficient to get $N.$

I don't think a proof of this heuristic would be easy. The sequence $c_\bullet$ only looks pseudorandom because $a_\bullet$ looks random, which is difficult to pin down. Further, getting some proven bound on the number of terms of $a_\bullet$ or $c_\bullet$ feels unhelpful: I get the feeling that what can be proven will be significantly worse than the heuristic (and probably practical use) suggests.

\subsubsection{February 7th}
Today I learned that random play in two-pile Nim give each player a $\frac12$ probability of winning if one pile has at least $2$ stones. Formally speaking, ``random play'' means that a player on her turn lists all possible moves and then makes a random move. This is different from picking a legal pile randomly and then taking a random number of stones, but I don't think the result changes.

We prove this by brute-force tabulation. Let $p(m,n)$ be the probability that player $1$ if the two piles have $m$ and $n$ stones. By convention, we set $p(0,0)=0$ because player $1$ is the first player to be unable to make a move. From here, we note that we have the recurrence
\[p(m,n)=1-\frac1{m+n}\left(\sum_{k=1}^mp(m-k,n)+\sum_{\ell=1}^np(m,n-\ell)\right)\]
for $m,n,m+n\ge0.$ To prove this, we note that player $1$ must make a move, which consists of either taking $k\le m$ stones from the first pile or $\ell\le n$ stones from the second pile. Each of these moves occur with probability $\frac1{m+n},$ and afterwards, player $1$ is effectively playing as player $2$ in the $p(m-k,n)$ or $p(m,n-\ell)$ game, so we subtract the probabilities from $1.$

Quickly, we simplify our recurrence to
\[p(m,n)=1-\frac1{m+n}\left(\sum_{k=0}^{m-1}p(k,n)+\sum_{\ell=0}^{n-1}p(m,\ell)\right)\]
by flipping the sums. As some motivation, we remark that we could use the recurrence relation to build a table of the various values of $p(m,n).$ New values are computed by one minus the average of all terms above or to the left. Here is the start of the table.
\[\begin{array}{r|c|c|c|c}
           & 0      & 1       & 2     & \cdots \\\hline
    0      & 0      & 1      & 1/2    & \cdots \\\hline
    1      & 1      & 0      & 1/2    & \cdots \\\hline
    2      & 1/2    & 1/2    & 1/2    & \cdots \\\hline
    \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}\]
However, the fast way to finish the proof is to claim directly that $p(m,n)$ is
\[p(m,n)=\begin{cases}
    0 & (m,n)\in\{(0,0),(1,1)\}, \\
    1 & (m,n)\in\{(0,1),(1,0)\}, \\
    1/2 & \text{else}.
\end{cases}\]
Note that this is the main claim. To prove, we see this matches our initial condition $p(0,0)=0,$ so it suffices to show that we satisfy the recurrence relation. We quickly check that $p(0,1)=p(1,0)=1-0$ and that $p(1,1)=1-\frac12(1+1)=0.$ It follows that, for $m\ge2,$ we see
\[\sum_{k=0}^{m-1}p(k,n)=p(0,n)+p(1,n)+\sum_{k=2}^{m-1}p(k,m)=1+\frac{m-2}2=\frac m2.\]
Here the trick is that $p(0,n)+p(1,n)$ is one of $1+0,\,0+1,\,\frac12+\frac12,$ all of which are equal to $1.$ Similarly, for $n\ge2,$ we see
\[\sum_{\ell=0}^{n-1}p(m,\ell)=p(m,0)+p(m,1)+\sum_{\ell=2}^{n-1}p(m,\ell)=1+\frac{n-2}2=\frac n2.\]
Again, the trick is that $p(0,m)+p(1,m)$ always evaluates to $1.$ It follows that
\[1-\frac1{m+n}\left(\sum_{k=0}^{m-1}p(k,n)+\sum_{\ell=0}^{m-1}p(m,\ell)\right)=1-\frac1{m+n}\cdot\frac{m+n}2=\frac12,\]
which completes the proof.

Quickly, we talk about the alternate method of ``random play'' mentioned at the beginning: a player picks a random pile and then picks up a random number of stones. Let the probability of player $1$ winning in this game with $m$ and $n$ stones be $q(m,n).$ We still have $q(0,0)=0,$ and a similar computation as before gives the recurrence relation
\[q(m,n)=1-\frac12\left(\frac1m\sum_{k=0}^{m-1}q(k,n)+\frac1n\sum_{\ell=0}^{n-1}q(m,\ell)\right)\]
for $m,n>0,$ and
\[q(m,0)=1-\frac1m\sum_{k=0}^{m-1}q(k,0),\qquad q(0,n)=1-\frac1n\sum_{\ell=0}^{n-1}q(0,\ell).\]
With these in hand, it's not difficult to actually show $q=p$ always. Indeed, our recurrence relations matches $p$ for the $m=0$ and $n=0$ cases, so we match there. We can also check that $q(1,1)=1-\frac12(1+1)=0$ still. Then for $m,n>0,$ the internal sums will match what we evaluates for $p$ in the above proof, so we see
\[1-\frac12\left(\frac1m\cdot\frac m2+\frac1n\cdot\frac n2\right)=\frac12.\]
This finishes the proof that $p$ satisfies the recurrence relation of $q,$ so they are equal.

\subsubsection{February 8th}
Today I learned some deeper type-theoretic significance to currying. Quickly, currying is, very roughly, induction for dependent pair types. That is, for $A:\UU$ and $B:A\to\UU$ with $C:\prod_{(a:A)}B(a)\to\UU,$ we define
\[\op{curry}:\left(\sum_{(a:A)}\sum_{(b:B(a))}C(a)(b)\right)\to\left(\prod_{(a:A)}\prod_{(b:B(a))}C(a)(b)\right)\]
by
\[\op{curry}(f):\equiv\lambda(a:A).\lambda(b:B(a)).f((a,b)).\]
We also have the sibling function
\[\op{uncurry}:\left(\prod_{(a:A)}\prod_{(b:B(a))}C(a)(b)\right)\to\left(\sum_{(a:A)}\sum_{(b:B(a))}C(a)(b)\right)\]
defined by
\[\op{uncurry}(f):\equiv\lambda\left((a,b):\textstyle\sum_{(a:A)}B(a)\right).f(a)(b).\]
We don't check that these type-check here. We remark that this $\lambda$-calculus on dependent pairs is really just doing an induction by defining how the pair behaves on a particular element. Indeed, $\op{uncurry}$ is the induction principle.

The significance here is that $\op{curry}$ and $\op{uncurry}$ actually turn out to be quasi-inverses. So it happens that we have an equivalence (Tewari called it ``isomorphism'')
\[\left(\sum_{(a:A)}\sum_{(b:B(a))}C(a)(b)\right)\simeq\left(\prod_{(a:A)}\prod_{(b:B(a))}C(a)(b)\right).\]
In lieu of the univalence axiom, we remark that we in fact have currying witness the equality
\[\left(\sum_{(a:A)}\sum_{(b:B(a))}C(a)(b)\right)\simeq\left(\prod_{(a:A)}\prod_{(b:B(a))}C(a)(b)\right).\]
This association of pairs and functions is somewhat remarkable. At least in terms of the functions they define, they give exactly the same things, even though functions and pairs feel like profoundly different objects.

Anyways, let's show this. In one direction, we need to witness $\op{curry}\circ\op{uncurry}\sim\op{id}.$ That is, given $f:\prod_{(a:A)}\prod_{(b:B(a))}C(a)(b),$ we need to witness
\[\op{curry}(\op{uncurry}(f))=f.\]
Plugging in, the left-hand side is
\[\lambda a.\lambda b.\op{uncurry}(f)((a,b)).\]
Continuing, we see it is
\[\lambda a.\lambda b.f(a)(b).\]
However, this is equal to $f$ just by $\refl_f.$ Indeed, repeated $\beta$-reduction says
\[f\equiv\lambda a.f(a)\equiv\lambda a.\lambda b.f(a)(b).\]
Remarkably, no function extensionality is required, although it does make for a quick proof of this as well. Namely, we merely have to plug in $a$ and $b$ into the expression and check that it works.

For the sake of completeness, I show the other direction even though it is pretty much the same. We need to witness $\op{uncurry}\circ\op{curry}\sim\op{id}.$ That is, given $f:\sum_{(a:A)}\sum_{(b:B(a))}C(a)(b),$ we need to witness
\[\op{uncurry}(\op{curry}(f))=f.\]
Plugging in, the left-hand side is
\[\lambda(a,b).\op{curry}(f)(a)(b).\]
Continuing, we see it is
\[\lambda(a,b).f((a,b)).\]
And again, this is equal to $f$ by just $\refl_f.$ Indeed, a single $\beta$-reduction says
\[f\equiv\lambda(a,b).f((a,b)).\]
Note that we have omitted the types of each element here, but they are present in the original definitions of $\op{curry}$ and $\op{uncurry}.$ We remark once again that no function extensionality was required.

\subsubsection{February 9th}
Today I learned the baby-step giant-step algorithm to solve the discrete log problem in general finite cyclic groups. As implied, our setup is to fix a finite cyclic group $G=\langle g\rangle$ of order $N$ and element $h\in G$ so that we want $x\in\ZZ$ satisfying $h=g^x.$ We note that the way we represent elements of $G$ is unimportant, as it should be in this algorithm.

Brute force would be trying all $N$ different values of $x$ until hitting the correct one. This requires $O(1)$ space but a terrible $O(N)$ run-time. The baby-step giant-step algorithm trades some of the $O(1)$ space for a better run-time, akin to the Dirichlet hyperbola method. The key observation is that
\[\frac x{\sqrt N}\le\sqrt N.\]
More explicitly, this implies that any $x$ can be written as
\[x=a\lceil\sqrt N\rceil+b\]
where $0\le a,b<\lceil\sqrt N\rceil.$ So if we can check $\lceil\sqrt N\rceil$ values of $x$ simultaneously, our run-time will drop to $O(\sqrt N).$

The way that this is done is by storing $\lceil\sqrt N\rceil$ values of $x$ in a lookup table. Here are the steps.
\begin{enumerate}
    \item Store $g^0,g^1,\ldots,g^{\lceil\sqrt N\rceil-1}$ in a lookup table.
    \item Precompute $g^{\lceil\sqrt N\rceil}$ into $g_*.$
    \item Move $h$ into a local $h_*.$
    \item Loop the following over $0\le a<\lceil\sqrt N\rceil.$
    \begin{enumerate}
        \item If $h_*$ is in the lookup table, return $a\lceil\sqrt N\rceil+b.$
        \item Else move $h_*g_*$ into $h_*.$ Now $h_*=hg_*^{-a-1},$ and we're ready to loop.
    \end{enumerate}
\end{enumerate}
We note that this is gauranteed to terminate because $x=a\lceil\sqrt N\rceil+b$ for some $0\le a,b<\lceil\sqrt N\rceil$ as above, so we'll find our output eventually.

As for quick run-time analysis, we note that creation of the lookup table and the loop are our limiting $O(\sqrt N)$ operations. There is some worry that the loop operation isn't $O(1),$ but we note that this can be done by hashing: we can hash $h_*$ in and then check the hashed entry in a table in $O(1)$ time without having to check each element of the lookup table manually. This also means that the creation of the lookup table requires hashing, but this is still an $O(\sqrt N)$ operation in total.

We close by remarking that no part of the proof required $G$ to have exactly $N$ elements. We merely needed $|G|\le N$ to make the bounding work out when sieving for $x.$ I find this somewhat remarkable and ``best possible'' in terms of what we get to know about the group. That is, if we want a discrete log algorithm of a group, knowing exactly how many elements are in the group feels unnecessary. But to upper-bound run-time of the algorithm, we probably need an upper-bound on the group size.

\subsubsection{February 10th}
Today I learned that the Eisenstein series are modular forms, from \href{https://www.math.arizona.edu/~swc/}{here}. These are the series, parameterized by $k\ge3,$
\[G_k(z)=\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{(mz+n)^k}.\]
We get the main computation out of the way first. Because $\op{SL}_2(\ZZ)$ is generated by $90^\circ$ rotations and horizontal shears, it suffices to note that
\[G_k\left(\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}z\right)=G_k\left(\frac{-1}z\right)=\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{\left(-\frac mz+n\right)^k}=\frac{G_k(z)}{z^k},\]
and
\[G_k\left(\begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}z\right)=G_k(z+1)=\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{(mz+m+n)^k}=G_k(z).\]
In particular, satisfying the modularity condition is closed under multiplication and inversion, so it is enough to satisfy the modularity condition for the above matrices.

We now show the uninteresting parts of this proof. We need $G_k$ to be holomorphic and holomorphic at $\infty$ to be a proper modular form. To show that $G_k$ is holomorphic, we need to know that it converges everywhere, and then I think manually checking the limit definition or something. Anyways, we begin by showing, for $k\ge3,$
\[S_k=\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{(|m|+|n|)^k}\]
converges, which looks of roughly the same growth rate as $G_k.$ All terms are positive, so we may rearrange terms into showing the convergence of
\[S_k=4\Bigg(\underbrace{\sum_{n=1}^\infty\frac1{n^k}}_{m=0}+\sum_{m,n=1}^\infty\frac1{(m+n)^k}\Bigg)\]
by taking the upper-right quadrant of $\ZZ^2.$ The first sum here is $\zeta(k)$ and therefore converges. To deal with the double sum, we need to bound what it looks like for a single $m,$ which is
\[\sum_{n=1}^\infty\frac1{(m+n)^k}\le\frac1{(m+1)^k}+\int_1^\infty\frac1{(m+t)^k}\,dt=\frac1{(m+1)^k}+\frac1{(k-1)(m+1)^{k-1}}.\]
This is bounded by $2/m^{k-1},$ so we see
\[\sum_{m,n=1}^\infty\frac1{(m+n)^k}\le2\sum_{m=1}^\infty\frac1{m^{k-1}}=2\zeta(k-1).\]
Note $k>2$ tells us that the sum is upper-bounded, and because it only has positive terms, the sum must converge.

Now we show that $G_k$ converges; as should be expected, we show it absolutely converges. That is, we focus on
\[|G|_k(z)=\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{|mz+n|^k}.\]
This feels like it should have similar growth rate as $S_k.$ We codify this by exhibiting a constant $C$ for which
\[|mz+n|\le C(|m|+|n|),\]
which will show $|G|_k(z)$ converges by comparison. For $m=0,$ any $C\ge1$ will suffice. Else, rearranging, we see we are interested in upper-bounding
\[\frac{|mz+n|}{|m|+|n|}=\frac{|z+|n/m||}{1+|n/m|}=:f(|n/m|).\]
The function $f:\RR_{\ge0}\to\RR$ is certainly defined over all real inputs as well as continuous, so it suffices to check its behavior as $|n/m|\to\infty.$ Letting $z=a+bi,$ this is
\[\lim_{x\to\infty}\frac{|z+x|}{1+x}=\lim_{x\to\infty}\frac{|z/x+1|}{1/x+1}=1<\infty.\]
So because $f(x)$ is continuous on the closed interval $[0,\infty)$ and bounded as $x\to\infty,$ we see $f$ must be bounded, giving us our $C.$ This finishes the proof of $G_k$ being holomorphic.

It remains to show that $G_k$ is holomorphic at $\infty.$ For this, we merely have to show that
\[\lim_{\op{Im}z\to\infty}G_k(z)<\infty.\]
Well, it (as usual) suffices to show this for $|G|_k,$ where the statement reads
\[\lim_{\op{Im}z\to\infty}\sum_{\substack{(m,n)\in\ZZ^2\\(m,n)\ne(0,0)}}\frac1{|mz+n|^k}<\infty.\]
The $m=0$ terms gives us $2\zeta(k)<\infty$; else, when $m>1,$ all of our terms vanish. Thus, $G_k$ is indeed holomorphic at $\infty,$ which completes the proof that $G_k$ is a modular form.

In other news, happy birthday to Sara Modur and Alan Baade!

\subsubsection{February 11th}
Today I learned (finally) the proof that $[0,1]$ is compact in the standard topology on $\RR.$ Of course, the proof generalizes to show that $\RR$ is locally compact, but we don't bother with this.

We begin by presenting the proof with no motivation. Suppose that $\{U_\alpha\}_{\alpha\in\lambda}$ is an open cover of $[0,1].$ The trick is to consider
\[S=\{\varepsilon\in(0,1]:[0,\varepsilon]\text{ can be covered with a finite subcover}\}.\]
Very quickly, we remark that if $\varepsilon_0\in S,$ then each $\varepsilon<\varepsilon_0$ is also in $S.$ Indeed, if $[0,\varepsilon_0]$ can be finitely covered, then the same cover can be used to cover $[0,\varepsilon]\subseteq[0,\varepsilon_0].$

We would like to use the least upper-bound property on $S.$ Note that some open set $U_\bullet$ covers $0$ and so contains an interval like $[0,2\varepsilon)$ for some $\varepsilon>0.$ (Formally, dissolve $U_\bullet$ into intervals because by the basis of our topology; one such interval contains $0.$) This implies that $[0,\varepsilon]\subseteq U_\bullet$ can be finitely covered, so $\varepsilon\in S$ inhabits $S.$

Additionally, $S$ has an upper bound: by definition, $\varepsilon\in S$ implies $\varepsilon\le1.$ Thus, we may apply the least upper-bound property and let $\varepsilon_0$ be the least upper bound of $S.$ Note $\varepsilon_0\le1$ because $1$ is an upper bound of $S.$ Further, we remark that, for $\alpha\in(0,1],$ we see $\alpha\not\in S$ implies nothing above $\alpha$ is in $S,$ so $\alpha$ is an upper bound of $S,$ so $\varepsilon_0\le\alpha.$ Thus, $\alpha<\varepsilon_0$ implies $\alpha\in S.$

We now do casework on $\varepsilon_0.$
\begin{itemize}
    \item If $\varepsilon_0=1,$ then we can finish quickly. As before, we can pick up $U_\bullet$ containing $1,$ and then extract an interval $(\alpha,1]$ inside of $U_\bullet,$ where $\alpha<1.$ In particular, $\alpha<\varepsilon_0,$ so $\alpha\in S.$ Thus we have a finite subcover
    \[\{V_k\}_{k=1}^N\]
    covering $[0,\alpha].$ To finish, we note
    \[\{V_k\}_{k=1}^N\cup\{U_\bullet\}\]
    which now finitely covers $[0,\alpha]\cup(\alpha,1]=[0,1]$ in full.
    \item If $\varepsilon_0<1,$ then we derive contradiction. To start, we pick up $U_\bullet$ containing $\varepsilon_0,$ and then extract an interval $(\alpha,\beta)$ inside of $U_\bullet$ which contains $\varepsilon_0.$ Now, again, $\alpha<\varepsilon_0,$ so $\alpha\in S,$ and we get a finite subcover
    \[\{V_k\}_{k=1}^N\]
    which covers $[0,\alpha].$ But then
    \[\{V_k\}_{k=1}^N\cup\{U_\bullet\}\]
    finitely covers $[0,\alpha]\cup(\alpha,\beta)=[0,\beta).$ In particular, we can cover $\left[0,\frac{\varepsilon_0+\beta}2\right]$ finitely, but $\frac{\varepsilon_0+\beta}2>\varepsilon_0,$ breaking our least upper bound.
\end{itemize}
Having covered all cases, we conclude that we can always extract a finite subcover of $[0,1].$

We remark with some motivation because the given proof is quite magical. The argument is somewhat motivated by pieces I heard of it a year ago but mainly (I would like to think) by the following framing. Imagine we are given an open cover $\{U_\alpha\}_{\alpha\in\lambda}$ of $[0,1],$ and we need to extract a finite subcover.

We can codify this situtaion as a game: I am allowed to query an ``interval genie'' with a real number $r\in[0,1],$ and I will receive some $U_\bullet$ containing $r.$ Effectively, this is all of the power I have over my cover. The naive approach, now, to extract an efficient subcover, is the following.
\begin{enumerate}
    \item Query the genie with $0.$ I get back some $U_\bullet$ containing the interval $[0,\varepsilon_0).$
    \item Query the genie with $\varepsilon_0.$ I get back some (maybe other) $U_\bullet$ containing the interval $(-,\varepsilon_1)$ containing $\varepsilon_0.$
    \item Query the genie with $\varepsilon_1.$ Rinse and repeat.
\end{enumerate}
This, of course, doesn't work because the genie can do things like giving me
\[\left[0,\frac14\right),\left(\frac18,\frac38\right),\left(\frac5{16},\frac7{16}\right),\cdots,\left(\frac{2^n-3}{2^{n+1}},\frac{2^n-1}{2^{n+1}}\right),\cdots.\]
Here I keep querying numbers of the form $\left(2^n-1\right)/2^{n+1},$ off to infinity, never even going to cover $\frac12$ with my generated subcover.

The key observation is that, after I realize what the genie is doing to me, I can query the genie for $\frac12.$ This gives me some open interval containing $\frac12$ with a little space to the left, which means that I really only need finitely many of the genie's garbage intervals from before.

As I understand, the argument presented above is more or less a formalization of this trick. Notably, this trick alone is not enough to get a finite subcover, for the genie could do the same thing trapping me below $\frac34,$ and then below $\frac78,$ and so on. I would have to iterate the trick on top of itself, and maybe more times on top of that.

What we have to do is consider covering, in the abstract, various intervals
\[[0,\varepsilon]\]
for various values of $\varepsilon.$ Notably, the genie can always play tricks up to any particular value of $\varepsilon,$ but we can always overcome the genie at this single $\varepsilon.$ The way to formalize the idea that we can ``always push farther'' up to $1$ is via the least upper-bound property, fixing $\varepsilon$ the farthest we can push, as done above.

\subsubsection{February 12th}
Today I learned the definition of support in topology. In terms of set theory, the support of a function $f:X\to\RR$ (or $f:X\to\CC$) is the set
\[\op{supp}(f):=\{x\in X:f(x)\ne0\}.\]
In general, we expect this to be ``large'' because the set of $0$s of a function are frequently ``small.'' However, this set is somewhat poorly-behaved in the general case, so we define the support topologically as
\[\op{supp}(f):=\op{Cl}(\{x\in X:f(x)\ne0\}),\]
the closure of the zero set. We remark that if $f$ is continuous, then $\{x\in X:f(x)\ne0\}=f^{-1}(\RR\setminus\{0\})$ is an open set.

Note that I think we expect $\op{supp}(f)$ to usually be isolated points anyways, so this is doesn't usually make much of a difference. But of course, if we do something like
\[f(x)=\max\{x,0\},\]
then $\{x\in\RR:f(x)\ne0\}=(0,\infty).$ We would like this set to be closed, so we have defined the support to be $\overline{(0,\infty)}=[0,\infty)$ to fix this.

Of note are the implications of being in or out of the support because these are somewhat confusing. The idea to remember is that $\op{supp}(f)$ is potentially a bit bigger than the nonzero set of a function (as it is defined). For example, we do have
\[f(x)\ne0\implies x\in\op{supp}(f),\]
but the converse does not hold: some $x$ with $f(x)=0$ could sneak in because of the closure. Explicitly, see $f(x)=\max\{x,0\}$ from before. Taking contrapositives, we also have
\[x\not\in\op{supp}(f)\implies f(x)=0,\]
but again the converse does not hold for the same reason. Here, we're saying that $X\setminus\op{supp}(f)$ is a restricted version of $f(x)=0.$

I will also mention the definition of compact support, which is actually why I had to look up the definition of support. A function has ``compact support'' merely if its support (topologically) is also compact. Our $f(x)$ from before does not work because $[0,\infty)$ is too big to be compact, but
\[f(x)=\max\left\{1-x^2,0\right\}\]
has support $\overline{(-1,1)}=[-1,1]$ and therefore has compact support. In general, I'm thinking about compact support as meaning some strongish form of very small---the nonzero set must be bounded for compact support.

\subsubsection{February 13th}
Today I learned some basic results in topological group theory. What's interesting here is that topology on its own has the potential to be quite disgusting, but adding in a little group theory makes the condition significantly nicer. I think the high-level reason why is that topological grops put all elements on a more equal footing because we require, for any element $g,$
\[U\text{ open}\iff gU\text{ open}\iff Ug\text{ open}.\]
This condition is called homogeneity. In particular, knowing all (open) neighborhoods around $g$ is equivalent to knowing the (open) neighborhoods around $e$ and then multiplying by $g.$

As an example of this in effect, we show that $T_1$ (points are closed) implies $T_2$ (Hausdorff). As a preliminary, we remark that if we an open neighborhood $U$ of $e,$ then $U\cap U^{-1}$ is a symmetric open neighborhood containing $U$ of $e.$ Additionally, the group operation defines a map
\[U\times U\to G.\]
This has image containing $(e,e),$ so the image has nontrivial intersection with $U.$ Tracking the pre-image of $U$ will give an open neighborhood $V\subseteq U$ of $e$ with $VV\subseteq U.$ Combining, we can also assume that $V$ is symmetric. In essence, this lemma lets us multiply and invert elements ``locally'' in a group with comfort.

Ok, now take $G$ a topological group which is $T_1.$ That is, for every pair of points $g,h\in G,$ there exists an open neighborhood of $g$ excluding $h.$ We show that $G$ is $T_2,$ which means for every pair $g,h\in G,$ there exist disjoint open neighborhoods of $g$ and $h.$

Using the intuition from before, we focus on the identity case where, say $h=e.$ Using the $T_1$ condition, we fix a neighborhood $U$ around $h=e$ excluding $g.$ We would like to use $gU$ for our neighborhood around $g,$ but $U$ and $gU$ might not be disjoint. However, any intersection
\[u_2=gu_1\in U\cap gU\]
forces $g=u_2u_1^{-1}$ to almost live inside of $U.$ So to fix this, we use the lemma to take a smaller open neighborhood $V\subseteq U$ of $e$ to be symmetric with $VV\subseteq U.$ Then we use $V$ and $gV$ as our neighborhoods, giving $u_1^{-1}\in V,$ and $g=u_2u_1^{-1}\in VV\subseteq U.$ This is our contradiction, proving $T_2.$

For clarity, let's do the general case with $h\ne e$ more directly. By $T_1,$ fix an open neighborhood $U$ around $e$ excluding $h^{-1}g,$ and the lemma gives us a smaller open neighborhood $V\subseteq U$ of $e$ which is symmetric and satisfies $VV\subseteq U.$ Now we claim
\[gV\quad\text{and}\quad hV\]
are the required neighborhoods for $T_2.$ Indeed, if $gV\cap hV$ is non-empty, then $h^{-1}gV\cap V$ is non-empty, so
\[\{h^{-1}g\}\cap VV^{-1}\]
is also non-empty. However, $V^{-1}=V,$ so $VV^{-1}\subseteq VV\subseteq U,$ and $h^{-1}g\not\in U,$ so this is a contradiction. Thus, $gV\cap hV=\emp,$ and we are done here.

\subsubsection{February 14th}
Today I learned some basic facts about Artin $L$-functions. We define these by an Euler product, but it turns out that unramified primes are a bit annoying to deal with, so I will adopt the notation
\[\prod_\mf pf(\mf p)\simeq\prod_\mf pg(\mf p)\]
to mean that the two Euler products agree on all but finitely many terms.

Fix $L/K$ a finite Galois extension of number fields. (I think this works for arbitrary global fields, but I don't want to think about that.) For an unramified prime $\mf p$ of $K$ under a prime $\mf P$ of $L,$ we have a Frobenius element $\op{Frob}_\mf P\in\op{Gal}(L/K),$ but in fact $\op{Frob}_\mf P$ is determined up to conjugacy by $\mf p.$ So, abusing notation, we write $\op{Frob}_\mf p$ to mean the entire conjugacy class.

Now, what Artin $L$-functions bring in is representation theory: for a representation $\rho:\op{Gal}(L/K)\to\op{GL}(V),$ we note that $\rho(\op{Frob}_\mf p)$ is determined up to change of basis (conjugacy), so
\[\op{charpoly}(\rho(\op{Frob}_\mf p))=\det(I-t\rho(\op{Frob}_\mf p))\]
is completely determined by $\op{Frob}_\mf p.$ Note that this is a bit of a nonstandard definition of the characteristic polynomial, but it functions. Now, we define the Artin $L$-function by plugging in $\op N(\mf p)^{-s}$ into the characteristic polynomial, writing
\[L(\rho,s)=\prod_{\mf p\text{ unramified}}\frac1{\det\left(I-\op N(\mf p)^{-s}\rho(\op{Frob}_\mf p)\right)}.\]
Note that only finitely many primes are unramified, so this is a pretty good definition of the Artin $L$-function. I have been told there are things one can do to deal with ramification, but I don't know this yet.

We do the trivial example for concreteness. If we fix our extension to be $K/\QQ$ and use the trivial representation $\rho_{\text{trivial}}:g\mapsto I,$ then we get
\[L(\rho,s)=\prod_{(p)\text{ unramified}}\frac1{\det(I-p^{-s}I)}=\prod_{(p)\text{ unramified}}\frac1{1-p^{-s}}.\]
This is $\zeta(s)$ (up to finitely many Euler factors), so our Artin $L$-functions do give the typical $\zeta$ function.

The main theorem we're going to show is that Artin $L$-functions include Dedekind $\zeta$-functions. Namely, if we fix our extension $K/\QQ,$
\[L(\rho_{\text{reg}},s)\simeq\zeta_K(s),\]
where $\zeta_K$ is the Dedekind $\zeta$ function. Notably, the regular representation actually matters. Expanding out the Euler products, we need to show that
\[\prod_{(p)\text{ unramified}}\frac1{\det\left(I-p^{-s}\rho_{\text{reg}}(\op{Frob}_p)\right)}\stackrel?\simeq\prod_{\mf p\subseteq K}\frac1{1-\op N(\mf p)^{-s}}.\]
For ease, fix $n=[L:K]$ with $e_p$ and $f_p$ the inertial and ramification information of $(p).$ Collecting the Euler factors on the right-hand side above a particular rational $(p),$ we want to show
\[\prod_{(p)\text{ unramified}}\frac1{\det\left(I-p^{-s}\rho_{\text{reg}}(\op{Frob}_p)\right)}\stackrel?\simeq\prod_{(p)}\left(\frac1{1-p^{-f_ps}}\right)^{n/(e_pf_p)}.\]
In light of the $\simeq,$ it suffices to ignore ramification and show that
\[\det\left(I-p^{-s}\rho_{\text{reg}}(\op{Frob}_p)\right)\stackrel?=\left(1-p^{-f_ps}\right)^{n/f_p}.\]
The $p^{-s}$ is somewhat arbitrary, so we might as well show $\det\left(I-t\rho_{\text{reg}}(\op{Frob}_p)\right)=\left(1-t^{f_p}\right)^{n/f_p}.$

By this point, the number theory is actually gone. For $g\in G=\op{Gal}(K/\QQ)$ with $|G|=n,$ we claim that
\[\det(I-t\rho_{\text{reg}}(g))\stackrel?=\left(1-t^f\right)^{n/f},\]
where $f$ is the order of $g.$ (Note $f_p$ is the order of $\op{Frob}_p.$) In order to make the characteristic polynomial more familiar, we send $t\mapsto\frac1\lambda$ and multiply by $\lambda^n$ so that we want
\[\det(\lambda I-\rho_{\text{reg}}(g))\stackrel?=\left(\lambda^f-1\right)^{n/f}.\]
Showing this is the matter of pick the right basis for $\rho_{\text{reg}}(g),$ for there is general theory about how to compute characteristic polynomials of permutation matrices. We organize $G$ by cosets $G/\langle g\rangle$: naming our cosets $h_1\langle g\rangle,h_2\langle g\rangle,\ldots,h_{n/f}\langle g\rangle,$ we order our basis for $\rho_{\text{reg}}(g)$ as
\[e_{h_1},e_{h_1g},\ldots,e_{h_1g^{f-1}},\quad e_{h_2},\ldots,e_{h_2g^{f-1}},\quad\ldots\quad e_{h_{n/f}},\ldots,e_{h_{n/f}g^{f-1}}.\]
This has the advantage that, for any of these coset-blocks of $f$ vectors, the definition of the regular representation tells us that
\[\rho_{\text{reg}}(g):e_{h_\bullet}\mapsto e_{h_\bullet g}\mapsto\cdots\mapsto e_{h_\bullet g^{f-1}}\mapsto e_{h_\bullet}.\]
Thus, under this basis, $\rho_{\text{reg}}(g)$ looks like
\[\begin{bmatrix}
    C_f \\ & \ddots \\ & & C_f
\end{bmatrix},\]
where $C_f$ is the $f\times f$ matrix
\[C_f=\begin{bmatrix}
    0 & 0 & \cdots & 0 & 1 \\
    1 & 0 & \cdots & 0 & 0 \\
    0 & 1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & 1 & 0
\end{bmatrix}.\]
We can now compute the characteristic polynomial with few tears. Because of the blocky form of $\rho_{\text{reg}}(g),$ we already have $\det(\lambda I-\rho_{\text{reg}}(g))=\det(\lambda I-C_f)^{n/f}.$ So we want to show
\[\det(\lambda I-C_f)\stackrel?=\lambda^f-1.\]
We could show this by computing eigenvalues of $C_f$ or just noting it is
\[\det\begin{bmatrix}
    \lambda & 0 & \cdots & 0 & -1 \\
    -1 & \lambda & \cdots & 0 & 0 \\
    0 & -1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & -1 & \lambda
\end{bmatrix},\]
which is
\[\lambda\det\begin{bmatrix}
    \lambda & \cdots & 0 & 0 \\
    -1 & \cdots & 0 & 0 \\
    \vdots & \ddots & \vdots & \vdots \\
    0 & \cdots & -1 & \lambda
\end{bmatrix}-(-1)^f(-1)\det\begin{bmatrix}
    -1 & \lambda & \cdots & 0 \\
    0 & -1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & -1
\end{bmatrix}\]
after doing expansion along the top row. The left-hand matrix is lower-triangle with $\lambda$ along the diagonal, so that term is $\lambda^f.$ The right-hand matrix is upper-triangle with $-1$ along the diagonal, so that term is $(-1)^f(-1)(-1)^{f-1}=1.$ In total, we see
\[\det(\lambda I-C_f)=\lambda^f-1,\]
which is what we wanted. We remark that this method can find the characteristic polynomial of a general permutation matrix.

We give a small application to close. Very quickly, we note that $L(\rho_1\oplus\rho_2,s)=L(\rho_1,s)L(\rho_2,s)$ because, after expanding Euler products, it suffices to show that
\[\det(I-p^{-s}(\rho_1\oplus\rho_2)(\op{Frob}_\mf p))=\det(I-p^{-s}\rho_1(\op{Frob}_\mf p))\det(I-p^{-s}\rho_2(\op{Frob}_\mf p)).\]
This is true by, for example, expanding out what $\oplus$ does in a basis to see that the characteristic polynomial does indeed multiply. Anyways, because
\[\rho_{\text{reg}}=\bigoplus_{\rho\text{ irred}}\rho^{\dim\rho}\]
by properties of the regular representation (which we showed last month), we conclude
\[\zeta_K(s)\simeq L(\rho_{\text{reg}},s)=\prod_{\rho\text{ irred}}L(\rho,s).\]
This is called Artin decomposition and is quite nice. For example, this tells us that $\zeta_\QQ\simeq L(\rho_{\text{trivial}},s)$ ``divides'' into $\zeta_K(s)$ in some natural way.

\subsubsection{February 15th}
Today I learned the statement of Artin reciprocity, which we use with some auxiliary results to derive Hilbert class fields and Kronecker-Weber, from \href{https://usamo.wordpress.com/2016/05/03/artin-reciprocity/}{here}. I'm kind of waiting to see what this has to with $L(\rho,s),$ but I suspect that it is somewhat beyond me. We require a sequence of definitions to get set up. A modulus $\mf m$ is defined as a formal product of places (real or infinite). We will restrict the formal product by having only finitely many nonzero powers and $\nu_\mf p(\mf m)=0$ if $\mf p$ is a complex infinite place and $\nu_\mf p(\mf m)\le1$ if $\mf p$ is a finite place. However,
\[\mf m=(2)(3)^3(5)^{123}(17)(101)\infty\]
is a perfectly valid modulus of $\QQ.$

Fixing a modulus $\mf m$ and number field $K,$ we define the generalized fractional ideal group by
\[I_K^{\mf m}=\{\mf a\in I_K:\mf a\text{ and }\mf m\text{ are relatively prime}\}.\]
Because of unique prime factorization of the fractional ideals $I_K,$ this is really just a restriction on the prime factorization of $\mf a.$ We do remark that a real infinite place $\sigma:K\to\RR$ gives $\alpha\equiv\beta\pmod\sigma$ if and only if $\sigma(\alpha/\beta)>0.$ Note that this is motivated by wanting $\alpha/\beta\equiv1.$

From this we also define generalized principal ideals by
\[P_K^{\mf m,1}=\{(\alpha):\alpha\equiv1\pmod{\mf m}\}.\]
Note $P_K(\mf m)\subseteq I_K(\mf m)$ and that $P_K(1)$ with $I_K(1)$ recover the $P_K$ and $I_K$ we are used to. This let us define a generalized class group by
\[\op{Cl}_K^{\mf m}=I_K^{\mf m}/P_K^{\mf m,1}.\]
These are called ray class groups. Note $\op{Cl}_K^1$ is the normal class group.

We are close to stating the Artin reciprocity law. Fix $L/K$ and abelian extension of number fields. Recalling the Frobenius map $\mf p\mapsto\op{Frob}_\mf p\in\op{Gal}(L/K)$ is well-defined, we can extend this multiplicatively to a mapping
\[I_K^{\mf m}\to\op{Gal}(L/K),\]
where our modulus $\mf m$ is divisible by all ramified places. It turns out that this mapping is surjective, so there exists a subgroup $H(L/K,\mf m)\subseteq I_K^{\mf m}$ for which
\[I_K^{\mf m}/H(L/K,\mf m)\cong\op{Gal}(L/K).\]
The subgroup $H(L/K,\mf m)$ is a congruence subgroup.

Now, the Artin reciprocity law asserts the existence of a modulus $\mf f$ (the conductor) divisible by exactly the ramified places for which
\[\mf f\mid\mf m\iff P_K^{\mf m,1}\subseteq H(L/K,\mf m)\subseteq I_K^{\mf m}.\]
What's nice here is that we get a sequence of surjective maps
\[I_K^{\mf f}\longrightarrow\frac{I_K^{\mf f}}{P_K^{\mf f,1}}\cong\op{Cl}_K^{\mf f}\longrightarrow\frac{I_K^{\mf f}}{H(L/K,\mf f)}\cong\op{Gal}(L/K).\]
So if we track an individual unramified prime $\mf p$ through the maps, we can determine $\op{Frob}_\mf p$ by first checking where $\mf p$ lands in $\op{Cl}_K^{\mf f}.$ In other words, aside from computation of the conductor $\mf f$ (which we can read as getting rid of ramification), we're saying that $\op{Frob}_\mf p$ is dependent entirely on information intrinsic to $K$ not involving $L$---there was no $L$ involved in our definition of $\op{Cl}_K^{\mf f}.$ This is miraculous.

In particular, $\op{Cl}_K^{\mf f}$ is a finite group where we have modded out by ``$1\pmod{\mf f},$'' so we can think about this like ``$\mf p\pmod{\mf f}.$'' So, for example, quadratic reciprocity cares about how $q$ splits in $\QQ(\sqrt{p^*}),$ which is associated to $\op{Frob}_\mf q.$ Well, we can check $\op{Frob}_\mf q$ by checking entirely where $(q)$ lands in $\op{Cl}_{\QQ(\sqrt{p^*})}^{\mf f},$ which turns into quadratic reciprocity.

It is a theorem (Takagi existence) that, in fact, every subgroup $H$ with
\[P_K^{\mf m,1}\subseteq H\subseteq I_K^{\mf m}\]
corresponds to $H(L/K,\mf m)$ for a unique abelian extension $L/K.$ So, for example, taking $\mf m=1$ and $H=P_K^{1,1}$ gives us an extension $M$ for which $H(M/K,1)=P_K^{1,1}.$ (Note that this means all primes are unramified by definition of $H(-,\mf m).$) Thus,
\[\op{Cl}_K=\frac{I_K^1}{P_K^{1,1}}=\frac{I_K^1}{H(M/K,1)}\cong\op{Gal}(M/K).\]
This remarkable field $M$ is called the Hilbert class field. Here we showed that its class Galois group is $\op{Cl}_K.$

Further, if $L$ is any other totally unramified abelian extension, then we can still look at the corresponding $H(M/K,1),$ which satisfies
\[H(L/K,1)=P_K^{1,1}\subseteq H(M/K,1)\subseteq I_K^1.\]
It turns out that $H(L/K,1)\subseteq H(M/K,1)$ actually implies $M\subseteq L$ (that is, congruence subgroups are inclusion-reversing), so $L$ is in fact that maximal totally unramified extension.

As a final nice property of Hilbert class fields, fix a prime $\mf p$ of $K.$ Now, $\mf p$ is principal if and only if $\mf p\in P_K^{1,1}=H(M/K,1).$ But this implies
\[\mf p\in H(M/K,1)\longmapsto\op{id}\in\frac{I_K^1}{H(M/K,1)}\cong\op{Gal}(M/K).\]
That is, $\mf p$ is principal if and only if $\op{Frob}_\mf p$ is the identity in $\op{Gal}(M/K),$ equivalent to $\op{Frob}_\mf p$ having order $1,$ equivalent to $f(\bullet/\mf p)=1.$ Because $\mf p$ is already unramified for free, this means $\mf p$ is principal if and only if $\mf p$ splits completely in $M.$ Cute.

We also remark that the Kronecker-Weber theorem falls out of this machinery. Here we are interested in abelian field extensions of $\QQ,$ which Artin reciprocity tells us we should be able to understand using information entirely intrinsic to $\QQ.$ This is good news because we understand $\QQ$ pretty well.

Anyways, we are trying to show that for any $K/\QQ,$ there exists an $m$ such that $K\subseteq\QQ(\zeta_m).$ Because of the inclusion reversal, this is equivalent to
\[H(\QQ(\zeta_m)/\QQ,\mf m)\subseteq H(K/\QQ,\mf m)\]
for some suitable modulus $\mf m.$ In order to use Artin reciprocity, we would like $\mf m$ to be divisible by the conductor of $K.$ Letting this conductor be $\mf f,$ we remark that we will also need $\infty\mid\mf m$ for $\QQ(\zeta_m)/\QQ,$ so we take $\mf f\mid m\infty=:\mf m,$ where $m\in\ZZ$ is divisible by exactly all the ramified primes in $K.$ We claim this $m$ works for $\QQ(\zeta_m).$

Indeed, the remarkable property of $\QQ(\zeta_m)$ is that we claim
\[H(\QQ(\zeta_m)/\QQ,m\infty)=P_\QQ^{\mf m,1}.\]
This makes $\QQ(\zeta_m)$ a ``ray class field'' of $\mf m.$ Anyways, this is the kernel of the map
\[I_\QQ^\mf m\longrightarrow\op{Gal}(\QQ(\zeta_m)/\QQ)\cong(\ZZ/m\ZZ)^\times.\]
This map by definition takes $p\ZZ$ (coprime to $m$) to $\zeta_m\mapsto\zeta_m^p$ (this maps to the Frobenius property), which is taken to $[p]_m.$ Extending multiplicatively, we're taking
\[\frac ab\ZZ\mapsto\left[\frac ab\right]_m.\]
Note that this makes sense because $I_\QQ^\mf m$ requires $\gcd(a,m)=\gcd(b,m)=1.$ So being in the kernel merely asserts that $\frac ab\ZZ$ has $\frac ab\equiv1\pmod m.$ That is, our kernel is
\[P_\QQ^{\mf m,1}=\left\{\frac ab\ZZ:\frac ab\equiv1\pmod m\text{ and }\frac ab>0\right\}.\]
Here the condition $\frac ab>0$ is inherited from $I_K^\mf m.$ This completes the claim.

However, we are now done by Artin reciprocity, for we know
\[H(\QQ(\zeta_m)/\QQ,\mf m)=P_\QQ^\mf m\subseteq H(K/\QQ,\mf m),\]
which is exactly what we wanted.

\subsubsection{February 16th}
Today I learned that the ray class groups are finite assuming that the normal class group is finite, from \href{https://math.stackexchange.com/questions/74206/ray-class-group}{here}. This is the first finiteness proof from an exact sequence I've seen, so it looks quite clever, though I am sure there is some more general group cohomology at play. Anyways, fix a number field $K$ and a modulus $\mf m.$ Then we claim that
\[1\longrightarrow P_K^\mf m/P_K^{\mf m,1}\longrightarrow I_K^\mf m/P_K^{\mf m,1}\longrightarrow I_K^\mf m/P_K^\mf m\longrightarrow1\]
is an exact sequence with the second and fourth terms finite. Here, $P_K^\mf m$ refers to the principal ideals in $I_K^\mf m$ while $P_K^{\mf m,1}$ are those with generators $\equiv1\pmod{\mf m}.$

Very quickly, we show how to finish the proof if we have the exact sequence and that the second and fourth terms are finite. We have that $I_K^\mf m/P_K^{\mf m,1}$ surjects onto $I_K^\mf m/P_K^\mf m,$ so the homomorphism theorem implies
\[\left|I_K^\mf m/P_K^{\mf m,1}\right|=\left|I_K^\mf m/P_K^\mf m\right|\cdot\left|\ker(I_K^\mf m/P_K^{\mf m,1}\to I_K^\mf m/P_K^\mf m)\right|.\]
However, the exact sequence tells us that $P_K^\mf m/P_K^{\mf m,1}$ bijects with this kernel, implying
\[\left|I_K^\mf m/P_K^{\mf m,1}\right|=\left|I_K^\mf m/P_K^\mf m\right|\cdot\left|P_K^{\mf m}/P_K^{\mf m,1}\right|.\]
So the finiteness of the ray class group follows.

Now we show the exact sequence. We begin with the right-hand mapping, which is
\[I_K^\mf m/P_K^{\mf m,1}\longrightarrow I_K^\mf m/P_K^\mf m.\]
Right now we need to show that this is surjective, but we're merely expanding the kernel, so it is indeed surjective: just track $\mf aP_K^\mf m\in I_K^\mf m/P_K^\mf m$ back up to $\mf aP_K^{\mf m,1}.$ Additionally, we take all principals in $P_K^\mf m$ to the identity, which is our kernel.

It remains to show that
\[P_K^\mf m/P_K^{\mf m,1}\longrightarrow I_K^\mf m/P_K^{\mf m,1}\]
injects into the kernel $P_K^\mf m$ of the next map. Well this map merely takes $(\alpha)\in P_K^\mf m$ to $(\alpha)\in I_K^\mf m,$ which is injective, and then we mod out by $P_K^{\mf m,1}.$ So indeed $P_K^\mf m\to I_K^\mf m/P_K^{\mf m,1}$ injects onto the kernel $P_K^\mf m$ exactly, and the kernel of this map is only $P_K^{\mf m,1}.$ Thus, we do have an exact sequence.

Now we have to study $P_K^\mf m/P_K^{\mf m,1}$ and $I_K^\mf m/P_K^\mf m$ a bit. We note that $I_K^\mf m/P_K^\mf m$ is actually $\op{Cl}_K.$ Indeed, we can consider the mapping
\[I_K^\mf m\longrightarrow I_K/P_K,\]
which takes a fractional ideal $\mf a\in I_K^\mf m$ to its ideal class. This mapping is surjective because, for example, theory from the class number formula showed that there are infinitely many primes in all ideal classes. So choosing any of these representative primes outside of $\mf m$ will do the trick. Of course, the kernel is the principals in $I_K^\mf m,$ which is $P_K^\mf m.$ From this it follows
\[I_K^\mf m/P_K^\mf m\cong I_K/P_K=\op{Cl}_K.\]
We note that this somewhat motivates why we define $I_K^\mf m/P_K^{\mf m,1}$ to be the ray class group instead of $I_K^\mf m/P_K^\mf m.$ Anyways, we see $I_K^\mf m/P_K^\mf m$ is finite.

It remains to understand $P_K^\mf m/P_K^{\mf m,1}.$ In order to access $1\pmod{\mf m},$ we would like to take $(\alpha)\in P_K^\mf m$ to $\alpha\in K^\times,$ but this direction is somewhat annoying because we have to track $P_K^{\mf m,1}$ later. So instead we map
\[\alpha\in(K^\mf m)^\times\longmapsto(\alpha)\in P_K^\mf m,\]
where $K^\mf m$ is the set of elements of $K$ coprime to $\mf m.$ Note that this mapping is surjective. From here we can mod out by $P_K^{\mf m,1}$ safely to give a mapping
\[(K^\mf m)^\times\longrightarrow P_K^\mf m/P_K^{\mf m,1}.\]
The kernel of this mapping is made of elements $\alpha\in K^\mf m$ that give $(\alpha)\in P_K^{\mf m,1}.$ Some care is required here because we can exchange out the generator of $(\alpha)$ by units, and its possible this changes the status of $\alpha\pmod{\mf m}.$ Surely we can say our kernel is
\[\left\{\alpha\in(K^\mf m)^\times:\alpha\mathcal O_K^\times\cap K^{\mf m,1}\ne\emp\right\},\]
where $K^{\mf m,1}$ is the set of elements of $K$ which are $1\pmod{\mf m}.$ While we're here, we simplify this a bit. We can say the kernel is $(K^\mf m)^\times\cap\mathcal O_K^\times K^{\mf m,1}$---the set of elements in $K^\mf m$ for which we can divide out (or multiply) by a unit to get into $K^{\mf m,1}.$ Further, $u\alpha\in\mathcal O_K^\times K^{\mf m,1}$ is in $K^\mf m$ if and only if
\[u\alpha\text{ is coprime to }\mf m.\]
Now, $\alpha\equiv1\pmod{\mf m},$ so $u\alpha$ is coprime to $\mf m$ if and only if $u$ is, so the only permitted units are $K^\mf m\cap\mathcal O_K^\times,$ and restricting units here is sufficient. So our kernel is $(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}.$ It follows
\[\frac{P_K^\mf m}{P_K^{\mf m,1}}\cong\frac{(K^\mf m)^\times}{(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}}.\]
It's not clear that this is finite, but that kernel turns out to be quite large. For example, it contains the subgroup $K^{\mf m,1}=(K^{\mf m,1})^\times,$ so because our groups are abelian (implying all subgroups are normal), we see
\[\frac{P_K^\mf m}{P_K^{\mf m,1}}\cong\frac{(K^\mf m)^\times/(K^{\mf m,1})^\times}{(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}/(K^{\mf m,1})^\times}.\]
In order to show finiteness, it suffices to show that the numerator here is finite. We'll return to the denominator later.

The benefit of $(K^\mf m)^\times/(K^{\mf m,1})^\times$ over $P_K^\mf m/P_K^{\mf m,1}$ is that we now get to deal with actual elements, and$\pmod{\mf m}$ behaves how we want it to. In order to tease out$\pmod{\mf m},$ we homomorphically map
\[(K^\mf m)^\times\longrightarrow\prod_{\substack{\mf p\mid\mf m\\\mf p\text{ real}}}\{\pm1\}\times\prod_{\substack{\mf p\mid\mf m\\\mf p<\infty}}\left(\mathcal O_K/\mf p^{\nu_\mf p(\mf m)}\right)^\times.\]
That is, we take $\alpha\in(K^\mf m)^\times$ to a tuple $(\alpha_\mf p)_{\mf p\mid\mf m}.$ For real primes $\mf p,$ we have $\alpha_\mf p$ equal to the sign of $\sigma_\mf p(\alpha),$ and for finite primes $\mf p,$ we know $\alpha$ is coprime to $\mf p$ by hypothesis, so it goes somewhere in $\left(\mathcal O_K/\mf p^{\nu_\mf p(\mf m)}\right)^\times.$ The kernel consists of those $\alpha\in(K^\mf m)^\times$ which give $\sigma_\mf p(\alpha/1)>0$ for real primes and $\alpha\equiv1\pmod{\mf p^{\nu_\mf p(\mf m)}},$ which by the Chinese remainder theorem is equivalent to
\[1\pmod{\mf m}.\]
Thus, we see
\[(K^\mf m)^\times/(K^{\mf m,1})^\times\cong\prod_{\substack{\mf p\mid\mf m\\\mf p\text{ real}}}\{\pm1\}\times\prod_{\substack{\mf p\mid\mf m\\\mf p<\infty}}\left(\mathcal O_K/\mf p^{\nu_\mf p(\mf m)}\right)^\times\]
and is therefore finite. This completes the proof.

We actually have the tools to compute exactly what the size of the ray class group is. We recall that
\[\left|\op{Cl}^\mf m_K\right|=\left|I_K^\mf m/P_K^\mf m\right|\cdot\left|P_K^{\mf m}/P_K^{\mf m,1}\right|\]
from (much) earlier. We now know $I_K^\mf m/P_K^\mf m\cong\op{Cl}_K,$ so its size is understood. The best we know about the final term is
\[\frac{P_K^\mf m}{P_K^{\mf m,1}}\cong\frac{(K^\mf m)^\times/(K^{\mf m,1})^\times}{(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}/(K^{\mf m,1})^\times}.\]
We deal with the numerator and denominator separately.

We already know that the numerator is
\[(K^\mf m)^\times/(K^{\mf m,1})^\times\cong\prod_{\substack{\mf p\mid\mf m\\\mf p\text{ real}}}\{\pm1\}\times\prod_{\substack{\mf p\mid\mf m\\\mf p<\infty}}\left(\mathcal O_K/\mf p^{\nu_\mf p(\mf m)}\right)^\times,\]
which is ripe to compute size directly. The $\{\pm1\}$ terms add a $2^{r_0},$ where $r_0$ is the number of real places. Then for finite primes $\mf p$ with $\nu:=\nu_\mf p(\mf m)>0,$ we take a detour: fix some $\pi\in\mf p\setminus\mf p^2,$ and we claim that we can biject elements $\alpha\in\mathcal O_K/\mf p^\nu$ with sequences $(a_0,\ldots,a_{\nu-1})\in(\mathcal O_K/\mf p)^\nu$ by
\[\alpha\equiv\sum_{k=0}^{\nu-1}a_k\pi^k\pmod{\mf p^\nu}.\]
To apply the claim to the problem, we note $(\mathcal O_K/\mf p^\nu)^\times$ forces $a_0\in(\mathcal O_K/\mf p)^\times,$ but the rest of the coordinates have free reign over $\mathcal O_K/\mf p,$ implying that we have
\[(\op N(\mf p)-1)\op N(\mf p)^{\nu-1}\]
total choices at $\mf p.$ Combining, we see we have
\[(K^\mf m)^\times/(K^{\mf m,1})^\times=2^{r_0}\cdot\prod_{\substack{\mf p\mid\mf m\\\mf p<\infty}}\op N(\mf p)^{\nu_\mf p(\mf m)}\left(1-\frac1{\op N(\mf p)}\right).\]
Aesthetically, we can think of this like $\varphi(\mf m).$

To show the claim, we can induct on $\nu.$ There is nothing to prove when $\nu=1,$ and to get $\alpha\in\mathcal O_K/\mf p^{\nu+1}$ into such a sequence, we take $(a_0,\ldots,a_{\nu-1})$ by reducing $\alpha\pmod{\mf p^\nu}.$ Then
\[\alpha-\sum_{k=0}^{\nu-1}a_k\pi^k=:A\in\mf p^\nu.\]
We need to find a unique residue $a_\nu\in\mathcal O_K/\mf p$ such that $A\equiv a_\nu\pi^\nu\pmod{\mf p^{\nu+1}}.$ Well, our options live in $\pi^\nu\mathcal O_K,$ so we need to show that $\pi^\nu\mathcal O_K+\mf p^{\nu+1}$ represents all $\mf p^\nu.$ But $+$ yields the greatest common divisor, so in fact
\[\pi^\nu\mathcal O_K+\mf p^{\nu+1}=\mf p^\nu.\]
Uniqueness of our $a_\nu\pmod{\mf p}$ follows from the fact $a_\nu\pi^\nu\equiv a'_\nu\pi^\nu\pmod{\mf p^{\nu+1}}$ implies $\mf p^{\nu+1}\mid(a_\nu-a_\nu')(\pi)^\nu,$ so $a_\nu-a_\nu'\in\mf p.$

It remains to compute the size of the denominator $(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}/(K^{\mf m,1})^\times.$ It is a general fact of groups that, for $H$ normal, $GH/H\cong G/(G\cap H),$ for we have a homomorphism $gh\in GH$ to the coset $g(G\cap H).$ This is well-defined because $g_1h_1=g_2h_2$ implies
\[g_2^{-1}g_1=h_2h_1^{-1}\in G\cap H,\]
so indeed $g_1(G\cap H)=g_2(G\cap H).$ Now, the homomorphism is surjective because in fact $G\to G/(G\cap H)$ is surjective, and its kernel consists of elements $gh$ with $g\in G\cap H.$ So the kernel contains $H$ (for $e\in G$), but $gh\in H$ always as well, implying the kernel is exactly $H.$

Applying this to the denominator, we see that
\[\frac{(K^\mf m\cap\mathcal O_K^\times)K^{\mf m,1}}{(K^{\mf m,1})^\times}\cong\frac{(K^\mf m\cap\mathcal O_K^\times)}{(K^{\mf m,1}\cap\mathcal O_K^\times)}.\]
This is good enough for our purposes, and at least it looks pretty.

Putting everything together, we see
\[\left|\op{Cl}_K^\mf m\right|=\left|\op{Cl}_K\right|\cdot2^{r_0}\cdot\prod_{\substack{\mf p\mid\mf m\\\mf p<\infty}}\op N(\mf p)^{\nu_\mf p(\mf m)}\left(1-\frac1{\op N(\mf p)}\right)\cdot\frac1{[K^\mf m\cap\mathcal O_K^\times:K^{\mf m,1}\cap\mathcal O_K^\times]}.\]
This is at least a formula, so we call it quits.

\subsubsection{February 17th}
Today I learned Ostrowski's theorem for function fields, from \href{https://kconrad.math.uconn.edu/blurbs/gradnumthy/ostrowskiF(T).pdf}{here}. The analog here is that all of our nontrivial places $|\bullet|:F(x)\to\RR$ which are trivial on $F$ (!) are equivalent to $|\bullet|_\infty$ or $|\bullet|_\pi$ for a monic irreducible $\pi\in F[x].$ Here, $|\bullet|_\infty=c^{-\deg\bullet}$ and $|\bullet|_\pi=c^{\nu_\pi(\bullet)}$ for some positive constant $c<1.$ We remark quickly that being trivial on $F$ is somewhat reasonable because, when $F$ is finite, these are our ``torsion units'' in $F[x].$ So we expect this statement to reflect the case with $\QQ$ more closely.

Anyways, fix $|\bullet|$ a valuation on $F(x).$ Note that $|\bullet|$ is multiplicative, so we can focus on its behavior on $F[x]$ because $F(x)$ is just quotients of $F[x].$ The nice thing about function fields is that $|\bullet|$ is surely nonarchimedean: the embedding $\ZZ\to F[x]$ ends up in $F,$ and $F$ gets sent to $1,$ so
\[\max_{n\in\ZZ}|n|=1<\infty\]
is bounded. So being archimedean will not differentiate between $|\bullet|_\infty$ and $|\bullet|_\pi$ as in $\QQ.$ Instead, we do casework on $|x|$ because we know $|x|_\infty>1$ while $|x|_\pi\le1.$

On one hand, we might have $|x|>1.$ Then we hope $|\bullet|$ is equivalent to $|\bullet|_\infty.$ For a polynomial $\alpha(x)=\sum_{k=0}^{\deg\alpha}a_kx^k,$ we note that being nonarchimedean implies
\[|\alpha|=\left|\sum_{k=0}^{\deg a}a_kx^k\right|\le\max_{0\le k\le\deg a}\left(|a_k|\cdot|x|^k\right).\]
Because $a_k\in F$ has $|a_k|=1,$ this is actually the maximum of $|x|^\bullet.$ Well, $|x|>1,$ so the maximum occurs (uniquely!) at $|x|^{\deg\alpha}.$ The importance of a unique maximum is that, because all triangles are isosceles, $|a|$ must be equal to $|x|^{\deg\alpha},$ our unique minimum. That is,
\[|\alpha|=(1/|x|)^{-\deg\alpha}\]
for a positive constant $1/|x|<1.$ Thus, $|\bullet|$ is equivalent to $|\bullet|_\infty,$ as desired.

On the other hand, we might have $|x|\le1.$ Then we hope $|\bullet|$ is equivalent to $|\bullet|_\pi$ for some monic irreducible $\pi\in F[x].$ In particular, we need to extract the monic irreducible $\pi,$ which we do by thinking about this like $F[x]_\pi.$ Here, $\pi$ is our uniformizer, generating our topology. So, after noting that any $F[x]$ has
\[\left|\sum_{k=0}^da_kx^k\right|\le\max_{0\le k\le d}\left(|a_k|\cdot|x|^k\right)\le1,\]
and that $|\bullet|$ cannot be trivial on all $F[x]$ (else $|\bullet|$ would be trivial), we're allowed to extract $\pi$ as an element of $F[x]$ satisfying $|\pi|<1$ with least degree. Because multiplying by an element of $F$ won't affect $|\pi|,$ we take $\pi$ monic.

We verify that $\pi$ behaves like we want. Note $\pi$ is non-constant because $|\bullet|$ is trivial on $F$---this is where this condition shows up in this case. Further, $\pi$ is irreducible, for if $\pi=\alpha\beta$ with $\deg\alpha,\deg\beta<\deg\pi,$ then $|\alpha|=|\beta|=1$ by the degree of minimality of $\pi.$ But $|\pi|<1,$ so this is impossible.

To finish, fix any $\alpha\in F[x],$ and set $\alpha=\pi^\nu\beta$ where $\nu:=\nu_\pi(\alpha).$ We would like to show that $|\alpha|=|\pi|^\nu,$ where $c:=|\pi|<1$ will be our positive constant. Well, $|\alpha|=|\pi|^\nu\cdot|\beta|,$ so it suffices to show $|\beta|=1.$ We need to use the condition $\pi\nmid\beta,$ so we use the division algorithm to write
\[\beta=q\pi+r.\]
Here, $\pi\nmid\beta$ requires $r\ne0,$ so $r$ is a nonzero polynomial with degree smaller than $\deg\pi.$ But by minimality, this means $|r|=1,$ so the inequality
\[|\beta|\le\max\{|q|\cdot|\pi|,|r|\}\]
has $|r|=1$ as a unique maximum---$|q|\cdot|\pi|\le|\pi|<1.$ So because all triangles are isosceles, $|\beta|=1.$ This completes the proof.

\subsubsection{February 18th}
Today I learned an example of representation theory in the service of combinatorics, from Artin 10.6.5. The setup here is that we have $S_n$ act on $\CC^n$ by permuting coordinates, and we're interested in decomposing this representation into irreducible representations.

We do this decomposition manually. We claim the following.
\begin{proposition}
    All $S_n$-invariant subspaces of $\CC^n$ are one of
    \[\{0\},\qquad L:=\{\langle a,\ldots,a\rangle\in\CC^n\},\qquad P:=\{\langle a_1,\ldots,a_n\rangle:a_1+\cdots+a_n=0\},\qquad\CC^n.\]
\end{proposition}
Quickly, we check that these are actually $S_n$-invariant. Note that permuting the coordinates of $0$ or $\langle a,\ldots,a\rangle$ fixes the vector, so $\{0\}$ and $L$ are fixed. And if $\langle a_1,\ldots,a_n\rangle\in P,$ then for any $\sigma\in S_n,$ we have $\langle a_{\sigma1},\ldots,a_{\sigma n}\rangle$ satisfies
\[a_{\sigma1}+\cdots+a_{\sigma n}=a_1+\cdots+a_n=0,\]
so $\langle a_{\sigma1},\ldots,a_{\sigma n}\rangle\in P.$ Thus, $S_n$ also fixes $P.$ And of course $S_n$ fixes the entire space $\CC^n.$

We now show that these are the only subspaces; fix $V$ to be $S_n$-invariant. We begin by getting rid of trivial cases. If the only vector is $0,$ then $V=\{0\}.$ If all vectors in $V$ have all equal coordinates, and $V$ has a nonzero vector, then we can scale the nonzero vector to force $V\supseteq\{\langle a,\ldots,a\rangle\in\CC^n.$ But then these are all the vectors with equal coordinates, so $V=L.$

Otherwise, we have a vector $v\in S_n$ with not all coordinates equal; applying a suitable permutation in $S_n$ lets us assume that $v=\langle v_1,\ldots,v_n\rangle$ has $v_1\ne v_2.$ Transpositions are in $S_n,$ so we also know
\[\hat v=\langle v_2,v_1,v_3,v_4,\ldots,v_n\rangle\in V.\]
In particular,
\[\frac{v-\hat v}{v_1-v_2}=\langle1,-1,0,0,\ldots,0\rangle\in V.\]
So we have $e_1-e_2\in V,$ and applying cyclic shifts tells us $e_k-e_{k+1}\in V$ for $k\in[1,n).$ But this tells us $P\subseteq V$: taking $\langle a_1,\ldots,a_n\rangle\in P,$ we have
\[\langle a_1,\ldots,a_n\rangle=\sum_{k=1}^{n-1}\left(\sum_{\ell=1}^ka_\ell\right)(e_k-e_{k+1}).\]
Indeed, for $k<n$ we get the subtraction of two consecutive sums to give $a_ke_k,$ and at $k=n,$ we get $-(a_1+\cdots+a_{n-1})e_n,$ which is $a_n$ because $\langle a_1,\ldots,a_n\rangle\in P$ has coordinate sum $0.$ To finish, either $V\subseteq P,$ so $V=P,$ or $V$ has a vector $w$ outside of $P,$ in which case $P\cap\CC w=\{0\},$ and so
\[\dim V\ge\dim(P\oplus\CC w)=\dim P+\dim\CC w=(n-1)+1=n.\]
It follows $\dim V=n,$ so in fact $V=\CC^n.$ This completes the classification of $S_n$-invariant subspaces. $\blacksquare$

From here it is quick to decompose this representation into irreducible representations. Using our $S_n$-invariant subspaces, we label our representation $\rho:S_n\to\op{GL}_n(\CC)$ and let $\rho_L$ and $\rho_P$ be its restrictions to $L$ and $P$ respectively. (These are well-defined because $L$ and $P$ are $S_n$-invariant.) Further, $\CC^n=L\oplus P$ (count dimensions), so we see
\[\rho=\rho_L\oplus\rho_P.\]
We claim that these are irreducible. Indeed, because of our classification of $S_n$-invariant subspaces, we see that the only $S_n$-invariant subspaces of $L$ is either $\{0\}$ or $L,$ neither of which give us a useful decomposition. The same holds for $P.$ Thus, the above is our decomposition into irreducibles.

Let's use this to do some combinatorics. Because $\rho$ basically makes permutation matrices, we see that $g\in G$ makes $\op{trace}(\rho\sigma)$ equal to the number of $1$s along the diagonal, which is the number of elements fixed by $\sigma.$ That is, our character $\chi$ has
\[\chi(\sigma)=|\{n:\sigma n=n\}|.\]
Now, using our inner product, we see
\[\langle\chi,\chi_L\rangle=1.\]
But we can compute the inner product manually, for $\rho_L$ is the trivial representation: for any $\sigma,$ $\rho_L\sigma$ fixes everything in $L.$ So we write
\[\langle\chi_L,\chi\rangle=\frac1{n!}\sum_{\sigma\in S_n}\chi(\sigma)\overline{\chi_L(\sigma)}=\frac1{n!}\sum_{\sigma\in S_n}\chi(\sigma).\]
Using what we know about $\chi(\sigma),$ we see that this is implies
\[\mathbb E[\text{number of fixed points}]=1.\]
Of course, this follows quickly from linearity of expectation, but it's amusing that we got here from representation theory. Similarly, we can compute
\[2=\langle\chi,\chi\rangle=\frac1{n!}\sum_{\sigma\in S_n}|\chi(\sigma)|^2.\]
From this it follows
\[\mathbb E\left[(\text{number of fixed points})^2\right]=2,\]
which is less trivial to prove, so the ease by which representation theory got here is more impressive.

We do remark that we can actually compute $\mathbb E\left[(\text{number of fixed points})^2\right]$ using linearity of expectation. For a random permutation, we (as usual), let $X_k$ by the event that $k$ is fixed by the permutation. We want
\[\mathbb E\left[\left(\sum_{k=1}^nX_k\right)^2\right]=\mathbb E\left[\sum_{k=1}^nX_k^2+2\sum_{\substack{k,\ell=1\\k\ne\ell}}^nX_kX_\ell\right].\]
To be clear, the trick here is to believe that this expansion is useful. From here, we use linearity of expectation tells us we want
\[\sum_{k=1}^n\mathbb E\left[X_k^2\right]+2\sum_{\substack{k,\ell=1\\k\ne\ell}}^n\mathbb E[X_kX_\ell].\]
Now, $X_k$ is either $0$ or $1,$ so $X_k^2=X_k,$ so $\mathbb E[X_k^2]=1/n.$ So
\[\sum_{k=1}^n\mathbb E\left[X_k^2\right]=1.\]
As for $X_kX_\ell,$ this is equal to $1$ if and only if both $k$ and $\ell$ are fixed, which occurs with probability $\frac1n\cdot\frac1{n-1},$ which is our $\mathbb E[X_kX_\ell].$ Thus,
\[\sum_{\substack{k,\ell=1\\k\ne\ell}}^n\mathbb E[X_kX_\ell]=n(n-1)\cdot\frac1{n(n-1)}=1.\]
Combining, we see
\[\mathbb E\left[(\text{number of fixed points})^2\right]=1+1=2,\]
as desired.

Quickly, we say that the fact we have combinatorial proofs of those identities could actually be read backwards to prove the classification of $S_n$-invariant subspaces. Indeed, we claim $L$ and $P$ have no nontrivial $S_n$-invariant subspaces. Knowing $\langle\chi,\chi,\rangle=2$ tells us that $\rho$ is the direct sum of two irreducible representations; we can check $S_n$ is trivial on $L,$ so the trivial representation $\rho_L$ is one of the irreducibles, and
\[\langle\chi,\chi_L\rangle=1\]
tells us that there is only one other irreducible. Well, we also check that $P$ is $S_n$-invariant and that $\CC^n=L\oplus P,$ so we are forced to conclude
\[\rho=\rho_L\oplus\rho_P\]
is a decomposition into irreducibles. Thus, $L$ and $P$ cannot have any nontrivial $S_n$-invariant subspaces, which is what we wanted.

\subsubsection{February 19th}
Today I learned about the topology of profinite groups. A profinite group is the inverse limit of a family of finite groups; that is, for a directed set (proset where finite subsets are upper-bounded) $\mathcal I$ and a contravariant functor $F$ from $\mathcal I$ to $\texttt{Grp}$ with image only finite groups, we take
\[G:=\varprojlim_{\mathcal I}F(\bullet)\]
as our profinite group. In practice, we realize $G$ by writing $G_k:=F_k$ for $k\in\mathcal I$ and $\varphi_{\ell k}:G_\ell\to G_k$ for the morphism when $k\le\ell.$ Then we can define $G$ by coherent sequences of the $G_\bullet$ by writing
\[G:=\left\{(g_\bullet)\in\prod_\mathcal IG_\bullet:k\le\ell\implies\varphi_{\ell k}(g_k)=g_\ell\right\}.\]
Note the morphisms $\varphi_{\ell k}$ are best read from right to left. Our prototypical example of this construction is $\ZZ_p.$ We remark that it is a fact that this $G$ is non-empty when the $G_\bullet$ is non-empty, but this is nontrivial to prove.

Now we provide a topology to this. The $G_\bullet$ are finite (nonempty) groups, so we give them the discrete topology. Then we note our coherent sequences lets us think
\[G\subseteq\prod_\mathcal IG_\bullet,\]
so we first give $\prod_\mathcal IG_\bullet$ the product (``finite gate'') topology, and second give $G$ the subspace topology from this.

From this presentation, the topology on $G$ feels somewhat contrived, but it has some nice properties. Doing fact collection, we quickly remark that $G$ is Hausdorff because it is the subspace of a Hausdorff space, where $\prod_\mathcal IG_\bullet$ is Hausdorff because it is the product of Hausdorff spaces. Additionally, $G$ is closed in $\prod_\mathcal IG_\bullet$ because its complement is
\[G^c=\left\{(g_\bullet)\in\prod_\mathcal IG_\bullet:\exists k,\ell,k\le\ell,\varphi_{\ell k}(g_k)\ne g_\ell\right\},\]
which rearranges to
\[G^c=\bigcup_k\bigcup_{k\le\ell}\left\{(g_\bullet)\in\prod_\mathcal IG_\bullet:\varphi_{\ell k}(g_k)\ne g_\ell\right\}.\]
This last set is open because it is the union of various $\{(g_\bullet):\varphi_{\ell k}(g_k)\ne g_\ell\},$ a condition which only focuses on $G_k$ and $G_\ell.$ In particular, the $G_k$ and $G_\ell$ components of this set will be ugly---but open because their topology is discrete---while the other $G_\bullet$ components have all $G_\bullet.$ This means our sets are open in the product topology, so $G^c$ is open, so $G$ is closed.

Finally, our final piece of fact collection is that $G$ is compact, for it is a closed set in a compact space, where $\prod_\mathcal IG_\bullet$ is compact by Tychonoff's theorem. I don't actually know a proof of either of these facts (being closed means compact or Tychonoff's), but it won't affect my sleep schedule.

Anyways, what's really nice about profinite groups is that we have an equivalent topological definition: a topological group is profinite if and only if it is compact and totally disconnected. As usual, think $\mathbb Z_p.$ We recall that a space is connected if and only if it cannot be decomposed into two disjoint open subsets, and totally disconnected means that points are the largest connected subspaces.

Currently I know only one direction of this proof currently, where $G$ being profinite implies totally disconnected. Our definition of totally disconnected focuses on open sets, so it suffices to look only at the connected component of the identity $e\in G.$ Name this component $G^\circ,$ and we would like to show that $G^\circ=\{e\}.$

Quickly, we note it is generally true that $G^\circ$ is a subgroup of $G,$ in any topological group. Indeed, $g\in G^\circ$ gives $g^{-1}G^\circ$ connected and containing $e,$ so
\[g^{-1}\in g^{-1}G^\circ\subseteq G^\circ,\]
so $G^\circ$ is closed under inverses. Doing the same for $gG^\circ$ tells us $G^\circ$ is closed under products as well, so $G^\circ$ is indeed a subgroup.

Returning to the proof, the key trick here is to look at open subgroups. In particular, open subgroups of $G$ we expect to have ``substance,'' in that because multiplication is continuous, we expect to be able to ``wiggle'' $e$ around inside of an open subgroup. So intuitively, we expect $G^\circ$ to be inside every open subgroup. To show this, we use the fact $G^\circ$ is connected: fix any open subgroup $U,$ and look at
\[V=\bigcup_{x\in G^\circ\setminus U}x(G^\circ\cap U).\]
Note $V$ is open in $G^\circ$ because $G^\circ\cap U$ is open in $G^\circ$; in fact $V\subseteq G^\circ$ because $G^\circ$ is a subgroup. Because $e\in G^\circ\cap U,$ we see $G^\circ\setminus U\subseteq V,$ so $G^\circ$ splits into open sets $G^\circ\cap U$ and $V.$ And finally, $G^\circ\cap U$ and $V$ are disjoint because if $gu\in V$ with $u\in G^\circ\cap U$ also has $gu\in G^\circ\cap U,$ then $g\in G^\circ\cap U.$

Finishing up here, $G^\circ\cap U$ and $V$ decompose $G^\circ$ into disjoint open sets, so one of them must be empty. Certainly $e\in U,$ so $V$ must in fact be empty. But this requires $G^\circ\setminus U$ to be empty, so indeed $G^\circ\subseteq U.$

Now we know $G^\circ$ to be quite small, inside of every open subgroup of $G.$ This is not quite good enough to deduce that $G^\circ=\{e\}$---we haven't even used the fact $G$ is profinite yet! But we can finish now. Fix any $g\in G\setminus\{e\},$ which we label $(g_\bullet)$ to get us thinking profinitely. We want to know $g\not\in G^\circ,$ which we can show $x$ not in some open subgroup of $G.$

Well, $(g_\bullet)\ne e$ cannot be the identity on all coordinates, so fix $g_k\ne e_k\in G_k.$ Then we look at
\[U=\{(x_\bullet)\in G:x_k=e_k\}.\]
Note $U$ is open, for all but the $k$th coordinate is $G_\bullet,$ and $\{e_k\}$ is an open set in $G_k.$ Additionally, being the identity on one coordinate is a property closed under inversion and multiplication, so $U$ is also a subgroup. Thus, $G^\circ\subseteq U,$ but $U$ doe not contain $g,$ so $G^\circ$ doesn't either. This completes the proof.

\subsubsection{February 20th}
Today I learned about the Mellin transformation, from \href{http://dsp-book.narod.ru/TAH/ch11.pdf}{here} mostly. We take the following definition.
\begin{definition}
    For $f:\CC\to\CC,$ the Mellin transform of $f$ is defined as
    \[\{\mathcal Mf(t)\}(s):=\int_{\RR^+}f(t)t^{s-1}\,dt.\]
\end{definition}
\href{https://terrytao.wordpress.com/2008/07/27/tates-proof-of-the-functional-equation/}{Terrence Tao} mentions this is like a ``multiplicative Fourier transform,'' which we can already see in the $\RR^+.$ Quickly, we'll be a bit more explicit with this. Rewrite the definition as
\[2\{\mathcal Mf(t)\}(s)=\int_{\RR^\times}f(t)|t|^s\,\frac{dt}{|t|}.\]
So we claim $2\{\mathcal Mf(t)\}$ is the Fourier transform of $f(t):\RR^\times\to\CC.$ Note $dt/|t|$ is a multiplicative Haar measure on $\RR^\times,$ for $t\mapsto|c|t$ gives $dt/|t|\mapsto dt/|t|.$ Additionally, our character group $\RR^\times\to\CC$ is $|t|\mapsto |t|^s$; we don't show this here, but we do remark that it makes our desired (abstract) Fourier transform as above.

There are also some concrete connections to the Fourier transform. For clarity, we define the Fourier transform by
\[\{\mathcal Ff(t)\}(s):=\int_\RR f(t)e^{-2\pi its}\,dt.\]
To translate from $\mathcal M$ to $\mathcal F,$ we begin by applying the variable change $t\mapsto e^{-u}$ with $dt\mapsto-e^{-u}\,du.$ This gives
\[\{\mathcal Mf(t)\}(s)=\int_\infty^{-\infty}f\left(e^{-u}\right)e^{-us}e^u\cdot-e^{-u}\,du=\int_{-\infty}^\infty f\left(e^{-u}\right)e^{-us}\,dt.\]
This is almost the Fourier transform (actually this is the ``two-sided Laplace transform''). To finish, we fix $s=a+2\pi ib,$ which implies
\[\{\mathcal Mf(t)\}(s)=\int_{-\infty}^\infty\left(f\left(e^{-u}\right)e^{-ua}\right)e^{-2\pi iub}\,du=\left\{\mathcal Ff\left(e^{-u}\right)e^{-ua}\right\}(b).\]
I agree---this is a bit ugly. Regardless, I think we can see the multiplicativity hiding in the $e^{-u}.$

However, this relationship is somewhat useful. For example, we get an inversion formula for free from Fourier inversion. In particular, we recall
\[f(t)=\int_\RR\{\mathcal Ff(t)\}(s)e^{2\pi its}\,dt,\]
which when applied to $\{\mathcal Mf(t)\}$ gives
\[f\left(e^{-u}\right)e^{-ua}=\int_{-\infty}^\infty\{\mathcal Mf(t)\}(s)e^{2\pi iub}\,db.\]
Rearranging, this is
\[f\left(e^{-u}\right)=\int_{-\infty}^\infty\{\mathcal Mf(t)\}(s)e^{u(a+2\pi ib)}\,db.\]
We would like to integrate over $s=a+2\pi ib,$ but some care is required because we are integrating $db.$ Interpreting this as a variable change, we see
\[f\left(e^{-u}\right)=\frac1{2\pi i}\int_{a-i\infty}^{a+i\infty}\{\mathcal Mf(t)\}(s)e^{us}\,ds.\]
At this point, the $t=e^{-u}$ substitution is no longer helpful, so we rewrite this as
\[f(t)=\frac1{2\pi i}\int_{a-i\infty}^{a+i\infty}\{\mathcal Mf(t)\}(s)t^{-s}\,ds.\]
This is our inversion formula. Note the introduction of complex numbers. Also note that we are ignoring convergence issues, as usual.

I guess I should actually do an example. I think the example I'm supposed to care about is
\[\left\{\mathcal Me^{-t}\right\}(s)=\int_0^\infty e^{-t}t^{s-1}\,dt.\]
However, this is exactly $\Gamma(s),$ which is nice. In general, if we do $\left\{\mathcal Me^{-pt}\right\}$ for $p>0,$ then we get
\[\left\{\mathcal Me^{-pt}\right\}(s)=\int_0^\infty t^{s-1}e^{-pt}\,dt.\]
To make $\Gamma$ appear now, we have to do a variable transformation $pt\mapsto t.$ This gives
\[\left\{\mathcal Me^{-pt}\right\}(s)=\int_0^\infty\frac{t^{s-1}}{p^{s-1}}e^{-t}\,\frac{dt}p=\frac{\Gamma(s)}{p^s}.\]
I guess this feels multiplicative, in that taking exponents amounts to a multiplication factor.

In other news, happy birthday to me I guess.

\subsubsection{February 21st}
Today I learned the proof of the functional equation for the Riemann $\zeta$ function, from \href{http://math.mit.edu/~poonen/786/notes.pdf}{here}. We recall the definition of the ``completed'' Riemann $\zeta$ function, named
\[\xi(s):=\pi^{-s/2}\Gamma(s/2)\zeta(s),\]
where the $\Gamma$ function is defined as usual by
\[\Gamma(s):=\int_0^\infty e^{-t}t^s\,\frac{dt}t.\]
We note that the above definition only works for $\op{Re}(s)>0,$ but the we have a reflection formula we proved a while ago, exhibiting an analytic continuation of $\Gamma$ to all of $\CC$ with simple poles at $-1,-2,\ldots.$ Anyways, we show the following.
\begin{theorem}
    We have that $\xi(s)=\xi(1-s).$
\end{theorem}

I'm not sure where the completion of $\zeta$ comes from, but we can motivate the rest of the argument from here. We use the definition
\[\zeta(s)=\sum_{n=1}^\infty\frac1{n^s},\]
which holes for $\op{Re}(s)>1,$ and we will gradually continue this to $\CC$ as is possible. A single term $n^{-s}$ in the sum makes
\[\pi^{-s/2}\Gamma(s/2)n^{-s}\]
in $\xi.$ (This still holds for $\op{Re}(s)>1$ because we've merely factored into the sum.) Simplifying, this term is
\[\left(\pi n^2\right)^{-s/2}\int_0^\infty e^{-t}t^{s/2}\,\frac{dt}t.\]
Taking $t\mapsto t\pi n^2$ so that $dt/t\mapsto dt/t,$ this becomes
\[\int_0^\infty e^{-\pi n^2t}t^{s/2}\,\frac{dt}t.\]
Summing over all $n,$ we see that
\[\xi(s)=\sum_{n=1}^\infty\int_0^\infty e^{-\pi n^2t}t^{s/2}\,\frac{dt}t.\]
Note that we are still working over $\op{Re}(s)>1.$ We would like to interchange the sum and integral, which we do by testing absolute convergence (via Fubini's). Indeed, we know that the integral term is $\pi^{-s/2}\Gamma(s/2)n^{-s}$ because it's just from $\xi,$ and $\xi$'s sum converges absolutely for $\op{Re}(s)>1.$ Explicitly,
\[\sum_{n=1}^\infty\left|\frac1{n^s}\right|=\sum_{n=1}^\infty\frac1{n^{\op{Re}(s)}},\]
which converges for $\op{Re}(s)>1$ by the $p$-series test or something.

Anyways, we get to say
\[\xi(s)=\int_0^\infty\left(\sum_{n=1}^\infty e^{-\pi n^2t}\right)t^{s/2}\,\frac{dt}t\]
for $\op{Re}(s)>1.$ It might appear like we've made no progress, but in fact this integral is more malleable than $\xi$ because the sum $\sum e^{-\pi n^2t}$ is much better-behaved.

In particular, fix
\[\Theta(t)=\sum_{n\in\ZZ}e^{-\pi n^2t}\]
so that $\frac{\Theta(t)-1}2=\sum e^{-\pi n^2t}.$ This $\Theta$ can be understood by the Poisson summation formula to get a functional equation for $\Theta.$ We claim the following.
\begin{lemma}
    We have that
    \[\Theta(t)=\frac1{\sqrt t}\Theta\left(\frac1t\right).\]
\end{lemma}
What's nice here is that $f(x)=e^{-\pi x^2}$ has its own Fourier transform: $\hat f=f.$ (We do not show this here, but a proof exists \href{https://math.stackexchange.com/questions/270566/how-to-calculate-the-fourier-transform-of-a-gaussian-function}{here}.) So if we let $f_t(x)=e^{-\pi x^2t}=f(x\sqrt t),$ then
\[\hat f_t(s)=\int_\RR f(x\sqrt t)e^{-2\pi ixs}\,dx=\int_\RR f(x)e^{-2\pi ix(s/\sqrt t)}\,\frac{dx}{\sqrt t}=\frac1{\sqrt t}\hat f\left(\frac s{\sqrt t}\right).\]
Thus, $\hat f_t(s)=\frac1{\sqrt t}f\left(\frac x{\sqrt t}\right)=\frac1{\sqrt t}e^{-\pi x^2/t}.$ Now, the Poisson summation formula tells us
\[\sum_{n\in\ZZ}f_t(n)=\sum_{n\in\ZZ}\hat f_t(n),\]
which expands into
\[\Theta(t)=\sum_{n\in\ZZ}e^{-\pi n^2t}=\sum_{n\in\ZZ}\frac1{\sqrt t}e^{-\pi n^2/t}=\frac1{\sqrt t}\Theta\left(\frac1t\right).\]
This is our functional equation for $\Theta.$ We remark that, with more work, one can use this to show $\Theta$ is a modular form with ``weight $1/2$.''

Now we return to $\xi.$ For $\op{Re}(s)>1,$ we know
\[\xi(s)=\int_0^\infty\left(\frac{\Theta(t)-1}2\right)t^{s/2}\,\frac{dt}t.\]
We would like to extend this integral to all of $\CC,$ but the integral explodes with $s=1,$ so it's not obvious how to do this. (In fact, $\xi$ explodes at $s=0$ and $s=1,$ so some care is required.) To proceed, the trick is to split the integral to $(0,1)$ and $(1,\infty).$ On $(1,\infty),$ the integral is well-behaved over all $\CC,$ and $(0,1),$ we can use the functional equation we just established to turn the integral into the one over $(1,\infty).$

We make this explicit. Over $(1,\infty),$ we claim that
\[I(s):=\int_1^\infty\left(\frac{\Theta(t)-1}2\right)t^{s/2}\,\frac{dt}t\]
defines an entire function on $\CC.$ It suffices to show that this converges everywhere, which holds because, for $t>0,$
\[\frac{\Theta(t)-2}2=\sum_{n=1}^\infty e^{-\pi n^2t}<\sum_{n=1}^\infty\left(e^{-\pi t}\right)^n=\frac{e^{-\pi t}}{1-e^{-\pi t}}<e^{-\pi t}.\]
Thus,
\[|I(s)|<\int_1^\infty e^{-\pi t}t^{\op{Re}(s)/2-1}\,dt.\]
Now, $t^\bullet=O\left(e^t\right)$ for $t>1$ (say, by L'Hospital's rule on $t^\bullet\le t^{\ceil{\bullet}}$), so
\[|I(s)|=O\left(\int_1^\infty e^{(1-\pi)t}\,dt\right)=O(1).\]
This is what we wanted.

Now we deal with $(0,1).$ To coerce our integral into $(1,\infty),$ we make the substitution $t\mapsto1/t$ with $dt/t=-dt/t,$ giving
\[\int_0^1\left(\frac{\Theta(t)-1}2\right)t^{s/2}\,\frac{dt}t=\int_\infty^1\left(\frac{\Theta(1/t)-1}2\right)t^{-s/2}\cdot-\frac{dt}t.\]
This rearranges to
\[\int_1^\infty\left(\frac{\Theta(1/t)-1}2\right)t^{-s/2}\,\frac{dt}t.\]
We are now ripe to use the functional equation of $\Theta,$ which tells us this is
\[\int_1^\infty\left(\frac{\Theta(t)\sqrt t-1}2\right)t^{-s/2}\,\frac{dt}t.\]
Coercing the $(1,\infty)$ integral out of this, we add and subtract $\int_1^\infty\sqrt t\cdot t^{-s/2}\,\frac{dt}t$ (which is safe over $\op{Re}(s)>1$), giving
\[\int_1^\infty\left(\frac{\Theta(t)-1}2\right)\sqrt t\cdot t^{-s/2}\,\frac{dt}t+\int_1^\infty\frac{\sqrt t}2\cdot t^{-s/2}\,\frac{dt}t+\int_1^\infty-\frac12\cdot t^{-s/2}\,\frac{dt}t.\]
These integrals evaluate to
\[I(1-s)+\frac12\cdot\frac{t^{(1-s)/2}}{(1-s)/2}\bigg|_1^\infty-\frac12\cdot\frac{t^{-s/2}}{(-s/2)}\bigg|_1^\infty.\]
Thus, we have
\[\int_0^1\left(\frac{\Theta(t)-1}2\right)t^{s/2}\,\frac{dt}t=I(1-s)-\frac1{1-s}-\frac1s.\]
Note that $I(1-s)$ is legal for $\op{Re}(s)>1$ because we showed $I$ is entire.

Bringing this all together, we see that, for $\op{Re}(s)>1,$
\[\xi(s)=\int_0^\infty\left(\frac{\Theta(t)-1}2\right)t^{s/2}\,\frac{dt}t=I(s)+I(1-s)-\frac1{1-s}-\frac1s.\]
However, because $I$ is entire, this final expression is therefore an analytic continuation of $\xi$ to all of $\CC$ with simple poles at $0$ and $1.$ By extension, we can divide out by the $\pi^{-s/2}\Gamma(s/2)$ factor to give $\zeta$ its meromorphic continuation to all of $\CC.$ We close by saying the symmetry of $\xi$'s analytic continuation implies
\[\xi(s)=\xi(1-s).\]
This is what we wanted. $\blacksquare$

Expanding this out to $\zeta$ gives it a functional equation. I think some care with $\Gamma$ is required to give $\zeta$'s functional equation in its usual presentation, but whatever.

\subsubsection{February 22nd}
Today I learned about the $p$-adic Gaussian from \href{https://terrytao.wordpress.com/2008/07/27/tates-proof-of-the-functional-equation/}{Terrence Tao}. I think this is an instance of Pontryagin duality, but I don't know enough theory to say for sure. Anyways, the main character in our story is (fittingly) the character $\QQ_p\to\CC$ by
\[e_p(x)=e^{2\pi i\{x\}_p},\]
where $\{x\}_p$ is the $p$-adic fractional part of $x\in\QQ_p.$ Because $\QQ_p$ is locally compact (and abelian), we get an additive Haar measure, and we normalize it so that $\int_{\ZZ_p}dx_p=1.$ From this character we get a Fourier transform, which we denote
\[\hat f(s)=\int_{\QQ_p}f(x_p)e_p(-sx_p)\,dx_p.\]
Note that we're assuming stuff that I can't prove currently.

We're trying to build the machinery we had in Riemann's proof of the functional equation of $\zeta.$ We have an $\exp$ in $e_p,$ and now we would like to build a Gaussian $G_p$ like $G_\infty=e^{-\pi x^2}$ for $\RR.$ This function should be its own Fourier transform, so
\[G_p(s)=\int_{\QQ_p}G_p(x_p)e_p(-sx_p)\,dx_p.\]
We claim that the indicator function on $\ZZ_p$ works.
\begin{proposition}
    Let $G_p:\QQ_p\to\RR$ be $1$ on inputs in $\ZZ_p$ and $0$ otherwise. Then
    \[G_p(s)=\int_{\QQ_p}G_p(x_p)e_p(-sx_p)\,dx_p.\]
\end{proposition}
We're going to be obnoxiously careful because integration on $\QQ_p$ is somewhat new to me. We focus on the integral. Note $x_p\in\QQ_p\setminus\ZZ_p$ gives $G_p(x_p)=0,$ vanishing entirely, so we're left integrating $x_p\in\ZZ_p,$ which looks like
\[G_p(s)\stackrel?=\int_{\ZZ_p}e_p(-sx_p)\,dx_p.\]
Because we have the words Fourier and exponential and indicator floating around, we hope to turn this into a sum of roots of unity/discrete Fourier transform. In particular, we need to make the integral better-behaved. Fix $s=p^{-\nu}t$ with $t\in\ZZ_p\setminus p\ZZ_p,$ and we split up $\ZZ_p$ by$\pmod{p^\nu}.$ (These sets countably and disjointly partition $\ZZ_p.$) This looks like
\[\int_{\ZZ_p}e_p(-sx_p)\,dx_p=\sum_{k\in\ZZ/p^\nu\ZZ}\int_{x_p\equiv k}e_p(-sx_p)\,dx_p.\]
Now, for $x_p\equiv k\pmod{p^\nu},$ we get to write $x_p=k+\ell p^\nu$ for $\ell\in\ZZ_p,$ implying
\[\{sx_p\}_p=\left\{p^{-\nu}tk+t\ell\right\}_p=\left\{p^{-\nu}tk\right\}_p.\]
In particular, this is constant with respect to $k,$ so we are now proving
\[G_p(s)\stackrel?=\sum_{k\in\ZZ/p^\nu\ZZ}e^{2\pi i\{-p^{-\nu}tk\}_p}\int_{x_p\equiv k}dx_p.\]
At this point, we can remove the integral. The measures of $\{x_p\in\ZZ_p:x_p\equiv k\}$ must be equal for each $k$ because adding $1$ cyclically shifts them, and the Haar measures is fixed under such shifts. Because the total of all these $p^\nu$ cosets is $1,$ each individual coset must have measure $1/p^\nu.$ So it remains to prove
\[G_p(s)\stackrel?=\frac1{p^\nu}\sum_{k\in\ZZ/p^\nu\ZZ}e^{2\pi i\{-p^{-\nu}tk\}_p}.\]
Note $\left\{-p^{-\nu}tk\right\}_p=(-tk\bmod{p^\nu})/p^\nu,$ but $p\nmid t,$ so $-tk$ ranges over $\ZZ/p^\nu\ZZ$ as $k$ does. Thus, we want to show
\[G_p(s)\stackrel?=\frac1{p^\nu}\sum_{k\in\ZZ/p^\nu\ZZ}e^{2\pi ik/p^\nu}.\]
Only now do we remark that $s\in\ZZ_p$ implies that $\nu=0,$ in which case the sum degenerates to just a $1.$ Otherwise $s\in\QQ_p\setminus\ZZ_p,$ implying $\nu\ge1,$ so we are summing the $p^\nu$th roots of unity, which does come out to $0.$ So our Gaussian works. $\blacksquare$

\subsubsection{February 23rd}
Today I learned about the Zariski topology. I think the first definition came from algebraic varieties. If you have an affine space (say, over $\CC$) which we'll name $\AA^n,$ then the Zariski topology is defined by its closed sets
\[V(S)=\left\{x\in\AA^n:f\in S\implies f(x)=0\right\},\]
where $S$ is a set of polynomials in $n$ variables. Note that if $f$ and $g$ are polynomials, then $f(x)=0$ and $g(x)=0$ is equivalent to $af(x)+bg(x)=0$ for all $a,b\in\AA.$ Thus, the closed set generated by $S$ is equal to the closed set generated by the ideal generated by $S.$ So we consider ideals $I$ of polynomials in $n$ variables from here on.

We can quickly check that this is indeed a topology. For the easy stuff, note that $S=\emp$ gives $V(S)=\AA^n,$ and $S=\{1\}$ gives $V(S)=\emp.$ So we see that $\emp$ and $\AA^n$ are closed sets.

The intersection $V(I)\cap V(J)$ is extracting points which are $0$ on everyone in $I$ and everyone in $J,$ so we claim
\[V(I)\cap V(J)=V(I+J).\]
Indeed, $x\in V(I+J)$ if and only if $x$ is $0$ on every polynomial $af(x)+bg(x)$ for $f,g\in I,J$ and $a,b\in\AA.$ Setting $a=0$ and then $b=0$ tells us this occurs if and only if $x$ is $0$ on every polynomial $f\in I$ and every polynomial $g\in J,$ which is what we wanted. Generalizing, the same argument tells us that for a collection $\{I_\alpha\}_{\alpha\in\lambda}$
\[\bigcap_{\alpha\in\lambda}V(I_\alpha)=V\left(\sum_{\alpha\in\lambda}I_\alpha\right).\]
So our closed sets are closed under arbitrary intersection.

Similarly, the union $V(I)\cup V(J)$ is extracting points which are $0$ on either everyone in $I$ or everyone in $J.$ This is a bit fuzzier, but we claim
\[V(I)\cup V(J)=V(IJ).\]
If $x\in V(I)\cup V(J),$ then $x\in V(IJ),$ for $IJ$ is generated by products $fg$ with $f\in I$ and $g\in J,$ and we must have $(fg)(x)=0.$ Conversely, if $x\not\in V(I)\cup V(J),$ then there exists a polynomial $f\in I$ and a polynomial $g\in J$ such that
\[f(x)\ne0\quad\text{and}\quad g(x)\ne0.\]
Thus, $(fg)(x)\ne0$ with $fg\in IJ,$ so $x\not\in V(IJ).$ So the claim follows. Inducting, we see that for a collection $\{I_n\}_{n\in\NN}$
\[\bigcup_{n\in\NN}V(I_n)=V\left(\prod_{n\in\NN}I_n\right).\]
Note that this argument had to use a contrapositive to get the second inclusion, which I think is why we don't have arbitrary union.

Of course, the reason why I care about the Zariski topolgy is the spectrum of a ring. With this in mind, we state explicitly that we are looking towards a ring-theoretic Zariski topology. I think the motivation here comes from looking at the case $\AA^n=\CC^1.$ (I think we should be looking at $\CC P^1,$ but I don't know enough algebraic geometry to be able to tell the difference.) In this case, our closed sets are
\[V(S)=\{\alpha\in\CC:f\in S\implies f(\alpha)=0\}\]
for $S\subseteq\CC[t].$ Now, we remark that all polynomials over $\CC[t]$ factor, so for $f\in\CC[t],$ we see $V(f)$ is just the zeroes of $f.$ So our topology is ``really'' generated closed sets consisting of finitely many points. Of course, simple factorization in $\CC[t]$ doesn't generalize well, but ideal factorization does, where we can still write
\[(f(t))=\prod_{f(\alpha)=0}(t-\alpha).\]
Here $(t-\alpha)$ are the prime ideals of $\CC[t],$ and they both associate with $\CC$ and encode our closed sets. At this point, writing
\[V(I)=\{(t-\alpha):I\subseteq(t-\alpha)\}\]
turns our definition into an entirely ring-theoretic one. Note that this is pretty much the same Zariski topology over $\CC,$ but now we're encoding points $\alpha\in\CC$ with their ideals $(t-\alpha).$ With this in mind, here's our definition.
\begin{definition}
    For a commutative ring $R,$ we define the spectrum of a ring $\op{Spec}R$ with points the prime ideals of $R$ and a topology where
    \[V(I)=\{\mf p\subseteq R:I\subseteq\mf p\}\]
    is a closed set for each ideal $I\subseteq R.$
\end{definition}
We provide the check that this is a topology to hammer home that this really deserve the name ``Zariski topology.'' Notably, these proofs are pretty much identical. Again, $I=(0)$ means $\op{Spec}R$ is closed, and $I=R$ means $\emp$ is closed.

For intersections, we still have, for a collection $\{I_\alpha\}_{\alpha\in\lambda},$ that
\[\bigcap_{\alpha\in\lambda}V(I_\alpha)=V\left(\sum_{\alpha\in\lambda}I_\alpha\right).\]
Now, $\mf p$ is in the intersection is and only if each $I_\bullet\subseteq\mf p.$ But the sum $\sum I_\bullet$ is generated by (finite) linear combinations of elements in the $I_\bullet,$ so indeed $\mf p$ is $V\left(\sum I_\bullet\right).$ And then because $I_\bullet\subseteq V\left(\sum I_\bullet\right),$ we see $\mf p\in V\left(\sum I_\bullet\right)$ implies
\[I_\bullet\subseteq\mf p\]
for each $I_\bullet.$ So $\mf p$ is also in the intersection. So we have arbitrary intersection.

For unions, we again still have $V(I)\cup V(J)=V(IJ).$ Here, we see $\mf p\in V(I)\cup V(J)$ if and only if $I,J\subseteq\mf p.$ So because $IJ$ is generated by products $ab$ with $a\in I$ and $b\in J,$ we see $IJ\subseteq\mf p$ because each $ab\in\mf p.$ Conversely, if $\mf p\not\in V(I)\cup V(J),$ then there exists $a\in V(I)$ and $b\in V(J)$ with
\[a\not\in\mf p\quad\text{and}\quad b\not\in\mf p\]
But then $ab\not\in\mf p$ because $\mf p$ is prime (!), so $\mf p\not\in V(IJ).$ Inducting from here gives finite unions.

\subsubsection{February 24th}
Today I learned about the $p$-adic Gaussian to derive the completed Riemann $\xi$ function, from \href{https://terrytao.wordpress.com/2008/07/27/tates-proof-of-the-functional-equation/}{Terrence Tao}. We're going to synthetically derive the $\Gamma$ factor of the $\xi$ function, and then this will give us an idea of how to synthetically derive the entire $\xi$ function.

Fix $G_\infty(x)=e^{-\pi x^2}$ to be our Gaussian over $\RR=\QQ_\infty.$ Recall that we also have the Gaussian $G_p(x)=1_{\ZZ_p}$ on $\QQ_p$ as we showed two days ago. Additionally, we want our $\Gamma$ factor to be
\[\Gamma_\infty(s):=\pi^{-s/2}\Gamma(s/2)=\pi^{-s/2}\int_0^\infty e^{-t}t^{s/2}\,\frac{dt}t.\]
The correct to do here is to attempt to make $G_\infty$ appear in this expression. So we take $t\mapsto\pi t^2$ and $dt/t\mapsto 2dt/t,$ giving
\[\Gamma_\infty(s)=2\int_0^\infty e^{-\pi t^2}t^s\,\frac{dt}t.\]
Now this definition looks more motivated, for we get to write
\[\Gamma_\infty(s)=\int_{\QQ_\infty^\times}G_\infty(t)|t|^s\,\frac{dt}{|t|},\]
and all the artifacts of $\RR$ have disappeared: we can define the $\Gamma_\infty$ factor as (twice) the Mellin transform of the Gaussian! Further, note that $dt/|t|$ is a Haar measure on $\QQ_\infty^\times,$ so it feels a bit more natural (notationally) to integrate over $\QQ_\infty^\times$ instead of $\QQ_\infty\setminus\{0\}.$

Anyways, this suggests that we define
\[\Gamma_p(s):=\int_{\QQ_p^\times}G_p(t)|t_p|_p^s\left(\frac p{p-1}\cdot\frac{dt_p}{|t_p|_p}\right).\]
We remark that we choose our measure $\frac p{p-1}\cdot\frac{dt_p}{|t_p|_p}$ to be a multiplicative measure on $\QQ_p^\times,$ scaled so that the units $\ZZ_p^\times=\ZZ_p\setminus p\ZZ_p$ has measure $1.$ We show explicitly that $\ZZ_p\setminus p\ZZ_p$ has the desired measure, for it's a basic case of something we'll do shortly. Essentially, $t_p\in\ZZ_p\setminus p\ZZ_p$ will always have $|t_p|_p=1,$ so
\[\int_{\ZZ_p\setminus p\ZZ_p}\frac{dt_p}{|t_p|_p}=\int_{\ZZ_p\setminus p\ZZ_p}dt_p.\]
Now, $\ZZ_p\setminus p\ZZ_p$ is made up of $p-1$ cosets of $\ZZ_p/p\ZZ_p,$ of which there are $p.$ So with the $p$ total cosets, which all have the same measure because $dt_p$ is shift-invariant, partitioning $\ZZ_p$ of measure $1,$ it follows each coset has measure $\frac1p.$ Thus,
\[\int_{\ZZ_p\setminus p\ZZ_p}\frac{dt_p}{|t_p|_p}=\frac{p-1}p,\]
which is what we wanted.

This argument turns out to be helpful for the following claim, which is what we're really here for.
\begin{proposition}
    We have that
    \[\Gamma_p(s)=\frac1{1-p^{-s}}.\]
\end{proposition}
This is a miracle. The $p$-adic $\Gamma$ factor turns out to be an Euler factor! We will make more comments on this later.

At a high level, what makes this formula so nice is that our Gaussian $G_p$ is so nice. Because $G_p$ merely indicates $\ZZ_p,$ our integral over $\QQ_p^\times$ turns into an integral over $\ZZ_p\setminus\{0\}.$ So we see
\[\Gamma_p(s)=\frac p{p-1}\int_{\ZZ_p\setminus\{0\}}|t_p|^s_p\,\frac{dt_p}{|t_p|_p}.\]
Here we mimic the argument that $\ZZ_p\setminus p\ZZ_p$ has multiplicative maesure $1.$ We see $\ZZ_p$ is inherently granular, so we can make the integrand constant by summing over each possible $|x_p|_p.$ In particular, $x_p\in\ZZ_p\setminus\{0\}$ implies $|x_p|_p=p^{-\nu}$ for some nonnegative integer $\nu.$ So because $\ZZ_p\setminus\{0\}$ is countably partitioned by $\nu,$ we may write
\[\Gamma_p(s)=\frac p{p-1}\sum_{\nu=0}^\infty\int_{|x_p|=p^{-\nu}}p^{-\nu s}\,\frac{dt_p}{p^{-\nu}}.\]
Now, the integrand is constant with respect to $t_p,$ so we can move it out of the integral, giving
\[\Gamma_p(s)=\frac p{p-1}\sum_{\nu=0}^\infty\frac{p^{-\nu s}}{p^{-\nu}}\int_{|x_p|=p^{-\nu}}dt_p.\]
At this point, we can begin to see the $\left(1-p^{-s}\right)^{-1}$ coming from the infinite series. We note that $\left\{x_p:|x_p|=p^{-\nu}\right\}$ is the $p^\nu\ZZ_p\setminus p^{\nu+1}\ZZ_p,$ the set of elements divisible by $p^\nu$ but not $p^{\nu+1}.$ So our integral is
\[\Gamma_p(s)=\frac p{p-1}\sum_{\nu=0}^\infty\frac{p^{-\nu s}}{p^{-\nu}}\int_{p^{\nu}\ZZ_p\setminus p^{\nu+1}\ZZ_p}dt_p.\]
We can evaluate the integral as we did when showing $\ZZ_p\setminus p\ZZ_p$ has multiplicative measure $1.$ The coset $p^\nu\ZZ_p$ is one of $p^\nu$ in $\ZZ_p/p^\nu\ZZ_p,$ and all cosets have the same size, so the measure of $p^\nu\ZZ_p$ is $1/p^\nu.$ The same argument shows $p^{\nu+1}\ZZ_p$ has measure $1/p^{\nu+1},$ so
\[\int_{p^{\nu}\ZZ_p\setminus p^{\nu+1}\ZZ_p}dt_p=\frac1{p^\nu}-\frac1{p^{\nu+1}}=\frac{p-1}{p^{\nu+1}}\]
because $p^{\nu+1}\ZZ_p\subseteq p^\nu\ZZ_p.$ From here, we have
\[\Gamma_p(s)=\frac p{p-1}\sum_{\nu=0}^\infty\frac{p^{-\nu s}}{p^{-\nu}}\cdot\frac{p-1}{p^{\nu+1}}.\]
Simplifying, we see
\[\Gamma_p(s)=\sum_{\nu=0}^\infty p^{-\nu s}=\frac1{1-p^{-s}},\]
which is what we wanted. $\blacksquare$

Quickly, we remark that if I were more comfortable with $p$-adic integration, we could write $d^\times t_p:=\frac p{p-1}\cdot\frac{dt_p}{|t_p|_p}$ as our multiplicative measure so that
\[\int_{|t_p|_p=p^{-\nu s}}d^\times t_p=\int_{|t_p|_p=1}d^\times t_p=\int_{\ZZ_p\setminus p\ZZ_p}d^\times t_p=1\]
because we are integrating over that multiplicative (!) measure. This is faster, but the multiplicative measure scares me.

We now remark on the miracle that just happened. It would appear that we can define $\zeta$ synthetically (!) by
\[\zeta(s)=\prod_{p<\infty}\Gamma_p(s),\]
and this feels somewhat more natural than the the infinite sum definition. In particular, the analysis that we did above kind of implies that the ``correct'' way to define our $\zeta$-functions is by an Euler product, which aligns with the way Artin $L$-functions are defined by Euler factors. Further,
\[\xi(s)=\prod_\nu\Gamma_\nu(s),\]
where $\nu$ ranges over all places (finite and infinite) of $\QQ.$ This presents a much more motivated view of the $\Gamma$ factor by placing it on equal footing with the rest of the Euler factors.

\subsubsection{February 25th}
Today I learned about the Lesbegue measure. The basic idea is that we want to create a Haar (translation-invariant and outer Radon) measure which communicates niecly with our basis elements. To start off, we define $\lambda((a,b))=b-a$ to be our measure on the basis elements of our topology.

Next we extend this to all open sets. This is not actually trivial because even though we can write
\[U=\bigcup_\alpha(a_\alpha,b_\alpha)\]
for some collection of basis elements $(a_\alpha,b_\alpha),$ these basis elements might overlap. The trick is that our basis elements are totally ordered by containment, so we can intuitively choose the ``largest'' basis elements to cover $U$ and not worry about overlap. Making this rigorous is somewhat annoying, but one way to do this is to write
\[\mathcal B=\{(a,b):(a,b)\subseteq U\},\]
and then $\mathcal B$ is a partially ordered set under $\subseteq.$ Each ascending chain has an upper bound (say, $\RR$), so each $B\in\mathcal B$ is contained in some maximal element of $\mathcal B.$ So we can define a function $\varphi:\mathcal B\to\mathcal B$ taking each basis element to its maximal and define
\[\mu(U)=\sum_{B\in\varphi(\mathcal B)}\lambda(B).\]
This is disgusting, but it provides the idea of what's to come.

We notice now that we want an outer Radon measure, which requires being outer regular. So it would be lovely if we could just define
\[\mu(S)=\inf_{\text{open }U\supseteq S}\mu(U)\]
to get outer regular for free. However, this forces us to take $\inf$ over the $\varphi$ function from earlier which we already don't understand very well. The technical trick to avoid $\varphi,$ then, is to let the $\inf$ check for overlaps for us. We take the following definition.
\begin{definition}
    Define the Lebesgue measure $\mu$ on $\RR$ to be
    \[\mu(S):=\inf\left\{\sum_{k=0}^\infty(b_k-a_k):S\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}.\]
\end{definition}
Note now that overlaps between the $(a_\bullet,b_\bullet)$ are entirely permitted, but the $\inf$ will tell us that overlapping is not minimal.

It remains to check that $\mu$ is actually a Haar measure. We will show it is an outer measure today and move towards showing Haar. The easiest part of this showing that $\mu$ is translation-invariant, so we get that out of the way. Indeed, for set $S$ and constant $c\in\RR,$ we have to show that $\mu(S)=\mu(c+S),$ or
\[\inf\left\{\sum_{k=0}^\infty(b_k-a_k):S\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}=\inf\left\{\sum_{k=0}^\infty(b_k-a_k):c+S\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}.\]
Now, for any collection $\{(a_k,b_k)\}_{k=0}^\infty$ covering $S,$ we note $\{(a_k+c,b_k+c)\}_{k=0}^\infty$ will cover $c+S$ while preserving all of the lengths $b_k-a_k=(b_k+c)-(a_k+c).$ It follows that any bound $M$ occuring in the left-hand set will also appear in the right-hand set, so
\[\mu(S)\ge\mu(c+S).\]
Taking $S\mapsto c+S$ and $c\mapsto-c$ implies $\mu(c+S)\ge\mu(S),$ so $\mu(S)=\mu(c+S),$ as desired.

Before continuing, we establish a lemma. We show that if $A\subseteq B,$ then $\mu(A)\le\mu(B).$ Expanding, we have to show that
\[\inf\left\{\sum_{k=0}^\infty(b_k-a_k):A\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}\le\inf\left\{\sum_{k=0}^\infty(b_k-a_k):B\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}.\]
Well, we note that any collection $\{(a_k,b_k)\}_{k=1}^\infty$ covering $B$ will also cover $A,$ so any bound $M$ appearing in the right-hand set also appears in the left-hand set. It follows $\mu(A)\le\mu(B).$

We now show that $\mu$ is outer regular. That is, we have to show
\[\mu(S)=\inf_{\text{open }U\supseteq S}\mu(U).\]
Quickly, we note that $U\supseteq S$ directly implies $\mu(U)\ge\mu(S),$ so we immediately know
\[\mu(S)\le\inf_{\text{open }U\supseteq S}\mu(U)\]
from the lemma. To get the other direction, we expanding out the definition of $\mu,$ so we have to show
\[\inf\left\{\sum_{k=0}^\infty(b_k-a_k):S\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}\stackrel?\ge\inf_{\text{open }U\supseteq S}\left\{\sum_{k=0}^\infty(b_k-a_k):U\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}.\]
Now, note that any collection $\{(a_k,b_k)\}$ covering $S$ on the left-hand side will union to an open set that we can give to the right-hand side, with the same cover. So any bound $M$ appearing in the left-hand set will also appear in the right-hand set, so
\[\mu(S)\ge\inf_{\text{open }U\supseteq S}\mu(U).\]
Combining the two inequalities gets what we want.

Only now do we try to check that $\mu$ is a measure. Take $\{S_k\}_{k=0}^\infty$ sets so that we want to show
\[\sum_{k=0}^\infty\mu(S_k)\stackrel?=\mu\left(\bigcup_{k=0}^\infty S_k\right).\]
Expanding, we want to know
\[\sum_{k=0}^\infty\inf\left\{\sum_{\ell=0}^\infty(b_\ell-a_\ell):S_k\subseteq\bigcup_{\ell=0}^\infty(a_\ell,b_\ell)\right\}\stackrel?=\inf\left\{\sum_{\ell=0}^\infty(b_\ell-a_\ell):\bigcup_{k=0}^\infty S_k\subseteq\bigcup_{\ell=0}^\infty(a_\ell,b_\ell)\right\}.\]
The outer sum on the left-hand side is unnatural, so we write
\[\inf\left\{\sum_{k,\ell=0}^\infty(b_{k,\ell}-a_{k,\ell}):S_k\subseteq\bigcup_{\ell=0}^\infty(a_{k,\ell},b_{k,\ell})\right\}\stackrel?=\inf\left\{\sum_{\ell=0}^\infty(b_\ell-a_\ell):\bigcup_{k=0}^\infty S_k\subseteq\bigcup_{\ell=0}^\infty(a_\ell,b_\ell)\right\}.\]
Now, in one direction, note that for any bound $M$ in the left-hand side, we can associate it with a countable (!) collection $(a_{k,\ell},b_{k,\ell})$ of basis sets. This collection covers each of the $S_\bullet,$ so it covers their union, so this collection will also appear in the right-hand set. Thus, the bound $M$ also appears in the right-hand side. It follows
\[\sum_{k=0}^\infty\mu(S_k)\ge\mu\left(\bigcup_{k=0}^\infty S_k\right).\]
This is enough for $\mu$ to be an outer measure, but it remains to check the $\le$ inequality. This is because it is actually impossible to show this for arbitrary sets $S_\bullet,$ even though we haven't used the disjoint condition. I don't currently know enough about measurable sets to be able to fix this.

\subsubsection{February 26th}
Today I learned how to show intervals $[a,b]$ have Lebesgue measure $b-a.$ I don't think I'm going to complete showing that $\mu$ is a measure soon because it looks significantly more involved than I bargained for.

Computing $\mu([a,b])$ is mostly to sanity-check that our definition makes sense. This turns out to be somewhat subtle---because we're using countably many intervals in our definition of $\mu,$ we have to deal with infinite sums somehow. What makes closed intervals nice is that they are compact, letting us deal with finite sums instead. We claim the following.
\begin{proposition}
    The closed interval $[a,b]$ for $a,b\in\RR$ has Lebesgue measure $b-a.$
\end{proposition}
To show this, we want.
\[\mu([a,b])=\inf\left\{\sum_{k=0}^\infty(b_k-a_k):[a,b]\subseteq\bigcup_{k=0}^\infty(a_k,b_k)\right\}.\]
Note that $(a-\varepsilon/2,b+\varepsilon/2)$ makes a basis cover of $[a,b]$ of length $b-a+\varepsilon,$ so sending $\varepsilon\to0$ means that $\mu([a,b])\ge b-a.$ For the reverse inequality, take any basis cover $\{(a_k,b_k)\}_{k=0}^\infty$ of $[a,b].$ We want to show
\[\sum_{k=0}^\infty(b_k-a_k)\stackrel?\ge b-a.\]
However, $[a,b]$ is compact (!), so we can extract a finite subcover from $\{(a_k,b_k)\}_{k=0}^\infty.$ Note that each interval has nonnegative $b_\bullet-a_\bullet,$ so the sum does not increase by removing all but finitely many of the intervals. Re-indexing, we name our subcover $\{(a_k,b_k)\}_{k=0}^N$ so that we want
\[\sum_{k=0}^N(b_k-a_k)\stackrel?\ge b-a.\]
The reason why a finite sum is nice is that we can now rearrange terms freely. The picture now is that the intervals $(a_\bullet,b_\bullet)$ should look something like this.
\begin{center}
    \begin{asy}
        unitsize(1cm);
        real a=0, b=6;
        real space=0.3;
        dot((a,0));
        dot((b,0));
        draw((a,0)--(b,0));
        int steps = 4;
        for(int i = 0; i < steps; ++i)
        {
            label("$($",(a + (b-a)/steps * i - space, 0));
            label("$)$",(a + (b-a)/steps * (i+1) + space, 0));
            if((int)i % 2 == 0)
            {
                label("$a_"+string(i)+"$",(a + (b-a)/steps * i - space, -0.4));
                label("$b_"+string(i)+"$",(a + (b-a)/steps * (i+1) + space, -0.4));
            }
            else
            {
                label("$a_"+string(i)+"$",(a + (b-a)/steps * i - space, 0.4));
                label("$b_"+string(i)+"$",(a + (b-a)/steps * (i+1) + space, 0.4));
            }
        }
    \end{asy}
\end{center}
In particular, the differences $b_k-a_{k+1}$ are all positive while $a_0<a$ and $b_N>b.$ This suggests how to get our inequality. To help with technicalities, we apply the following filters and rearranging.
\begin{itemize}
    \item We get rid of any intervals in the $\{(a_k,b_k)\}_{k=0}^N$ that are a strict subset of any other interval.
    \item We get rid of intervals that do not intersect $[a,b]$ at all.
    \item Among the $(a_\bullet,b_\bullet)$ with $a_\bullet<a,$ we choose the one with greatest $b_\bullet$ and get rid of the others. (This is effectively chooses the largest interval with respect to $[a,b].$)
    \item Similarly, among the $(a_\bullet,b_\bullet)$ with $a_\bullet<a,$ we choose the one with least $a_\bullet.$
    \item We arrange the $(a_\bullet,b_\bullet)$ to ascend by $a_\bullet.$ We note that this makes the $b_\bullet$ ascend as well, for $a_k\le a_{k+1}$ with $b_k\ge b_{k+1}$ implies $(a_k,b_k)\supseteq(a_{k+1},b_{k+1}).$
\end{itemize}
Now, we think about the sum like
\[b_N+(-a_N+b_{N-1})+(-a_{N+1}-b_{N+2})+\cdots+(-a_1+b_0)-a_0\stackrel?\ge b-a.\]
We note $b_N$ is the largest $b_\bullet,$ so we must have $b_N>b$ to cover $[a,b].$ Also, it is the only $b_\bullet$ with $b_\bullet>b$ because we filtered the others out. Similarly, $a_0<a$ and is the only $a_\bullet$ with this property. Thus, $b_N-a_0\ge b-a,$ and we need to show the remaining terms are nonnegative.

Our ordering by $a_\bullet$ means $a_{k+1}\in(a_k,b_k)$ or $k<N$ for each $k.$ Indeed, $a_{k+1}\ge a_k,$ and if $a_{k+1}\ge b_k,$ then all of the remaining $a_\bullet$ past $a_{k+1}$ are also at least $b_k,$ so $b_k$ never gets covered in $[a,b],$ which is a problem: $b_k<a$ means $(a_k,b_k)$ doesn't intersect $[a,b],$ and $b_k>b$ requires $k=N,$ for this is the only $k$ (the largest one) satisfying this.

It follows $-a_{k+1}+b_k\ge0,$ so we see
\[b_N+(-a_N+b_{N-1})+\cdots+(-a_1+b_0)-a_0\ge b-a.\]
This is what we wanted. $\blacksquare$

We take a second to remark that we have not actually shown $[a,b]$ is measurable, and I have not even defined what measurable means. For the sake of completeness, we take the following definition.
\begin{definition}
    A set $A\subseteq\RR$ is Lebesgue-measurable if and only if $\mu(A)=\mu(A\cap S)+\mu(A\cap S^c)$ for all sets $S\subseteq\RR.$
\end{definition}
It turns out that Lebesgue-measurable sets form a $\sigma$-algebra, so showing that (open or closed) intervals are Lebesgue-measurable implies that this is a Borel measure. (We still have to verify that countable disjoint unions can be separated.) I don't know how to prove either of these, but honestly that sounds like someone else's job.

\subsubsection{February 27th}
Today I learned about the adeles of $\CC(t),$ which provides some nice analogy to the situation with number fields. Recall geometrically, we are interested in prime ideals as the connection in our analogy: $\CC[t]$ has prime ideals $(t-\alpha)$ for $\alpha\in\CC$ while number fields $K$ have prime ideals $\mf p.$ (There is also the prime $(0).$) We also recall this analogy gave rise to the Zariski topology on $\op{Spec}\mathcal O_K$ by comparing with the Zariski topology on $\op{Spec}\CC[t].$

The individual components of the adeles are local components, and this is clear from $\CC(t).$ Quite literally, studying a rational function $f(t)\in\CC(t)$ ``locally'' at the prime $(t-\alpha)$ means we write
\[f(t)=\sum_{k=-N}^\infty c_k(t-\alpha)^k\in\CC((t-\alpha))\]
as our Taylor series. The term ``local'' means that we're studying $f$ by focusing on the individual point at $\alpha.$

I had some confusion in intuition for a while because I felt like ``local'' information should still communicate to nearby neighborhoods: studying a rational function at $\alpha$ should give information about $\alpha+\varepsilon.$ However, algebraically this doesn't make much sense, and even analytically, some conditions are needed on $f$ and $\varepsilon$ to make $\alpha$ and $\alpha+\varepsilon$ behave nicely. To be explicit,
\[f(t)=\frac{t-(\alpha+\varepsilon)}{t-\alpha}\]
has very different behavior at $\alpha$ and $\alpha+\varepsilon.$ It is not clear to me how to prevent this from happening, and I'm not sure if it's easily doable at all. Further, in the number field case, this is more explicit: the prime $(101)$ does not communicate with $(103)$ well.

Anyways, to combine all of our local information together, we define the adeles as
\[\sideset{}{'}\prod_{\alpha\in\CC}\CC((t-\alpha)).\]
The $\prod'$ refers to the restricted product: we require that all but finitely many of our coordinates in $\CC((t-\alpha))$ to actually live in our ``ring of integers'' $\CC[[t-\alpha]].$ Note that rational functions in $\CC(t)$ do embed into these adeles because a rational function only has finitely many poles (say, by Lagrange's theorem on fields). We remark that the ideles can also be defined as
\[\sideset{}{'}\prod_{\alpha\in\CC}\CC((t-\alpha))\]
where now all but finitely many of the coordinates are in $\CC[[t-\alpha]]^\times.$ We still have $\CC(t)$ embedding into the ideles because a rational function has both finitely many poles and finitely many zeroes. I also think the image of $\CC(t)$ into the ideles is discrete as in the number field case, where
\[\prod_{\alpha\in\CC}\CC[[t-\alpha]]^\times\]
is our fundamental domain of the ideles modulo $\CC(t).$ I'm not totally sure about this.

The restricted product on the adeles are exactly analogous to wanting all but finitely many of the coordinates of $\AA_K$ to live in $\mathcal O_{K,\nu}$ over our places $\nu.$ We even have the analogy that local fields $K_\nu$ behave as Taylor series like $\CC((t-\alpha)).$ I think what's happening in this analogy is that we think of (fractional) ideals $\mf a$ as functions over primes $\mf p$ with
\[\mf a(\mf p)=\mf p^{\nu_\mf p(\mf a)}.\]
I should say that I'm trying to think of ideals $\mf a$ as functions of primes $\mf p$ because that's what's going in $\CC$: elements of $\CC(t)$ are effectively rational functions of the primes $(t-\alpha).$

Of course, the provided function doesn't quite catch the behavior we want, but it's catching some: expanding $\mf a$ locally at $\mf p$ like a Taylor series now will maintain the idea of zeroes and poles. Additionally, we see that these primes really do not communicate well with each other.

\subsubsection{February 28th}
Today I learned the classification of characters over $\QQ_p$ as $\QQ_p\cong\widehat{\QQ_p},$ from \href{https://kconrad.math.uconn.edu/blurbs/gradnumthy/characterQ.pdf}{Keith Conrad} as usual. Fix $e_p(x):=e^{2\pi i\{x\}_p}.$ The following is what we're building towards.
\begin{theorem}
    The mapping $\Psi_\bullet:\QQ_p\to\widehat{\QQ_p}$ defined by taking $\Psi_a:=(x\mapsto e_p(ax))$ is an isomorphism of groups.
\end{theorem}
In fact it is true that $\Psi_\bullet$ is an isomorphism of the groups as well as a homeomorphism of the two spaces.

We begin by showing that $\Psi_\bullet$ is well-defined. For any $a\in\QQ_p,$ we need to know $\Psi_a$ is a character. Well, $\Psi_a$ is homomorphic because
\[\Psi_a(x+y)=e^{2\pi i\{a(x+y)\}_p}=e^{2\pi i\{ax\}_p}\cdot e^{2\pi i\{ay\}_p}=\Psi_a(x)\Psi_a(y).\]
In particular, we know $\{x+y\}_p=\{\{x\}_p+\{y\}_p\}$ by expanding $x=x_0+x_\bullet/p^\bullet$ and $y=y_0+y_\bullet/p^\bullet$ for $x_0,y_0\in\ZZ_p$ and $x_\bullet,y_\bullet\in\ZZ/p^\bullet\ZZ.$ Further, $\Psi_a$ is continuous because if we have a Cauchy sequence $x_\bullet\to x$ in $\QQ_p,$ then
\[\{x_\bullet\}_p\to\{x\}_p\]
in $\RR.$ For example, $\{x_\bullet\}_p=\{x\}_p$ holds as soon as $|x_\bullet-x|\le1,$ which must happen eventually. It follows that $\Psi_a(x_\bullet)\to\Psi_a(x).$ Note how discrete this argument feels.

It also not terribly difficult to show that $\Psi_\bullet$ is an injective homomorphism. Showing show $\Psi_\bullet$ is homomorphic is similar to showing $\Psi_\bullet$ is well-defined. For $a,b,x\in\QQ_p,$ we see
\[\Psi_{a+b}(x)=e^{2\pi i\{(a+b)x\}_p}=e^{2\pi i\{ax\}_p}\cdot e^{2\pi i\{bx\}_p}=\Psi_a(x)\Psi_b(x),\]
which is what we wanted. Again, $\{x+y\}_p=\{\{x\}_p+\{y\}_p\}$ by expanding out fractional parts.

For injective, we need to know that distinct $a,b\in\QQ_p$ give distinct characters $\Psi_a\ne\Psi_b.$ Well, $a\ne b$ means $b-a\ne0,$ so $|b-a|_p>0.$ If we let $|b-a|_p=p^{-\nu},$ then surely
\[a\not\equiv b\pmod{p^\nu}.\]
In particular, $\left\{a/p^\nu\right\}_p\ne\left\{b/p^\nu\right\}_p,$ so $\Psi_a\left(1/p^\nu\right)\ne\Psi_b\left(1/p^\nu\right).$ So $\Psi_a\ne\Psi_b,$ as desired.

To show $\Psi_\bullet$ is an isomorphism of groups, it remains to show that $\Psi_\bullet$ is surjective. The algebraic core of the argument is the following claim.
\begin{lemma}
    All characters $\chi:\QQ_p\to S^1$ satisfying $\chi(\ZZ_p)=\{1\}$ take the form $\chi(x)=\Psi_c(x)=e_p(cx)$ for some $c\in\ZZ_p.$
\end{lemma}
The idea here is to extract $c\in\ZZ_p$ as a Cauchy sequence. The interesting behavior of $\QQ_p$ occurs at negative powers of $p,$ so we note that for a power $p^\bullet,$ we have
\[\chi\left(1/p^\bullet\right)^{p^\bullet}=\chi\left(p^\bullet\cdot1/p^\bullet\right)=1.\]
It follows $\chi\left(1/p^\bullet\right)$ is a $p^\bullet$th root of unity, so we may define the sequence $\{c_\bullet\}$ to satisfy $\chi\left(1/p^\bullet\right)=e^{2\pi ic_\bullet/p^\bullet}.$ Noting that
\[e^{2\pi ic_\bullet/p^\bullet}=\chi\left(1/p^\bullet\right)=\chi\left(1/p^{\bullet+1}\right)^p=e^{2\pi ic_{\bullet+1}/p^\bullet}\]
implies that $c_{\bullet+1}\equiv c_\bullet\pmod{p^\bullet}.$ In particular $\{c_\bullet\}$ is a Cauchy/coherent sequence in $\ZZ_p,$ so it will converge to some $c\in\ZZ_p$ with $\left\{c/p^\bullet\right\}=c_\bullet/p^\bullet.$

Having extracted $c,$ we claim each $x\in\QQ_p$ has
\[\chi(x)\stackrel?=e^{2\pi i\{cx\}_p}.\]
This comes down to some technical computations involving $\{\bullet\}_p.$ Fix $x=y/p^\bullet$ for some $y\in\ZZ_p,$ and we can write $y=x_\bullet+x_0p^\bullet$ with $x_\bullet\in\ZZ/p^\bullet\ZZ$ and $x_0\in\ZZ_p$ so that $x=x_0+x_\bullet/p^\bullet.$ Combining this with $\chi(x_0)=1,$ we see
\[\chi(x)=\chi(x_0)\chi\left(x_\bullet/p^\bullet\right)=1\cdot\chi\left(1/p^\bullet\right)^{x_\bullet}=e^{2\pi ic_\bullet x_\bullet/p^\bullet}.\]
It remains to compare $c_\bullet x_\bullet/p^\bullet$ with $\{cx\}_p.$ Well, we can write $c=c_\bullet+c_0p^\bullet$ with $c_\bullet\in\ZZ/p^\bullet\ZZ$ and $c_0\in\ZZ_p$ so that
\[cx=\underbrace{cx_0}_{\in\ZZ_p}+\underbrace{c_0x_\bullet}_{\in\ZZ_p}+c_\bullet x_\bullet/p^\bullet.\]
It follows $\{cx\}_p=\left\{c_\bullet x_\bullet/p^\bullet\right\}_p,$ and because $c_\bullet,x_\bullet\in\ZZ,$ we have $\{cx\}_p=\left\{c_\bullet x_\bullet/p^\bullet\right\}.$ This completes the proof of the lemma. It is somewhat remarkable that we have not used continuity anywhere in this argument; I think $\chi(\ZZ_p)=\{1\}$ guarantees that $\chi$ is locally constant, which obviates the need for continuity. $\blacksquare$

At this point, we're a clever step away from getting any character $\chi.$
\begin{lemma}
    All characters $\chi:\QQ_p\to S^1$ take the form $\chi(x)=\Psi_c(x)=e_p(cx)$ for some $c\in\QQ_p.$
\end{lemma}
The main idea here is to show that $\chi$ should be $1$ on small inputs. This roughly comes from a difference in the way the topologies on $\QQ_p$ and $S^1$ behave, for $\QQ_p$ has subgroups of arbitrarily small norm while $S^1$ does not. To make this explicit, look at the open set
\[U_1:=\left\{z\in S^1:|z-1|<1\right\}\subseteq S^1.\]
Because $\chi$ is continuous, $\chi^{-1}(U_1)$ is also open. Note $\chi(0)=1,$ so $0\in\chi^{-1}(U_1),$ so there's a basis element around $0$ named $p^\nu\ZZ_p\subseteq\chi^{-1}(U_1).$ This is our only use of the continuity of $\chi.$

So far everything done is topological, but now noting that $p^\nu\ZZ_p$ is a subgroup $\QQ_p$ requires $\chi\left(p^\nu\ZZ_p\right)\subseteq U$ to also be a subgroup. But looking at $U,$ the only available subgroup is $\{1\}$! Thus, we define $\chi_0(x):=\chi\left(p^\nu x\right)$ so that
\[\chi_0\left(\ZZ_p\right)=\chi\left(p^\nu\ZZ_p\right)=\{1\}.\]
In particular, the previous lemma now kicks in so that $\chi_0(x)=e^{2\pi i\{cx\}_p}$ for some $c\in\ZZ_p.$ It follows
\[\chi(x)=\chi_0\left(x/p^\nu\right)=e^{2\pi i\left\{c/p^\nu\cdot x\right\}_p}.\]
Thus we have shown $\chi$ has the desired form with constant $c/p^\nu\in\QQ_p.$ $\blacksquare$

While we're here, we remark that the above argument can show that $\Psi_\bullet$ is an open map.
\begin{lemma}
    The map $\Psi_\bullet:\QQ_p\to\widehat{\QQ_p}$ is an open map.
\end{lemma}
Here $\widehat{\QQ_p}$ is endowed with the compact-open topology, which is created by the subbasis of sets
\[\left\{\chi\in\widehat{\QQ_p}:\chi(K)\subseteq U\right\}\]
for any compact $K$ and open set $U.$ It is a fact that $\widehat{\QQ_p}$ is a topological group under this topology.

Anyways, any open set in $\QQ_p$ can be decomposed into basis elements, and because function application preserves unions, it therefore suffices to show that basis elements of the topology of $\QQ_p$ map to open sets. Further, for any basis element $x+p^\nu\ZZ_p,$ we see
\[\Psi\left(x+p^\nu\ZZ_p\right)=\Psi(x)\cdot\Psi\left(p^\nu\ZZ_p\right),\]
so it suffices to show that $\Psi\left(p^\nu\ZZ_p\right)$ is an open set. Well, $a\in p^\nu\ZZ_p$ if and only if
\[\Psi_a\left(p^{-\nu}\ZZ_p\right)=\Psi_1\left(ap^{-\nu}\ZZ_p\right)=\Psi(\ZZ_p)=\{1\}.\]
But then we can substitute $\{1\}$ out with $U_1$ because $p^\nu\ZZ_p$ is a subgroup (which requires $\Psi\left(p^\nu\ZZ_p\right)$ to be a subgroup as above), so indeed
\[\Psi\left(p^\nu\ZZ_p\right)=\left\{\chi\in\widehat{\QQ_p}:\chi\left(p^{-\nu}\ZZ_p\right)\subseteq U_1\right\}.\]
So we see $\Psi$ is an open map. $\blacksquare$

To show that $\Psi$ is an isomorphism of topological groups, we need to complete the proof that $\Psi$ is a homeomorphism by showing $\Psi$ is continuous. I don't currently know how to do this.
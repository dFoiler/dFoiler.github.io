\subsection{March}

\subsubsection{March 1st}
Today I learned that local fields $F$ have character group $\widehat F$ isomorphic to themselves, from the \href{http://math.mit.edu/~poonen/786/notes.pdf}{MIT notes} I've been reading. This is somewhat similar to the proof $\QQ_p\cong\widehat{\QQ_p}$ I did yesterday, but this is different in topological ways.

To set up, we fix any nontrivial character $\psi\in\widehat F.$ One exists by Pontryagin duality: $\widehat{F}\cong\{e\}$ means that $F\cong\widehat{\smash{\widehat F}\vphantom{1^]}\,}\cong\{e\},$ but $F\not\cong\{e\}.$ Then we define the map $\Psi:F\to\widehat F$ by
\[\Psi_a:=(x\mapsto\psi(ax)).\]
Before continuing, we check that this is well-defined. In particular, $\Psi_a$ is still a character because $\Psi_a(0)=\psi(a\cdot0)=\psi(0)=1.$ And $\Psi_a$ is continuous because it is the composite of the continuous maps $x\mapsto ax$ and $x\mapsto\psi(x).$ Anyways, we now claim the following.
\begin{theorem}
    The map $\Psi:F\to\widehat F$ is an isomorphism of topological groups. 
\end{theorem}
This means that we have show $\Psi$ is both an isomorphism of groups and a homeomorphism of topological spaces.

We begin by getting the easier stuff out of the way first.
\begin{lemma}
    The map $\Psi$ is an injective homomorphism of groups.
\end{lemma}
To show that $\Psi$ is a homomorphism, we have to show that $\Psi_{(a+b)}=\Psi_a\Psi_b$ for $a,b\in F.$ Well, plugging into the definitions, we see
\[\psi((a+b)x)=\psi(ax+bx)=\psi(ax)\psi(bx),\]
which is what we wanted. Now to show that $\Psi$ is injective, it suffices to show that it has trivial kernel. Well, $\psi$ is nontrivial (note we have used this condition here), so some $\psi(g)\ne1.$ Then for $a\ne0,$ we have $a\in F^\times,$ so
\[\Psi_a\left(a^{-1}g\right)=\psi\left(aa^{-1}g\right)=\psi(g)\ne1.\]
Thus, $\Psi_a$ is not the identity, so we see $\Psi$ has trivial kernel. $\blacksquare$

It would be nice, thematically speaking, to show surjectivity now to complete the proof that $\Psi$ is a group isomorphism. However, this will turn out to follow cleanly from a topological argument, so we postpone doing so.
\begin{lemma}
    The map $\Psi$ is a homeomorphism onto its own image.
\end{lemma}
Because $\Psi$ is injective, note that we can pull back the open sets in $\Psi(F)$ to make a topology on $F.$ To be explicit, we our open sets in the subspace $\Psi(F)$ have subbasis
\[\big\{\psi_a\in\widehat F:\psi_a(K)\subseteq U\big\}\]
for compact $K$ and open $U,$ and now we violently push this backwards to
\[V(U,K):=\{a\in F:\psi_a(K)\subseteq U\}=\left\{a\in F:aK\subseteq\psi^{-1}(U)\right\}.\]
This gives us a funny topology on $F,$ and we want to show that the normal topology on $F$ is the same as this one. That is, if an open set $U$ is open in the normal topology if and only if it's open in the funny topology, then we can push the funny topology back through to $\Psi(F)$ so that $U$ is open in the normal topology if and only if $\Psi(U)$ is open in $\widehat F$ and conversely.

The normal topology on $F$ is generated by open balls $B(a,r).$ To equate topologies, it suffices to focus on (sub)basis elements: we show that each $\bigcap_\bullet V(U_\bullet,K_\bullet)$ contains open ball neighborhoods of any individual element of a basis element $\bigcap_\bullet V(U_\bullet,K_\bullet),$ and each open ball contains some basis element $\bigcap_\bullet V(U_\bullet,K_\bullet)$ containing the center of the ball. Visualize this by populating funny open basis elements with balls and populating balls with funny open basis elements.

Quickly, we show this is enough. Indeed, then for any open ball $B(a,r),$ we write
\[B(a,r)=\bigcup_{u\in B(a,r)}\left(\bigcap_{k=1}^{n_u}V(U_{u,k},K_{u,k})\right),\]
where $u\in\bigcap_\bullet V(U_{u,\bullet},K_{u,\bullet})\subseteq B(u,r-|a-u|)\subseteq B(a,r).$ Thus open balls can be expressed as a union of funny basis elements, and anything open in the normal topology is open in the funny topology. And conversely, each basis element $\bigcap_{k=1}^nV(U,K)$ contains open ball neighborhoods of any individual element, so we can write
\[\bigcap_{k=1}^nV(U,K)=\bigcup_{v\in V}B(a_v,r_v),\]
where $v\in B(a_v,r_v)\subseteq\bigcap_{k=1}^nV(U,K).$ Thus, the open balls also form a subbasis of the funny topology, and anything open in the funny topology is open in the normal topology.

Continuing, the topological group structure actually lets us focus only around $0.$ Indeed, it will be enough to claim the following.
\begin{lemma}
    Any funny open set $V(U,K)$ containing $0$ contains an open ball centered at $0$ and conversely.
\end{lemma}
Indeed, suppose we knew this. To show that each funny basis element $V$ contains open ball neighborhoods around each element of $v,$ then we note $-v+V$ is an a funny open set (by homogeneity) containing $0.$ So fixing a finite intersection of some $V(U_\bullet,K_\bullet)$ to be a basis element around $0$ in $V,$ each $V(U_\bullet,K_\bullet)$ contains $0$ and thus some $B(0,r_\bullet)$ by lemma. Taking the minimum of the $r_\bullet$ lets us assert $B(0,r_\bullet)\subseteq-v+V,$ so
\[B(v,r_\bullet)\subseteq V.\]
To show that each open ball $B(a,r)$ contains a funny basis element $V$ around the center $a,$ we note that $B(0,r)$ contains some funny open set $V(U,K)$ around $0,$ and then $a+V(U,K)$ is still a funny open set now containing $a.$ Thus, we can extract a basis element of $a+V(U,K)$ containing $a$ to finish.

Now we show the (sub)lemma. To show that each $V(U,K)$ containing $0$ contains an open ball centered at $0,$ we note that this means exhibiting an $\varepsilon$ for which
\[|a|<\varepsilon\implies aK\subseteq\psi^{-1}(U)\]
after unraveling the definitions. Note that none of the previous arguments about basis elements have used anything about the topology on $F,$ but we note now that $K$ is compact requires that $K$ is bounded, so we're essentially trying to squeeze a small set into $\psi^{-1}(U).$ Well, $\psi^{-1}(U)$ is open by continuity, and it contains $0,$ so fix some
\[B(0,r)\subseteq\psi^{-1}(U).\]
Because $K$ is bounded, we can say $k\in K$ implies $|k|\le m$ for some $m,$ which now lets us finish with $\varepsilon=r/m.$ Indeed, if $|a|<r/m,$ then any $ak\in K$ satisfies
\[|ak|=|a|\cdot|k|<\frac rm\cdot m=r,\]
so $ak\in B(0,r)\subseteq\psi^{-1}(U).$ So we finish this part.

Conversely, we now show that each open ball $B(0,r)$ around $0$ contains some $V(U,K)$ containing $0.$ Unpacking definitions, we will exhibit compact $K$ and open $U$ containing $1$ such that
\[aK\subseteq\psi^{-1}(U)\implies|a|<r.\]
We are forcing $1\in U$ so that $a=0$ works. We note that if $\psi$ is trivial, then this is false, so we need to introduce $x\in G$ so that $\psi(x)\ne1.$ Now, the main trick here is to take a closed ball for $K,$ which are compact, and this will let us move this to a size condition. Namely, we take $U=S^1\setminus\{\psi(x)\},$ and
\[K=\{k\in G:|k|\le|x|/r\}.\]
The visual here is that $a\in F$ is permitted as long as $aK$ doesn't ``blow up'' too much to hit $x.$ Anyways, we show this works; fix $a\in F,$ and we show the contrapositive that
\[|a|\ge r\implies aK\not\subseteq\psi^{-1}(U).\]
We see $|x/a|\le|x|/r,$ so $x/a\in K,$ implying that $x\in aK.$ However, $\psi(x)\not\in U,$ so $x$ witnesses $aK\not\subseteq\psi^{-1}(U).$ This completes the proof that $\Psi$ is a homeomorphism onto its image. $\blacksquare$

We are now almost ready to show that $\Psi$ is surjective. To do this cleanly, we pick up some theory about the ``annihilator.'' In a general locally compact abelian group $G,$ we fix a subgroup $H\subseteq G.$ Then the annihilator $H$ denoted $H^\perp\subseteq\widehat G$ is defined as
\[H^\perp:=\big\{\chi\in\widehat G:\chi(H)=\{1\}\big\}.\]
We note that $H^\perp$ is a subgroup of $\widehat G$ ($\chi_1(H)=\{1\}$ and $\chi_2(H)=\{1\}$ certainly implies $(\chi_1\chi_2)(H)=\{1\}$), and in fact writing
\[H^\perp=\bigcap_{h\in H}\big\{\chi\in\widehat G:\chi(h)=1\big\},\]
we see that $H^\perp$ is in fact closed. Indeed, it suffices to show that the intersected sets are closed, for which we note that the complement of these sets is
\[\big\{\chi\in\widehat G:\chi(\{h\})\in S^1\setminus\{1\}\big\}.\]
This complement is open because we're using the compact-open topology on $\widehat G.$

With the introductory remarks complete, we claim the following.
\begin{lemma}
    Fix $G$ a locally compact abelian group. Then the annihilator $\perp$ is a bijective and inclusion-reversing mapping from closed subgroups of $G$ to closed subgroups $\widehat G.$
\end{lemma}
This lemma is quite remarkable, but we won't spend time admiring it. We note that $\perp$ always outputs closed subgroups, so at least this restriction to closed subgroups makes sense.

We start by showing that $\perp$ is inclusion-reversing because this is easier. For closed subgroups $A\subseteq B$ of $G,$ then we note $\chi\in B^\perp$ if and only if $\chi(B)=\{1\}.$ But now $A\subseteq B,$ so certainly
\[\chi(A)=\{1\}.\]
It follows $\chi\in A^\perp,$ so we see $B^\perp\subseteq A^\perp.$

To show that $\perp$ is bijective, we actually show that $\perp$ is kind of involutive, which is stronger. That is, we fix a closed subgroup $A$ of $G$ and show $\left(A^\perp\right)^\perp=A,$ in the sense that these are equal under the Pontryagin duality correspondence. (The technicality is that $\left(A^\perp\right)^\perp$ lives in the character group of $\widehat G,$ which is isomorphic but not equal to $G.$) Anyways, unpacking the definitions, we are interested in
\[\left(A^\perp\right)^\perp=\Big\{\chi\in\widehat{\smash{\widehat G}\vphantom{1^]}\,}:\chi\left(A^\perp\right)=\{1\}\Big\}.\]
Using Pontryagin duality, we can parameterize characters of $\widehat G$ with $a\in G$ by the map $\chi\mapsto\chi(a).$ So we are actually interested in showing that
\[A\stackrel?=\left\{a\in G:\chi\in A^\perp\implies\chi(a)=1\right\}.\]
This is equivalent to showing that
\[A\stackrel?=\bigcap_{\chi(A)=\{1\}}\{a\in G:\chi(a)=1\}.\]
So essentially we have to show that we can determine a closed subgroup $A\subseteq G$ by looking entirely at the character group of $G.$ Note that surely $A$ is a subset of the right-hand side, so the difficult part is showing that if $g\in G$ has $\chi(g)=1$ for each $\chi\in A^\perp,$ then $g\in A.$

We do this by modding out $G$ by $A.$ Because $G$ is abelian, $A$ is normal, and because $A$ is closed, the quotient space is still Hausdorff. Additionally, $G/A$ inherits being locally compact from $G.$ Indeed, focus on the projection map $\pi:G\to G/A$ which is open and continuous by construction. Then for any $gA\in G/A,$ we have some compact neighborhood of $g$ named $K_g\subseteq G.$ Then
\[\pi^{-1}(K_g)\subseteq G/A\]
is also compact because an open cover of one makes an open cover of the other by $\pi.$

Now, considering the space $G/A$ means that it will suffice to show that
\[\bigcap_{\chi\in\widehat G'}\{a\in G:\chi(a)=1\}\stackrel?=\{e\}\]
in an arbitrary locally compact abelian group $G'.$ Indeed, this is enough because then if $\chi(g)\in A$ for each $\chi\in A^\perp,$ then $\chi\circ\pi^{-1}$ is a character on $G/A$ for which
\[\left(\chi\circ\pi^{-1}\right)(gA)=A.\]
Additionally, all characters on $G/A$ are of this form because $\chi:G/A\to S^1$ can be pulled back to $\chi\circ\pi:G\to S^1,$ which satisfies $\chi=\chi\circ\pi\circ\pi^{-1}.$ Anyways, the point is that all characters take $gA$ to the identity element $A,$ so we must actually have $gA=A,$ implying $g\in A.$

Anyways, we now show that
\[\bigcap_{\chi\in\widehat G'}\{a\in G:\chi(a)=1\}\stackrel?=\{e\}.\]
This follows quickly from Pontryagin duality. Suppose $g$ is in the left-hand set so that we want to show $g=e.$ Well, under Pontryagin duality, $g$ turns into a character on $\widehat G,$ taking $\chi\mapsto\chi(g).$ But then we're taking each $\chi$ to $1$ directly, so $g$ corresponds to the identity mapping! So indeed $g=e.$

Because $\perp$ isn't actually involutive but only under a canonical isomorphism, we provide the details for why we now know $\perp$ is bijective. We do this clumsily because I can't be bothered to find the correct way. For injectivity, suppose $A$ and $B$ are closed subgroups of $G$ for which $A^\perp=B^\perp.$ However, then we see
\[A=\bigcap_{\chi\in A^\perp}\{a\in G:\chi(a)=1\}=\bigcap_{\chi\in B^\perp}\{b\in G:\chi(b)=1\}=B.\]
For surjectivity, fix a closed subgroup $C$ of $\widehat G,$ and we note that
\[C=\bigcap_{a\in C^\perp}\big\{\chi\in\widehat G:a(\chi)=1\big\}.\]
However, Pontryagin duality lets us equivalently associate $a\in C^\perp$ characters of $\widehat G$ with elements of $G.$ So now elements $a\in C^\perp$ look like elements $a\in G$ for which $\chi(a)=1$ for each $\chi\in C$; i.e., $\{a\}^\perp\supseteq C.$ Thus,
\[C=\bigcap_{\{a\}^\perp\supseteq C}\big\{\chi\in\widehat G:\chi(a)=1\big\}.\]
Moving the intersection into a single set, we are essentially asserting that
\[C=\left\{a\in G:\{a\}^\perp\supseteq C\right\}^\perp.\]
It remains to show that the inner set is a closed subgroup of $G.$ Well, it consists of $a\in G$ for which $\chi(a)=1$ for each $\chi\in C.$ This condition is closed under group multiplication and inversion, so we have a subgroup. Additionally, we can write the set as
\[\bigcap_{\chi\in C}\{a\in G:\chi(a)=1\},\]
which shows that it is closed, for $G\setminus\{a\in G:\chi(a)=1\}=\chi^{-1}(S^1\setminus\{1\})$ is open. $\blacksquare$

Having studied the annihilator sufficiently, we are now ready to complete the proof of the theorem.
\begin{lemma}
    The map $\Psi$ is surjective.
\end{lemma}
Recall $F$ is complete with respect to its metric. So considering a convergent sequence in $\Psi(F),$ we can map this sequence back into $F,$ note that it must converge in $F,$ and then map the limit point back into $\Psi(F).$ (This only works because we know $\Psi$ is a homeomorphism onto its image.) Thus $\Psi(F)$ is closed.

To finish, we note that showing $\Psi(F)=\widehat F$ is equivalent to showing $\Psi(F)^\perp=\widehat F^\perp,$ from the theory developed above. We also know $\widehat F^\perp$ only consists of the trivial character, so we have to show that $\Psi(F)^\perp$ also only consists of the trivial character. Expanding out definitions, we want to show
\[\Psi(F)^\perp=\Big\{x\in\widehat{\smash{\widehat F}\vphantom{1^]}\,}:\chi\in\Psi(F)\implies x(\chi)=1\Big\}\stackrel?=\{e\}.\]
Using Pontryagin duality, this is the same as showing
\[\Psi(F)^\perp=\{x\in F:\chi\in\Psi(F)\implies\chi(x)=1\}\stackrel?=\{0\}.\]
However, $\Psi(F)$ is parameterized by $F,$ so the condition on $x$ is that $\Psi(a)(x)=1$ for all $a\in F.$ But $\Psi(a)(x)=\Psi(x)(a),$ so we require that $\Psi(x)$ be a trivial character, which requires $x=0$ because $\Psi$ is known to be injective. Thus, we are done here. $\blacksquare$

\subsubsection{March 2nd}
Today I learned a proof of the classification of finitely generated abelian groups from the Smith normal form, from \href{https://www.cs.uleth.ca/~holzmann/notes/abelian.pdf}{here}. The overarching idea is to view finitely generated abelian groups as ``just'' $\ZZ$-modules and then focus on $\ZZ$'s structure. That is, we note that $G$ is finitely generated and abelian if and only if there is a surjective homomorphism
\[\ZZ^n\to G\]
for some nonnegative integer $n.$ Formally, $G$ is finitely generated by $\{\alpha_1,\ldots,\alpha_n\}$ means that every element of $G$ can be written as a product of powers of the $\alpha_\bullet,$ but being abelian means that all elements can be rearranged to look like
\[\alpha_1^{\nu_1}\cdots\alpha_n^{\nu_n}\in G.\]
Taking $(\nu_1,\ldots,\nu_n)\in\ZZ^n$ to the above element is our surjective homomorphism. Anyways, the point is that we can understand $G$ by fixing $K\subseteq\ZZ^n$ the kernel of $\ZZ^n\to G$ so that
\[\frac{\ZZ^n}K\cong G.\]
In particular, it suffices to understand subgroups of $\ZZ^n.$

Before continuing, we need a size bound on $K.$
\begin{lemma}
    Any subgroup $K\subseteq\ZZ^n$ is finitely generated of dimension at most $n.$
\end{lemma}
This pretty much follows from induction. If $n=0,$ then $K\subseteq\ZZ^0=\{e\}$ must be the identity, so there is nothing to prove. Else suppose $n>1.$ The trick is to project onto the $n$th coordinate: note that
\[\pi_n:(k_1,\ldots,k_n)\mapsto k_n\]
is a homomorphism of groups into $\ZZ.$ All subgroups of $\ZZ$ take form $d\ZZ,$ so we fix $d_n$ a generator of this group; pick $(d_1,\ldots,d_n)\in K$ to be a corresponding element of $K.$ Now, for any $(k_1,\ldots,k_n)\in K,$ we know $k_n\in d_n\ZZ,$ so we can find $\ell\in\ZZ$ so that
\[(k_1,\ldots,k_n)-\ell(d_1,\ldots,d_n)=(k_1-\ell d_1,\ldots,k_{n-1}-\ell d_{n-1},0).\]
This operation of zeroing out the last coordinate commutes with inversion ($\ell\mapsto-\ell$) and addition (add the $\ell$s), so these elements are a subgroup of $K\cap\left(\ZZ^{n-1}\times\{0\}\right).$ That is, these elements are a subgroup of $\ZZ^{n-1},$ so there is a surjective homomorphism
\[\ZZ^{n-1}\to K\cap\left(\ZZ^{n-1}\times\{0\}\right).\]
Adding back in $(d_1,\ldots,d_n)$ will make the domain $\ZZ^{n-1}\oplus\ZZ\cong\ZZ^n$ and the image onto $K$ because all elements of $K$ had a representative in $K\cap\left(\ZZ^{n-1}\times\{0\}\right)\oplus\ZZ(d_1,\ldots,d_n).$ So we are done here. $\blacksquare$

With a size bound in hand, we can classify all subgroups $K.$
\begin{lemma}
    Any subgroup $K\subseteq\ZZ^n$ is isomorphic to some group $d_1\ZZ\times d_2\ZZ\times\cdots\times d_n\ZZ,$ where the $d_\bullet$ are integers satisfying $d_1\mid d_2\mid\cdots\mid d_n.$ Further, modding each group out from $\ZZ^n$ gives isomorphic quotients.
\end{lemma}
Note that we are permitting $d_\bullet=0,$ but this would force all components after $d_\bullet$ to also be $0.$ Using our size bound, the main idea here is to fix our generators $k_\ell:=(k_{1\ell},\ldots,k_{n\ell})$ for $1\le\ell\le n$ and then ``row-reduce'' the matrix
\[M:=\begin{bmatrix}
    k_{11} & k_{12} & \cdots & k_{1n} \\
    k_{21} & k_{22} & \cdots & k_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    k_{n1} & k_{n2} & \cdots & k_{nn}
\end{bmatrix}.\]
The point is that the image of $M$ under all $\ZZ^n$ is exactly $K,$ so now we're more or less solving a linear algebra problem. In particular, it will be sufficient to show that the image of $M$ is isomorphic to the image of some matrix that looks like
\[D=\begin{bmatrix}
    d_1 & 0 & \cdots & 0 \\
    0 & d_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_n
\end{bmatrix},\]
where $d_1\mid d_2\mid\cdots\mid d_n,$ and $\ZZ^n/D\ZZ^n\cong\ZZ^n/M\ZZ^n.$ It remains to show how we do this, but we note that it feels quite similar to row-reduction in linear algebra.

We begin with column operations because they aren't supposed to change the image. We note that we need to be somewhat careful because division is not allowed, so our column operations are the following.
\begin{itemize}
    \item We can swap two columns. Effectively this swaps vectors $k_a$ and $k_b$ in the basis of $K.$ Formally, the image doesn't change because for vector $v=(v_1,\ldots,v_n),$ we have
    \[Mv=\sum_{\ell=1}^nv_\ell k_\ell=\sum_{\substack{\ell=1\\\ell\ne a,b}}^nv_\ell k_\ell+k_bv_b+v_ak_a.\]
    That is, we take $(v_1,\ldots,v_b,\ldots,v_a,\ldots,v_n)$ to recover $Mv.$ Swapping back gives the reverse inclusion.
    \item We can negate a column. Suppose we took $k_a\mapsto-k_a.$ Formally, the image doesn't change because for vector $v=(v_1,\ldots,v_n),$ we have
    \[Mv=\sum_{\ell=1}^nv_\ell k_\ell=\sum_{\substack{\ell=1\\\ell\ne a}}^nv_\ell k_\ell+(-v_a)(-k_a).\]
    That is, we take $(v_1,\ldots,-v_a,\ldots,v_n)$ to recover $Mv.$ Negating back gives the reverse inclusion.
    \item We can add one column to another one. Effectively this changes the basis of $K$ from having a vector $k_a$ to $k_a+k_b.$ The image does not change because for vector $v=(v_1,\ldots,v_n),$ we have
    \[Mv=\sum_{\ell=1}^nv_\ell k_\ell=\sum_{\substack{\ell=1\\\ell\ne a,b}}^nv_\ell k_\ell+v_a(k_a+k_b)+(v_b-v_a)k_b.\]
    That is, we take $(v_1,\ldots,v_a,\ldots,v_b-v_a,\ldots,v_n)$ to recover $Mv.$ Negating $k_a,$ adding it back to $k_a+k_b,$ and negating $-k_a$ back to $k_a$ gives the reverse inclusion.
\end{itemize}
These turn out to be insufficient to get the result we want, so we consider row operations as well. These will change the subgroup $K,$ but we will get out an isomorphic one. Some extra care is required, however, to ensure modding out from $\ZZ^n$ still gives isomorphic quotients.
\begin{itemize}
    \item We can swap two rows. In effect, we take each basis vector $k_\ell=(k_{1\ell},\ldots,k_{a\ell},\ldots,k_{b\ell},\ldots,k_{n\ell})$ to a basis vector $k_\ell'=(k_{1\ell},\ldots,k_{b\ell},\ldots,k_{a\ell},\ldots,k_{n\ell})$ of some group $K'.$ In fact, name this operation $\sigma,$ and it is our isomorphism.
    
    We don't show this formally, but it is a homomorphism because one can add and then swap or swap and then add; it is surjective by definition of $K'$; it is injective because the only vector swapping to $(0,\ldots,0)$ would have to start with all $0$s to begin with.
    
    We also have $\ZZ^n/K\cong\ZZ^n/K'$ because the isomorphism extends to $\ZZ^n$: take $\ZZ^n$ to $\ZZ^n$ via $\sigma$ and then mod out by $K$; the kernel is $K'.$
    \item We can negate a row. In effect, we take each basis vector $k_\ell=(k_{1\ell},\ldots,k_{a\ell},\ldots,k_{n\ell})$ to a basis vector $k_\ell'=(k_{1\ell},\ldots,-k_{a\ell},\ldots,k_{n\ell})$ of some group $K'.$ In fact, name this operation $\sigma,$ and it is our isomorphism.
    
    Again, we don't show this formally, but it is a homomorphism because negation distributes over addition; it is surjective by definition of $K'$; it is injective because to get to $(0,\ldots,0),$ we can negate back to see the original vector had to be all $0$s.
    
    We still have $\ZZ^n/K\cong\ZZ^n/K'$ for the same reason: take $\ZZ^n$ to $\ZZ^n$ via $\sigma$ and then mod out by $K$; the kernel is $K'.$
    \item We can add one row to another row. In effect, we take each basis vector $k_\ell=(k_{1\ell},\ldots,k_{a\ell},\ldots,k_{n\ell})$ to a basis vector $k_\ell'=(k_{1\ell},\ldots,k_{a\ell}+k_{b\ell},\ldots,k_{n\ell})$ of some group $K'.$ In fact, name this operation $\sigma,$ and it is our isomorphism.
    
    We still don't show this formally, but it is a homomorphism because we can add the $k_{b\bullet}$ elements before or after; it is surjective by definition of $K'$; it is injective because to get to $(0,\ldots,0),$ we must have had $k_b=0,$ so we must have had $k_a=0,$ and everything else would have to be $0$ automatically.
    
    And of course, $\ZZ^n/K\cong\ZZ^n/K'$ because we can take $\ZZ^n$ to $\ZZ^n$ via $\sigma$ and then mod out by $K$; the kernel is $K'.$
\end{itemize}
More or less, we can picture these operations as applying some module transformation on $K,$ which does change $K,$ but not in any meaningful way.

Anyways, it remains to show how we use these column and row operations to ``row-reduce.'' We take the following definition.
\begin{definition}
    A matrix $D\in\ZZ^{n\times n}$ is said to be in Smith normal form if it looks like
    \[D=\begin{bmatrix}
        d_1 & 0 & \cdots & 0 \\
        0 & d_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & d_n
    \end{bmatrix},\]
    where $d_1\mid d_2\mid\cdots\mid d_n.$
\end{definition}
So we have the following (sub)lemma, which will complete the proof of the lemma as discussed above.
\begin{lemma}
    The discussed column and row operations are sufficient to transform any matrix $M\in\ZZ^{n\times n}$ to Smith normal form.
\end{lemma}
We induct on $n.$ If $n=1$ (or $n=0$), there is nothing to show. For the inductive step, it suffices to show that column operations can turn $M$ into a matrix
\[\begin{bmatrix}
    d & 0 \\
    0 & M'
\end{bmatrix},\]
where each element of $M'$ is divisible by $d,$ From here, we apply the inductive hypothesis on $M',$ for column and row operations can't change the fact that every entry is divisible by $d,$ so every element of the Smith normal form of $M'$ will be divisible by $d.$

We divide the reduction into two parts.
\begin{itemize}
    \item We begin by noting that we can certainly transform any matrix $M$ into the form
    \[\begin{bmatrix}
        d & 0 \\
        0 & M'
    \end{bmatrix},\]
    without requiring each element of $M'$ to be divisible by $d.$ This is done by repeated division algorithm: because we are allowed to negate and repeated add columns, we can apply the division algorithm to entries of the first row and column.
    
    If all elements are $0$ already, we're done. Else swap columns or rows to make the top-left element $d$ nonzero. Now, using division by $d,$ each element of the first column and row will be smaller (in absolute value) than $d.$ If they're all $0,$ then we're done; else swap to replace the current $d$ with some smaller (in absolute value) nonzero value and repeat the divisions. Note that the top-left position is always nonzero.
    
    We always have the top-left position at least each element in the first row and column, and this process requires the top-left element to strictly decrease (in absolute value) while always being nonzero. Because $\ZZ$ is well-ordered, this must terminate, and it the top-left position will terminate to a nonzero value.
    \item Now suppose that we are in the form
    \[\begin{bmatrix}
        d & 0 \\
        0 & M'
    \end{bmatrix},\]
    but some element in $M'$ is not divisible by $d.$ Well, if an element in the $k$th row isn't divisible by $d,$ then add the $k$th row to the first, and apply the first step. If still someone isn't divisible by the new $d,$ we can repeat the process. Note that the fact some element is not divisible by $d$ means that this element is nonzero, so the new value of $d$ will surely be nonzero.
    
    We note further that adding the $k$th row to the first does not change $d.$ But if $d$ starts nonzero (as it must be after the first iteration of this process), then the resulting divisions in the above process imply that the value of $d$ will decrease (in absolute value) while still being nonzero. So $d$ is decreasing (in absolute value) and nonzero, so well-order of $\ZZ$ requires this to terminate.
\end{itemize}
Applying the first process and then repeatedly applying the second process will eventually terminate to the desired form with each element of $M'$ divisible by $d.$ This completes the proof of the lemma. $\blacksquare$

It remains to turn this into a classification of finitely generated abelian groups. This follows from the discussion at the beginning along with our classification of subgroups of $\ZZ^n.$
\begin{theorem}
    Suppose $G$ is finitely generated (of dimension $\le n$) and abelian. Then there exist integers $d_1\mid d_2\mid\cdots\mid d_n$ such that
    \[G\cong\prod_{k=1}^n\ZZ/d_k\ZZ.\]
\end{theorem}
Because $G$ is finitely generated (of dimension $\le n$) and abelian, we have a surjective homomorphism
\[\ZZ^n\to G.\]
Let $K$ be the kernel of this homomorphism so that $G\cong\ZZ^n/K$. Because of our work with the Smith normal form, we know that $K$ is isomorphic to a a group of the form $\prod_{k=1}^nd_k\ZZ$ such that $d_1\mid d_2\mid\cdots\mid d_n,$ and even
\[G\cong\frac{\ZZ^n}K\cong\frac{\ZZ^n}{d_1\ZZ\times\cdots\times d_n\ZZ}.\]
It is a fact that $(A\times B)/(C\times D)\cong A/C\times B/D$ (take $A\times B$ to $A/C\times B/D$ by modding; the kernel is $C\times D$). Applying this inductively gives
\[G\cong\prod_{k=1}^n\ZZ/d_k\ZZ.\]
Do note that $d_\bullet=0$ is permitted, in which case $\ZZ/0\ZZ\cong\ZZ.$ This is what we wanted, so we are done. $\blacksquare$

I am told this generalizes nicely to general modules over PIDs. I can see where some of the generalization is possible, but the existence of the Smith normal form made heavy use of the division algorithm, which is nontrivially stronger than PID structure. I suspect there are ways to go around the division algorithm, however, by (for example) considering the ideal generated by all elements in a row or column and then extracting its generator.

\subsubsection{March 3rd}
Today I learned (formally) about locally constant functions. We take the following abstract definition.
\begin{definition}
    For topological space $X$ and set $S,$ we say $f:X\to S$ is locally constant if and only if, for each $x\in X,$ there exists an open neighborhood $U$ of $x$ such that $f(X)=\{f(x)\}.$
\end{definition}
It is a fact (that we will prove) that locally constant functions on $\RR$ are in fact constant everywhere. This is not hard to visualize: if every point has a little interval around it where $f$ is constant, then we can imagine extending out these intervals out to all of $\RR.$ Note how similar this feels to compactness.

Anyways, we claim the following, which is stronger. The following is (almost) a characterization of locally constant functions.
\begin{proposition}
    If $f:X\to S$ is locally constant, then $f$ is locally constant on connected components of $X.$
\end{proposition}
Fix any $x\in X,$ and we show that $f$ is constant on the connected component of $x.$ In fact, we claim that
\[Y:=\{y\in X:f(y)=f(x)\}\]
is both closed and open. This is open because, for any $y\in Y,$ we can select an open neighborhood $U_y$ of $y$ on which $f$ is locally constant. Then each $U_y\subseteq Y,$ and surely
\[Y=\bigcup_{y\in Y}U_y.\]
So $Y$ is open.

Conversely, this is closed because the complement of $Y$ is
\[\{y\in X:f(y)\in S\setminus\{f(x)\}\}=\bigcup_{s\in S\setminus\{f(x)\}}\{y\in X:f(y)=s\}.\]
We already know that each $\{y\in X:f(y)=s\}$ is open by either the above argument or because it is empty, so it follows $X\setminus Y$ in total is open.

To finish, let $C_x$ be the connected component of $x\in X.$ This implies that both $C_x\cap Y$ and $C_x\cap(X\setminus Y)$ are open in the subspace of topology of $C_x,$ and these two open sets are disjoint and partition $C_x.$ So because $C_x$ is by definition connected, we must have one of $C_x\cap Y$ or $C_x\cap(X\setminus Y)$ be empty, but $x\in C_x\cap Y,$ so
\[C_x\cap(X\setminus Y)=\emp.\]
But this requires $C_x\subseteq Y,$ so $f(C_x)=\{f(x)\}.$ So we are done here. $\blacksquare$

We remark now that the connectedness of $\RR$ implies what we wanted, and no compactness condition was necessary. Of course, compactness provides a nice visual for what the argument above is doing, but it is less efficient. Anywyas, if we wanted to rigorize the compactness argument, I think the least-upper principle on $Y$ can get $Y=\RR$ quickly.

We also note here that this characterization of locally constant is pretty unhelpful on totally disconnected spaces. (We do care about totally disconnected spaces because of $\QQ_p.$) To fix our intuitive picture, I think it's helpful to attach the discrete topology on $S$ and visualize $f$ as continuous. That is, ``locally constant'' is somehow similar to ``continuous'' on totally disconnected spaces.

To be explicit, Tate's thesis is concerned with Fourier analysis on ``smooth'' functions on which ``tend to $0$ rapidly.'' On $\RR$ or $\CC,$ we can just say that $f:\RR\to\CC$ is in $C^\infty$ and has all of its derivatives go to $0.$ But on local fields, we make ``smooth'' into ``locally constant'' and ``tend to $0$ rapidly'' into ``compact support.'' We have the following characterization; read as $F=\QQ_p.$
\begin{proposition}
    Fix $F$ a non-Archimedean topological field. Suppose $f:F\to\CC$ is locally constant and has compact support. Then $f$ is a $\CC$-linear combination of indicators on open sets.
\end{proposition}
For $x\in F,$ denote $B_x$ an open ball (inside an open neighborhood) around $x$ such that $f$ is constant. We would like to just sum indicators on all of these open balls, but there are infinitely many. The main trick is that
\[\op{supp}f\subseteq\bigcup_{x\in\op{supp}f}B_x,\]
so we have an open cover of $\op{supp}f.$ So we are allowed to extract a finite subcover named $\{B_{x_k}\}_{k=1}^N.$ Because $F$ is non-Archimedean, these balls are either disjoint or contain each other, so we can refine our cover to consist of disjoint balls. From this we claim
\[f=\sum_{k=1}^Nf(x_k)\cdot1_{B_{x_k}}.\]
Note that this will finish the proof of the proposition. Indeed, for $x\in F,$ if $x$ is in no open ball $B_{x_\bullet},$ then $x\not\in\op{supp}f,$ so $f(x)=0$ anyways. But if $x\in B_{x_\bullet}$ for some $x_\bullet,$ then $x_\bullet$ is in exactly one such ball by disjointness. Further, $f$ is constant on $B_{x_\bullet},$ so $f(x)=f(x_\bullet),$ and our claim is finished. $\blacksquare$

\subsubsection{March 4th}
Today I learned some examples of $p$-adic geometry being horrible, from \href{https://scholar.rose-hulman.edu/rhumj/vol8/iss1/2/}{this undergraduate paper}. For example, we have the following.
\begin{proposition}
    No three distinct points in $\QQ_p$ are collinear.
\end{proposition}
The meaning of ``collinear'' is unclear here because $\QQ_p$ doesn't have a clear geometry, but we will take lines as geodesics. That is, if $a,b,c\in\QQ_p$ all lie on the same line with (say) $b$ between $a$ and $c,$ then the shortest path between $a$ and $c$ goes through $b.$ Do we must have
\[|a-c|_p=|a-b|_p+|b-c|_p\]
by following the path through $b.$ This is not a totally rigorous argument, but it will serve fine as a definition for collinear. Anyways, it's not hard to see there are no three distinct $a,b,c\in\QQ_p$ satisfying this by the ultrametric inequality. Indeed, we have
\[|a-c|_p\le\max\{|a-b|_p,|b-c|_p\}<|a-b|_p+|b-c|_p=|a-c|_p,\]
which is our contradiction. In particular, we have a $<$ because $a,b,c$ are distinct, implying all distances are positive. $\blacksquare$

Visually, I see the above proposition as a geometric way to view ``totally disconnected.'' As in, we do have a metric and notion of distance, but this distance is quite discrete in $\QQ_p$ and gives very separated space. Lines would require some connections between points, which doesn't exist in $\QQ_p.$ We also remark that the above proof works for any ultrametric space.

However, I find the following more interesting with respect to $\QQ_p$'s geometry.
\begin{proposition}
    At most $p$ distinct points in $\QQ_p$ are equidistant.
\end{proposition}
We note that equality is possible: choose points in each coset $\ZZ_p/p\ZZ_p,$ and then each will differ by some element in $\ZZ_p\setminus p\ZZ_p,$ making each point a distance $1$ from each other point. Here's a picture in $\ZZ_5.$
\begin{center}
    \begin{asy}
        unitsize(1.5cm);
        int p = 5;
        pair[] ps = new pair[5];
        for(int i = 0; i < p; ++i)
        {
            draw(circle( dir(360.0/p * i + 90), 2/3*sin(3.14159/p) ));
            label("$"+string(i)+"$", (2-2/3*sin(3.14159/p))*dir(360.0/p * i + 90));
            // some kind-of mersenne twister to psuedorandomly choose point placement
            ps[i] = dir(360.0/p * i + 90) + 1/2*sin(3.14159/p)*sin((i+1)*(i+7)) * dir(360*3.14159*(i+1)*(i+2));
            dot(ps[i]);
        }
        for(int i = 0; i < p; ++i)
            for(int j = 0; j < i; ++j)
                draw(ps[i]--ps[j], dashed);
    \end{asy}
\end{center}
This equality case actually tells us where to look: if there are more than $p$ elements, we should try to force two elements into some small coset. In fact, we will manipulate our sequence to fit nicely into $\ZZ_p/p\ZZ_p.$

Suppose we have $n$ points $a_0,\ldots,a_{n-1}\in\QQ_p.$ To make things easier to handle, we note that
\[|a-b|_p=|(a-c)-(b-c)|_p,\]
so shifting all points by some constant $c$ will not change being equidistant. So we subtract $a_0$ from all points and relabel our sequence to $0,a_1,\ldots,a_{n-1}.$

We would like to force our nonzero points in $\ZZ_p\setminus p\ZZ_p,$ so we write $a_\bullet=b_\bullet p^{\nu_\bullet}$ with $b_\bullet\in\ZZ_p\setminus p\ZZ_p$ for $\bullet>0.$ We want to get rid of the $p^{\nu_\bullet},$ so we fix let $\nu$ be the minimum of the $\nu_\bullet$s and get ready to divide by $p^\nu.$ Namely, we see
\[|c|_p\cdot|a-b|_p=|ac-bc|_p,\]
so multiplying all points by some constant $c$ will not change being equidistant either. Thus, we reorder our sequence to put $\nu_1=\nu$ and then multiply through by $p^{-\nu}.$ Relabeling our sequence, we now have $0,a_1,\ldots,a_{n-1}$ where $a_1\in\ZZ_p\setminus p\ZZ_p$ and each $a_\bullet\in\ZZ_p$ for each $\bullet>0.$

Now we finish. Note that $|a_1-0|_p=1$ because $a_1\in\ZZ_p\setminus p\ZZ_p,$ implying that all points must be a distance of $1$ from each other because they're equidistant. In other words, for any two points $a_k$ and $a_\ell,$ we must have $\nu_p(a_k-a_\ell)=0,$ or
\[a_k-a_\ell\not\in p\ZZ_p.\]
However, this forces all $n$ points to be in different cosets of $\ZZ_p/p\ZZ_p,$ of which there are only $p.$ Thus, the pigeonhole principle forces $n\le p,$ and we are done. $\blacksquare$

The proposition is written as an ``at most'' statement, but I find it remarkable that the equality case exists at all. The fact that we can embed the complete graph $K_p$ in $\QQ_p$ (and with ease!) makes its geometry feel super weird. For comparison, $\RR$ can only do $2,$ and $\CC$ can only do $3$; and typically, I think this many equidistant points is visualized as a $p$-simplex in $\RR^p,$ which $\QQ_p$ feels very different from.

\subsubsection{March 5th}
Today I learned about multiplicative (quasi)characters of local fields.\todo{drp}

\subsubsection{March 6th}
Today I learned a little motivation for the compact-open topology: the evaluation map is continuous, from \href{https://ncatlab.org/nlab/show/compact-open+topology}{the nlab page}. That is, we have the following proposition.
\begin{proposition}
    For locally compact Hausdorff space $X$ and space $Y,$ the map $\op{ev}:X\times\op{Hom}(X,Y)\to Y$ by $(x,f)\mapsto f(x)$ is continuous.
\end{proposition}
Here, $\op{Hom}(X,Y)$ means the set of continuous functions $X\to Y,$ equipped with the compact-open topology. Also, we are ordering the arguments like this to have a future with currying. However, I did not do currying today.

Fix $U_Y\subseteq Y$ any open set. We need to show that its preimage under $\op{ev}$ is open. We note that it suffices to show, for each $(x,f)\in\op{ev}^{-1}(U_Y),$ that there is an open neighborhood $U_{(x,f)}$ around $(x,f)$ such that $U_{(x,f)}\subseteq\op{ev}^{-1}(U_Y).$ Indeed, we could then write
\[\op{ev}^{-1}(U_Y)=\bigcup_{(x,f)\in\op{ev}^{-1}(U_Y)}U_{(x,f)},\]
which is manifestly open. In fact, this condition is equivalent to $\op{ev}^{-1}(U_Y)$ being open, for it if were open, then we could just set $U_{(x,f)}=\op{ev}^{-1}(U_Y).$ Anyways, the point is that we are able to look locally at points $(x,f)$ of $X\times\op{Hom}(X,Y).$

It remains to exhibit $U_{(x,f)}.$ This comes down to the interaction of $X$ being locally compact with the compact-open topology. For any function $f,$ we can use the continuity of $f$ to yield an open set $f^{-1}(U_Y),$ which in fact contains $x.$ The reason we do this is to use local compactness: now we get an open set $U_x$ and compact $V$ such that
\[x\in U_x\subseteq V\subseteq f^{-1}(U_Y).\]
Read this definition of local compactness as essentially meaning that we have a basis of compact sets around $x.$

We still have to use the compact-open topology. Well, we have an instantiated a compact $V\subseteq X$ in our input space, and we have an open set $U_Y\subseteq Y$ of the output space, so we note that
\[\{g\in\op{Hom}(X,Y):g(V)\subseteq U_Y\}\]
is open in $\op{Hom}(X,Y).$ So we claim that
\[U_{(x,f)}:=U_x\times\{g\in\op{Hom}(X,Y):g(V)\subseteq U_Y\}\]
suffices. Certainly this open in the product topology. Further, $x\in U_x$ and $f(V)\subseteq f(f^{-1}(U_Y))\subseteq U_Y,$ so $U_{(x,f)}$ is indeed a neighborhood of $(x,f).$ And finally,
\[\op{ev}(U_{(x,f)})=\bigcup_{g(V)\subseteq U_Y}g(U_x)\subseteq\bigcup_{g(V)\subseteq U_Y}g(V)\subseteq U_Y,\]
which is what we wanted. So we have exhibited our local neighborhoods, and we are done. $\blacksquare$

Of course, this is not really motivation for the compact-open topology. We could replace ``locally compact'' with something like ``locally open'' and the topology on $\op{Hom}(X,Y)$ with an ``open-open'' topology, and the proof would still work fine.

However, the fact that w are focusing on mostly locally compact topologies in what I'm reading (e.g., for Pontryagin duality) motivates the ``locally compact'' condition for me, which makes the compact-open topology feel more natural given the above argument.

\subsubsection{March 7th}
Today I learned that determining if a single-variable (!) algebraic expression is identically $0$ is uncomputable. The main lemma is to create a function $\RR\to\RR^2$ with dense image, only using the closed-form expressions (functions made by $\op{id},\exp,\sin,\QQ,\pi,$ and addition/multiplication/composition of these). The main idea is that $y=\sin\left(x^2\right)$ oscillates with increasing frequency, so the graph of (say) $\sin\left(x^2\right)\pmod1$ would be dense in $[0,1]^2.$

However, we need to extend the $x$ and $y$ coordinates we are allowed. We need some kind of modular arithmetic, so we need to repeat over all intervals infinitely often, so we set $x(t)=t\sin t,$ which will relayer over our time intervals.

But now to cover with increasing vertical oscillations, $y(t)=t\sin t^2$ is no longer quite good enough because our $x$ speed is also increasing. So we choose $t\sin t^3.$ To make some of the computations easier, we actually choose the following function.
\begin{lemma}
    The function
    \[\left(t\cos(2\pi t),t\sin\left(16\pi t^3\right)\right)\]
    has image dense in $\RR^2.$
\end{lemma}
The idea in the proof is that we can cover large intervals using the highly oscillatory $y$ coordinate, merely letting the $x$ coordinate move us around. Let $x(t)=t\cos(2\pi t),$ and $y(t)=t\sin\left(16\pi t^3\right).$ To give you an idea of how this behaves, here is the curve from $t=1$ to $t=1.5.$
\begin{center}
    \begin{asy}
        import graph;
        unitsize(1cm);
        real x(real t)
        {
            return t*cos(2*3.14159*t);
        }
        real y(real t)
        {
            return t*sin(16*3.14159*t*t*t);
        }
        draw(graph(x,y,1,1.5));
        dot("$(-1.5,0)$",(-1.5,0), W);
        dot("$(1,0)$",(1,0), E);
    \end{asy}
\end{center}

We only consider time intervals like $[N,N+0.5]$ for sufficiently large $N\in\NN,$ and we are going to bound the horizontal length of a period of the $y$ coordinate to bound how dense our curve is. The reason for choosing $[N,N+0.5]$ is that $x(t)$ moves from $x(N)=N$ to $x(N+0.5)=-(N+0.5),$ and $y(t)$ has periods overcoming $N.$

Anyways, the start of periods of $y(t)$ occur at times when $16\pi t^3=2\pi k$ for some $k\in\ZZ,$ implying $t=\sqrt[3]k/2.$ So the horizontal displacement over a single period is
\[d_k=\left|x\left(\frac{\sqrt[3]{k+1}}2\right)-x\left(\frac{\sqrt[3]k}2\right)\right|.\]
This is somewhat obnoxious to bound directly, so we bound it stupidly. Note $x(t)$ is infinitely differentiable and whatnot, so we can write this displacement as
\[d_k=\left|\int_{\sqrt[3]k/2}^{\sqrt[3]{k+1}/2}x'(t)\,dt\right|\le\int_{\sqrt[3]k/2}^{\sqrt[3]{k+1}/2}|x'(t)|\,dt.\]
Now, $x'(t)=\cos(2\pi t)-2\pi t\sin(2\pi t),$ so $|x'(t)|\le1+2\pi t.$ In particular, all of our time values are positive because our time interval $N\in\NN$ is positive. Over the interval $[N,N+0.5],$ we can therefore absolutely bound $|x'(t)|\le1+2\pi(N+0.5).$ Thus,
\[d_k\le\int_{\sqrt[3]k/2}^{\sqrt[3]{k+1}/2}(1+\pi+2\pi N)\,dt=\frac{\sqrt[3]{k+1}-\sqrt[3]k}2\cdot(1+\pi+2\pi N).\]
To make computations easier, we note that $8N\ge1+\pi+2\pi N$ for $N\ge\frac{1+\pi}{8-2\pi}.$ So we can say
\[d_k\le2N\left(\sqrt[3]{k+1}-\sqrt[3]k\right)\]
for sufficiently large $N.$ Note the function $f(k)=\sqrt[3]{k+1}-\sqrt[3]k$ is strictly decreasing over $k\ge0$ because its derivative is $f'(k)=\frac1{3(k+1)^{2/3}}-\frac1{3k^{2/3}},$ which is shown to be negative after some rearranging. Anyways, the point is that
\[\sqrt[3]{k+1}-\sqrt[3]k\le\sqrt[3]{N^3+1}-N\]
because our periods needed to take place in $[N,N+0.5].$ We are assuming that a single period takes place in $[N,N+0.5],$ which requires $\sqrt[3]{N^3+1}\le N+0.5.$ But $(N+0.5)^3\ge N^3+1.5N^2,$ so $(N+0.5)^3\ge N^3+1$ for $N\ge1.$ So for sufficiently large $N,$
\[d_N\le2N\left(\sqrt[3]{N^3+1}-N\right)\]
over an entire interval in $[N,N+0.5].$ This bound is empirically huge, but it will work.

In particular, this displacement bound goes to $0$ as $N\to\infty.$ Indeed, we see
\[L=\lim_{N\to\infty}2N\left(\sqrt[3]{N^3+1}-N\right)=2\lim_{x\to0^+}\frac{\sqrt[3]{(1/x)^3+1}-1/x}x.\]
The limit now factors as
\[\frac L2=\lim_{x\to0^+}x\cdot\frac{\sqrt[3]{x^3+1}-1}{x^3}.\]
The $x$ of the limit vanishes. The fraction is the derivative of $g(x)=\sqrt[3]{x+1}$ at $0,$ which is $g'(0)=\frac13\cdot1^{-2/3}<\infty.$ So the entire limit goes to $0.$

This is all abstract, but the point is that, over the interval $[N,N+0.5],$ we cover the horizontal interval $[-N-0.5,N]\supseteq[-N,N]$ with vertical oscillations that are at least $N$ high and with frequency approaching $\infty.$ Showing density in $\RR^2$ from here is a matter of conversion.

Indeed, suppose we want to hit some arbitrary open ball $B((x_0,y_0),\varepsilon)$ in $\RR^2.$ Fix $N\in\NN$ a sufficiently large integer to consider the time interval $[N,N+0.5]$: ensure $N\ge|y_0|,|x_0\pm\varepsilon/2|$ so that we cover the point, and ensure that the maximum horizontal distance between vertical oscillations (named $d_N$) is less than $\varepsilon.$ The maximum horizontal distance goes to $0,$ so this is legal.

Now, the interval $[x_0-\varepsilon/2,x_0+\varepsilon/2]\subseteq[-N,N],$ and these $x$ coordinates are covered by $x(t)$ over $[N,N+0.5].$ Further, the horizontal distance over this interval is
\[x_0+\varepsilon/2-x_0+\varepsilon/2=\varepsilon>d_N\]
by construction of $N,$ so there must be a full vertical oscillation in $[x_0-\varepsilon/2,x_0+\varepsilon/2].$ Because a full period of $y(t)$ will cover at least $[-N,N]\supseteq[-|y_0|,|y_0|]\ni y_0,$ continuity of $y$ gives a time value $t_0\in[N,N+0.5]$ for which
\[\begin{cases}
    x(t_0)\in[x_0-\varepsilon/2,x_0+\varepsilon/2], \\
    y(t_0)=y_0.
\end{cases}\]
The distance between $(x,y)$ and $(x(t_0),y(t_0))$ is purely horizontal and bounded by $\varepsilon/2<\varepsilon.$ So we are done. $\blacksquare$

If we wanted to be more formal about this, we could manually extract time values $t_1$ and $t_2$ for a full period of $y(t),$ look at the peaks of this period, see that they exceed $|y_0|$ in absolute value, and use continuity to argue for points with matching $y$ coordinate. I think the presented argument is sufficiently rigorous.

Anyways, as a corollary, we can extend this to arbitrary dimension.
\begin{proposition}
    For positive integers $n,$ there exist closed-form functions $x_1,\ldots,x_n:\RR\to\RR$ such that $t\mapsto(x_1(t),\ldots,x_n(t))$ is dense in $\RR^n.$
\end{proposition}
For this, we induct. At $n=1,$ choose $x_1(t)=t.$ Further, $n=2$ case is given above; let $x(t)$ and $y(t)$ be the functions from the $n=2$ case.

Now suppose we have $n$ closed-form functions $x_1,\ldots,x_n$ with image dense in $\RR^n,$ and we want to make $n+1$ closed-form functions with image dense in $\RR^{n+1}.$ The idea is to split the coordinate $x_n$ into $x_n$ and $x_{n+1}$ with the $n=2$ case. That is, we claim that
\[t\longmapsto\big(x_1(t),x_2(t),\ldots,x_{n-1}(t),\,x(x_n(t)),y(x_n(t))\big)\]
has image dense in $\RR^{n+1}.$ Indeed, fix any ball $B((p_1,\ldots,p_{n+1}),\varepsilon)\subseteq\RR^{n+1}.$ Focusing on the final two coordinates, there is some time value $t_0$ such that $(x(t_0),y(t_0))$ is within $\varepsilon/2$ of $(p_n,p_{n+1}).$ Tracking the preimage of the ball $B((p_n,p_{n+1}),\varepsilon/2),$ there must be an entire interval $(t_1,t_2)\ni t_0$ which maps into $B((p_n,p_{n+1}),\varepsilon/2).$

It follows that we would like
\[\begin{cases}
    (x_1(t),\ldots,x_{n-1}(t))\in B((p_1,\ldots,p_{n-1}),\varepsilon/2), \\
    x_n(t)\in(t_1,t_2).
\end{cases}\]
Indeed, this would make the distance between $\big(x_1(t),\ldots,x_{n-1}(t),x(x_n(t)),y(x_n(t))\big)$ and $(p_1,\ldots,p_{n+1})$ be bounded above by $\sqrt{(\varepsilon/2)^2+(\varepsilon/2)^2}<\varepsilon,$ which is what we want. In particular, $(x(x_n(t)),y(x_n(t)))$ would have distance less than $\varepsilon/2$ from $(p_n,p_{n+1}).$

Anyways, we can combine the two desired conditions into asserting
\[(x_1(t),\ldots,x_{n-1}(t),x_n(t))\in B((p_1,\ldots,p_{n-1}),\varepsilon/2)\times(t_1,t_2).\]
This desired set is contains by the ball
\[B\left(\left(p_1,\ldots,p_{n-1},\frac{t_1+t_2}2\right),\min\left(\frac\varepsilon2,\frac{t_2-t_1}2\right)\right),\]
so because $(x_1(t),\ldots,x_n(t))$ has image dense in $\RR^n,$ there is a value of $t$ with output in that open ball. So we are done. $\blacksquare$

We also note that the inductive process presented above means that we can generate these dense functions computably. Our construction is really just
\[(x(t),x(y(t)),x(y(y(t))),\ldots).\]
This is somewhat important for rigor, but it doesn't matter too much.

Anyways, the punch-line is the following.
\begin{theorem}
    There is no computable way to determine if a single-variable, closed-form algebraic expression is always nonnegative.
\end{theorem}
Back in the PROMYS 2021 logic seminar, we computably constructed a closed-form algebraic expression $E_e$ that was always nonnegative if and only if the polynomial $p_e$ had no integer roots. (Here, $p_e$ was the degree-$4$ representation of the square of a Hilbert's 10th polynomial, which has roots if and only if $\varphi_e(0)=0,$ where $\varphi_\bullet$ is an encoding of a Turing machine.) For concreteness, it was
\[E_e(x_1,\ldots,x_n)=p_e(x_1,\ldots,x_n)+\sum_{\ell=1}^k|\sin(\pi x_\ell)|-\varepsilon,\]
where
\[\varepsilon=\exp\left(-1-\sum_{a,b,c,d=1}^n|p_{abcd}|(M+1)^4\right),\]
with $p_{abcd}$ the coefficient of the $x_ax_bx_cx_d$ term of the polynomial $p_e.$ In particular, one can show that $E_e$ has any negative portions if and only if the polynomial $p_e$ has roots, via some aggressive bounding.

Anyways, we note now that we can let the variables $x_1,\ldots,x_n$ of $E_e$ be our functions $x_1(t),\ldots,x_n(t),$ which will turn the expression $E_e$ into one that has a single variable $t.$ Name this new expression $E_e'.$ Note that this transition can be done computably by pattern-matching the $x_\bullet$ with the computable representation of our dense functions given above.

It remains to show that the new expression $E_e'$ is ever negative if and only if $E_e$ is ever negative. This comes down to continuity.
\begin{itemize}
    \item If $E_e'$ is ever negative so that $t$ gives $E_e'(t)<0,$ then the corresponding point $(x_1(t),\ldots,x_n(t))$ makes $E_e$ negative by construction.
    \item If $E_e$ is ever negative, then the preimage $E_e^{-1}((-\infty,0))$ is non-empty. Because $E_e:\RR^n\to\RR$ is a continuous function (all closed-form functions are continuous), this preimage must be open. A non-empty open set contains a nontrivial ball (because balls are a basis), so there is a ball
    \[B((p_1,\ldots,p_n),\varepsilon)\]
    over which $E_e$ is negative. Because our path $(x_1(t),\ldots,x_n(t))$ is dense in $\RR^n,$ there is a time value $t$ which lives in that ball, which makes $E_e'(t)=E_e((x_1(t),\ldots,x_n(t)))<0.$
\end{itemize}
Thus, $E_e\ge0$ if and only if $E_e'\ge0,$ which is what we wanted. $\blacksquare$

We have the following theorem, stated at the beginning, as a corollary.
\begin{theorem}
    There is no computable way to determine if a single-variable, closed-form algebraic expression is equal to $0.$
\end{theorem}
If we could computably determine if a single-variable, closed-form expression $E(t)$ was equal to $0,$ then we could query if
\[E(t)-|E(t)|\]
is equal to $0,$ which is $0$ if and only if $E(t)\ge0.$ That is, an algorithm to determine if a closed-form expression is identically $0$ can be used for an algorithm to determine if a function is at least $0.$ The latter is not computable, so the former is not computable either. $\blacksquare$

As an aside, we note that if we could test $E(t)\ge0,$ then testing $E(t)\ge0$ and $-E(t)\ge0$ tests if $E(t)$ is identically $0.$ So testing if an expression is always nonnegative is just as hard as testing if an expression is identically $0.$

Anyways, I find this somewhat astounding. Determining if an expression is identically $0$ feels like it should be a relatively tractable problem: just test a whole bunch of values, maybe do some algebraic manipulation, and it should be doable. But it turns out that if you could determine if arbitrary (disgusting) expressions are identically $0,$ you could solve the halting problem.

\subsubsection{March 8th}
Today I learned some more solid motivation for the compact-open topology, again from the \href{https://ncatlab.org/nlab/show/compact-open+topology}{nlab page}. In particular, this time compactness actually matters. The following is our claim.
\begin{proposition}
    Let $X,Y,Z$ be topological spaces. Suppose $f:X\times Y\to Z$ is continuous. Then the map $\op{curry}:X\to\op{Hom}(Y,Z)$ defined by $x\mapsto[y\mapsto f((x,y))]$ is a continuous map, where $\op{Hom}(Y,Z)$ is equipped with the compact-open topology.
\end{proposition}
What's remarkable here is that $X,Y,Z$ have no hypotheses other than being topological spaces. In particular, we don't need locally compact or even something like Hausdorff.

We begin by showing that $\op{curry}$ is actually well-defined.
\begin{lemma}
    For fixed $x\in X,$ the function $\op{curry}(x):=[y\mapsto f((x,y))]$ is continuous.
\end{lemma}
Fix any $U\subseteq Z$ open so that we want to show its preimage
\[\op{curry}(x)^{-1}(U)=\{y\in Y:f((x,y))\in U\}\subseteq U\]
is also open. However, we know that $f^{-1}(U)\subseteq X\times Y$ is open because $f$ is continuous, and the idea is to attempt to project $f^{-1}(U)$ onto $\{x\}\subseteq X.$

Because $X\times Y$ has basis pairs of open sets, we know that any $y\in Y$ with $(x,y)\in f^{-1}(U)$ has some $U_\bullet$ and $U_y$ with $(x,y)\in U_\bullet\times U_y\subseteq f^{-1}(U).$ Further, we note that each $U_y$ has $U_y\subseteq\op{curry}(x)^{-1}(U)$ because
\[\op{curry}(x)(U_y)=f(\{x\}\times U_y)\subseteq U.\]
It follows that we can write
\[\op{curry}(x)^{-1}(U)\subseteq\bigcup_{(x,y)\in f^{-1}(U)}U_y\subseteq\op{curry}(x)^{-1}(U).\]
We see that equalities must hold, and the middle arbitrary union is an open set, so $\op{curry}(x)^{-1}(U)$ is therefore open. Thus, $\op{curry}(x)$ is continuous, which is what we wanted. $\blacksquare$

Anyways, we now go after the proposition directly. We need to show that each open set $U$ in $\op{Hom}(Y,Z)$ has $\op{curry}^{-1}(U)$ open in $X.$ We'll have to use the compact-open topology sometime, so we begin by reducing to subbasis elements. Letting $V(K,U):=\{f:f(K)\subseteq U\}$ being our subbasis elements of $\op{Hom}(Y,Z),$ we note that we can write
\[U=\bigcup_{\alpha\in\lambda}\left(\bigcap_{k=1}^{n_\alpha}V(K_{\alpha,k},U_{\alpha,k})\right),\]
for $V_{\bullet,\bullet}\subseteq Y$ compact and $U_{\bullet,\bullet}\subseteq Z$ open. Now, we note that
\[\op{curry}^{-1}(U)=\bigcup_{\alpha\in\lambda}\left(\bigcap_{k=1}^{n_\alpha}\op{curry}^{-1}\big(V(K_{\alpha,k},U_{\alpha,k})\big)\right),\]
so it suffices to show that $\op{curry}^{-1}(V(K,U))$ is open for any $K$ and $U.$

As an aside, we note briefly that it is in fact true that, for any function $\varphi:S\to T$ and subsets $\{T_\alpha\}_{\alpha\in\lambda}$ of $T,$
\[\varphi^{-1}\left(\bigcup_{\alpha\in\lambda}T_\alpha\right)=\bigcup_{\alpha\in\lambda}\varphi^{-1}(T_\alpha),\]
for $s$ is in either set if and only there exists an $\alpha\in\lambda$ for which $\varphi(s)\in T_\alpha.$ Similarly,
\[\varphi^{-1}\left(\bigcap_{\alpha\in\lambda}T_\alpha\right)=\bigcap_{\alpha\in\lambda}\varphi^{-1}(T_\alpha),\]
for $s$ is in either set if and only if $\varphi(s)\in T_\alpha$ for each $\alpha\in\lambda.$ We used these facts implicitly to reduce to subbasis; I only mention this because I thought the $\bigcap$ equation above was actually false because $\varphi(S_1\cap S_2)$ is not necessarily equal to $\varphi(S_1)\cap g(S_2)$ for subsets $S_1,S_2\subseteq S.$

Continuing, we fix $K_Y\subseteq Y$ compact and $U_Z\subseteq Z$ open, and we want to show $\op{curry}^{-1}(V(K_Y,U_Z))=:S$ is open. Expanding out the definitions, we want to show that
\[S=\{x\in X:[y\mapsto f((x,y))]\in V(K_Y,U_Z)\}=\{x\in X:f(\{x\}\times K_Y)\subseteq U_Z\}\]
is open. Here we see opportunity to use the continuity of $f,$ so we take it: it suffices to show that
\[S=\left\{x\in X:\{x\}\times K_Y\subseteq f^{-1}(U_Z)\right\}\]
is open. The set $f^{-1}(U_Z)$ is pretty much just an arbitrary open set by continuity, so we let it be $U_{XY}\subseteq X\times Y.$

To continue the argument, we look locally. It suffices to show that, for each $x\in S,$ there is an open neighborhood $U_x\subseteq X$ around $x$ for which $U_x\subseteq S.$ Indeed, then we could write
\[S\subseteq\bigcup_{x\in S}U_x\subseteq S,\]
from which $S$ being open would follow. So we have to show that $\{x\}\times K_Y\subseteq U_{XY}$ implies there exists an open set $U_x\subseteq X$ containing $x$ for which $U_x\times K_Y\subseteq U_{XY}.$ Visually, we have to ``expand'' $\{x\}$ into a slightly larger box inside $U_{XY}.$

We should have to use the compactness of $K_Y$ eventually because I promised it was important, so we attempt to build an open cover of $K_Y.$ For each $k\in K_Y,$ we note that $(x,k)\in U_{XY}.$ Using the basis of $X\times Y,$ this gives us open sets $U_{(x,k),x}$ and $U_{(x,k),k}$ for which
\[(x,k)\in U_{(x,k),x}\times U_{(x,k),k}\subseteq U_{XY}.\]
We would like to just intersect all of the $U_{(x,k),x}$ to generate a set around $x$ whose product with $K_Y$ is inside of $U_{XY}.$ However, we can't take arbitrary intersections, so we use the compactness of $K_Y$ to make it a finite intersection. Because the $U_{(x,k),k}$ are neighborhoods around each $k\in K_Y,$ we see
\[K_Y\subseteq\bigcup_{k\in K_Y}U_{(x,k),k},\]
so we have an open cover. So we extract $k_1,\ldots,k_n\in K_Y$ for which $U_{(x,k_\bullet),k_\bullet}$ completely coves $K_Y$ by compactness, and we claim that
\[U_x:=\bigcap_{\ell=1}^nU_{(x,k_\ell),x}\]
works. Indeed, $U_x$ is the finite intersection of open sets all containing $x,$ so $U_x$ is also an open set containing $x.$ And for any $x'\in U_x,$ we have that
\[\{x'\}\times U_{(x,k_\bullet),k_\bullet}\subseteq U_{(x,k_\bullet),x}\times U_{(x,k_\bullet),k_\bullet}\subseteq U\]
for each $k_\bullet,$ so $\{x'\}\times K_Y\subseteq U$ follows. Thus, we do have $U_x\subseteq U_{XY},$ and we are done here. $\blacksquare$

As a final remark, we note that both the ``open'' and the ``compact'' of the compact-open topology played subtle but crucial roles. The ``open'' part was used in the continuity of $f$ to get $U_{XY}=f^{-1}(U_Z)$ open. This feels quite natural to me, for it's not clear how we would use the continuity of $f$ except for requiring the open set to be open.

The ``compact'' part was used at the very end to expand out $\{x\}\times K_Y$ to an open neighborhood around $\{x\}.$ I can almost believe that compactness is the ``correct'' hypothesis to make currying work because otherwise it's not clear if we're always able to expand out $\{x\}\times K_Y.$ Namely, some kind of smallness condition is necessary on $K_Y$ to be able to expand out a box like this.

\subsubsection{March 9th}
Today I learned a way generating functions explicitly intersects with additive combinatorics. In reality, I just want to showcase the following problem from the IMOSL and then talk some philosophy.
\begin{proposition}
    There exists a set $X\subseteq\NN$ such that, for every $n\in\NN,$ there exists unique (but not necessarily distinct) $a,b,c\in X$ such that $a+2b+4c=n.$
\end{proposition}
The motivated way to solve this question is via generating functions. Let
\[f(x)=\sum_{a\in X}x^a.\]
Note that $f(x)<\sum_{n\in\NN}x^n=\frac1{1-x},$ so $f(x)$ at least converges (absolutely!) for $|x|<1.$ This doesn't matter so much because we're going to consider $f(x)$ formally anyways. To get a condition like $a+2b+4c=n,$ we have to consider as well the functions
\[f\left(x^2\right)=\sum_{b\in X}x^{2b}\quad\text{and}\quad f\left(x^4\right)=\sum_{c\in X}x^{4c}.\]
At this point, the translation to the question at hand is done by just multiplying
\[f(x)f\left(x^2\right)f\left(x^4\right)=\sum_{a,b,c\in X}x^{a+2b+4c}.\]
The condition that there exists exactly one unique way to write each $n\in\NN$ as $n=a+2b+4c$ means that each power $x^n$ occurs exactly once in the sum. Additionally, we certainly have $a+2b+4c\in\NN$ because $a,b,c\in X\subseteq\NN,$ so the right-hand sum must actually be $\sum_{n\in\NN}x^n.$ That is, we have to exhibit $f(x)$ so that
\[f(x)f\left(x^2\right)f\left(x^4\right)=\sum_{a,b,c\in X}x^n.\]
This is not actually easy.

Numerical experiments does one well here. Experimentally, we can basically just guess-and-check adding on monomials to $f(x)$ to force the expansion of $f(x)f\left(x^2\right)f\left(x^3\right)$ to hit every single $x^n$ while keeping the coefficients at $1.$ The first few terms come out to
\[f(x)=1+x+x^8+x^9+x^{64}+x^{65}+x^{72}+x^{73}+x^{512}+\cdots\]
The fact that terms come in pairs of $x^n+x^{n+1}$ suggests that we should factor out $1+x.$ Continuing the expansion, we get
\[f(x)=(1+x)\left(1+x^8+x^{64}+x^{72}+x^{512}+x^{520}+\cdots\right).\]
Now terms come in pairs of $x^n+x^{n+8},$ so we factor out $1+x^8$ to get
\[f(x)=(1+x)\left(1+x^8\right)\left(1+x^{64}+x^{512}+x^{576}+x^{4096}+x^{4160}+\cdots\right).\]
We see terms come in pairs of $x^n+x^{n+64},$ so we factor this out again to get
\[f(x)=(1+x)\left(1+x^8\right)\left(1+x^64\right)\left(1+x^{512}+x^{4096}+\cdots\right)\]
At this point, the pattern suggests that we have
\[f(x)\stackrel?=\prod_{k=0}^\infty\left(1+x^{8^k}\right).\]
This doesn't actually expand out to any nice (say, rational) function, so trying to solve $f(x)f\left(x^2\right)f\left(x^4\right)=\frac1{1-x}$ by inspection would likely fail.

Anyways, it remains to show that this $f(x)$ actually works. Well, now that we're thinking infinite products, we note that
\[\sum_{n\in\NN}x^n=\prod_{k=0}^\infty\left(1+x^{2^k}\right)\]
by considering the unique binary expansion of each $n\in\NN.$ So we can now split up this factorization as
\[\sum_{n\in\NN}x^n=\prod_{k=0}^\infty\left[\left(1+x^{2^{3k}}\right)\left(1+x^{2^{3k+1}}\right)\left(1+x^{2^{3k+2}}\right)\right],\]
which is
\[\sum_{n\in\NN}x^n=\underbrace{\left(\prod_{k=0}^\infty1+x^{8^k}\right)}_{f(x)}\underbrace{\left(\prod_{k=0}^\infty1+\left(x^2\right)^{8^k}\right)}_{f(x^2)}\underbrace{\left(\prod_{k=0}^\infty1+\left(x^4\right)^{8^k}\right)}_{f(x^4)}.\]
So we see that $f(x)$ does in fact exist. Expanding out $f(x)$ directly and extracting the powers back out gives the elements of $X.$ In particular, the coefficient of each nonzero term in the expansion of $f(x)$ is $1$ because, considering the base-$8$ expansion of each exponent, all oct-its need to be $0$ or $1,$ and the uniqueness follows for the same reason that base-$2$ expansion is unique. $\blacksquare$

What's nice about the generating functions proof is that, once one begins to use generating functions, no part of the proof requires extreme cleverness. However, now that we know that we are supposed to get
\[f(x)=\prod_{k=0}^\infty\left(1+x^{8^k}\right)\]
at the end, we can exhibit $X$ without mentioning generating functions at all. In particular, we mentioned at the end that the nonzero terms of $f(x)$ are those with only $0$s and $1$s in their base-$8$ expansion, which we see by directly expanding the above product. So we claim directly that
\[X\stackrel?=\{x\in\NN:x_8\text{ has only }0\text{ and }1\}.\]
Thinking in terms of base $8,$ it is not as hard to see that every $n\in\NN$ can be written as $a+2b+4c$ for $a,b,c\in X.$ Indeed, fix any $n\in\NN,$ and write its binary expansion as
\[n=\sum_{k=0}^\infty b_k2^k,\]
where $\{b_k\}_{k=0}^\infty$ is eventually constantly $0.$ Now, as in the generating functions proof, we split by $k\pmod3,$ writing
\[n=\sum_{k=0}^\infty\left(b_{3k}2^{3k}+b_{3k+1}2^{3k+1}+b_{3k+2}2^{3k+2}\right)\]
so that we can regroup as
\[n=\underbrace{\left(\sum_{k=0}^\infty b_{3k}8^k\right)}_{\in X}+2\underbrace{\left(\sum_{k=0}^\infty b_{3k+1}8^k\right)}_{\in X}+4\underbrace{\left(\sum_{k=0}^\infty b_{3k+2}8^k\right)}_{\in X}.\]
So we get our representation of each $n\in\NN,$ and it certainly feels very unique. Indeed, the computation has $a+2b+4c$ has no carries in base $2,$ so we can biject bits from each $a,b,c\in X$ with bits in $n\in\NN,$ and there is no overlap. Explicitly, bit $a_k2^k$ of $a$ goes to $n_{3k}8^k$ and so on.

This is not terribly rigorous, but we could make it rigorous if we wanted. For example, $n\pmod8$ certainly uniquely determines $a,b,c\pmod2$ because the higher bits of $a,b,c\in X$ don't matter. After subtracting the required bit, each of $n,a,b,c$ will be divisible by $8$ (because $a_8,b_8,c_8$ only have $0$s and $1$s), so we can divide everything by $8$ and induct downwards. $\blacksquare$

Anyways, the point of all this discussion is that expansion of generating functions can algebraically deal with sums of restricted subsets of $\NN,$ and they can even count the number of times some $n\in\NN$ gets hit. For example, I think the Hardy-Littlewood circle method concerns itself with products of
\[f(q)=\sum_{k\in\NN}q^{k^2},\]
which can feasibly compute properties of sums of squares (and other powers more generally).